Reinforcement Learning (RL) is a fascinating branch of computer science that trains agents to make sequential decisions and solve complex tasks. A new technique, Hierarchical Reinforcement Learning (HRL), aims to enhance RL agents' capabilities by dividing tasks into simpler subtasks. Despite recent successes, HRL methods often struggle with generalizing to new environments and transferring policies to different agents. In response, researchers have developed HiDe, a novel HRL architecture. HiDe introduces a hierarchical structure that separates planning and low-level control, improving performance in navigation tasks with sparse rewards. The framework consists of a planning layer that learns a valuable map of the environment and two additional layers for subgoal refinement and low-level control. Impressively, HiDe can be trained end-to-end and generalizes well to new environments, even with sparse rewards. The planning layer is transferable between agents, showcasing the benefits of functional decomposition. The approach is evaluated on complex continuous control tasks, demonstrating successful navigation and improved generalization. The paper highlights HiDe's potential for enhancing RL agents' capabilities and enabling effective planning over long horizons. This innovative work contributes to the growing field of RL and provides exciting insights for both researchers and students interested in intelligent agent behavior. The proposed framework paves the way for more sophisticated RL applications and underscores the importance of hierarchical structures in solving complex tasks.
