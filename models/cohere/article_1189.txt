Knowledge Distillation (KD) is a technique where a simpler student model learns from a more complex teacher model. A new approach to KD is proposed, utilizing data augmentation agents to generate unique training data for both the teacher and student. This method adapts to the specific needs of each model, enhancing the student's learning experience. The process involves two stages: first, training the teacher while learning data augmentation strategies specific to its role; second, distilling knowledge to the student while generating tailored training data for improved comprehension. This strategy is particularly effective for network quantization, where the goal is to transfer knowledge from a high-precision teacher to a low-precision student. By addressing the student's specific weaknesses, the method enhances its learning and mitigates performance loss. The approach is compared with existing KD techniques, showcasing its effectiveness in improving student performance. The code for these experiments will be made publicly available, offering a valuable resource for further exploration in this field.
