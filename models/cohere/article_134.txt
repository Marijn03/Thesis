Weight-sharing is a technique used in neural architecture search to optimize multiple neural networks simultaneously. Despite its success, weight-sharing is not well understood. In this article, researchers aim to shed light on weight-sharing by studying its impact on neural architecture search and hyperparameter optimization. They propose a new perspective, arguing that weight-sharing is about relaxing a structured hypothesis space. The authors then delve into the algorithmic and theoretical challenges that arise, offering solutions rooted in non-convex non-Euclidean optimization. They also analyze the learning behavior of the bilevel optimization used in practical weight-sharing methods. Through case studies and experiments, they demonstrate weight-sharing's effectiveness in architecture search and feature selection. Notably, they introduce EDARTS, an improved version of a well-known architecture search method, which finds better architectures more efficiently. This work contributes to our understanding of weight-sharing and provides tools to enhance neural architecture search, offering students valuable insights into the evolving field of computer science, specifically AI and machine learning.
