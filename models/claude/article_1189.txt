Knowledge Distillation (KD) is a technique used in machine learning to transfer knowledge from a large, high-performance model (teacher) to a smaller, less computationally expensive model (student). Typically, the student model is trained using the outputs or representations from the teacher model. However, existing KD methods overlook the importance of the training data itself. Just like human teachers adapt their teaching materials to fit each student's background and interests, this work proposes using separate data augmentation strategies to generate customized training examples for the teacher and student models. 

The motivation is that the student model may struggle to learn from the same data that the teacher model was trained on, due to its lower capacity. By generating tailored examples that highlight the teacher's strengths, the student can more effectively absorb its knowledge. This approach is particularly useful when distilling knowledge from high-precision models to low-precision models, which are important for deploying AI on devices with limited computing power. Experiments show that using role-wise data augmentation during KD improves the student's performance, especially for extremely low-precision models.
