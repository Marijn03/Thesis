Researchers are working on developing reusable neural network components called sentence encoders that can understand the meaning of sentences. A popular approach is to first pretrain these encoders on a language task like predicting the next word (language modeling), and then use the pretrained encoder for other language tasks. This paper systematically evaluated 40 different pretraining methods across 9 language understanding tasks. The study found that language modeling is the most effective single pretraining task. However, combining multiple pretraining tasks (multitask learning) can perform even better. Surprisingly, very simple baseline encoders performed almost as well as the best pretrained ones. Different language tasks also benefited from different pretraining methods. The researchers suggest that while pretraining is useful, new techniques beyond the current "pretrain and freeze" approach may be needed to make further progress.
