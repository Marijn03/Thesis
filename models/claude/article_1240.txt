Machine learning models are trained on data to learn patterns and make predictions. However, sometimes the training data contains irrelevant information that the model picks up on instead of the relevant patterns. This is called "negative transfer". For example, a model trained to identify cheetahs vs. snow leopards may look at the habitat backgrounds (grass vs. snow) instead of the animal fur patterns. Negative transfer makes models perform poorly on new data. This paper proposes using "adversarial training" to prevent negative transfer. Adversarial training pits two models against each other - one trying to predict from relevant features, the other from irrelevant ones. This forces the main model to ignore the misleading information. The researchers showed their method works better than previous approaches on image datasets. Preventing negative transfer can greatly improve the generalization of machine learning models.
