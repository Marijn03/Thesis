Dropout is a technique used in deep learning to prevent overfitting of neural networks to training data. Researchers found some drawbacks with traditional dropout like needing to tune rates for each layer and incompatibility with batch normalization. They proposed a new method called "Jumpout" that improves upon dropout. Jumpout samples dropout rates from a special distribution, adaptively adjusts rates per layer/sample, and rescales outputs for better compatibility with batch normalization. In experiments across multiple image datasets, Jumpout showed significantly better performance than original dropout while having very low additional computational cost. This research demonstrates how innovating on fundamental techniques can meaningfully advance the field of deep learning.
