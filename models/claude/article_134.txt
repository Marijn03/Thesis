Neural networks are complex machine learning models that can perform tasks like image recognition and language processing. However, designing the architecture (structure) of these networks is a challenging problem. Weight-sharing is a technique that allows multiple neural network architectures to be optimized simultaneously using the same parameters. This approach has been successful but is not well understood. The article explains that weight-sharing is not just a trick, but a way to extend the search space of possible architectures. It provides new algorithms to efficiently optimize weight-sharing objectives and analyzes the theoretical properties of this approach. Weight-sharing can be applied beyond just neural architecture search, like selecting features for text data. The article introduces a new method called EDARTS that uses weight-sharing to search for good convolutional neural network architectures on the CIFAR-10 image dataset. EDARTS exploits the geometry of the optimization problem and finds better architectures than previous methods in less time.
