Clustering is the task of automatically grouping similar data points together into clusters. Spectral clustering is a popular clustering technique, but it has limitations in scaling to very large datasets and generalizing to new data points not originally used for training. This paper introduces a new deep learning approach called SpectralNet that overcomes these limitations. SpectralNet is a neural network model that learns to map data points into an embedding space where spectral clustering can be applied efficiently. It uses stochastic optimization during training to scale to big datasets. SpectralNet also naturally generalized the learned embedding to cluster any new data points. To improve clustering quality, the researchers trained additional neural networks called Siamese nets to learn better similarity measures between data points. Applying SpectralNet to data representations from autoencoders further boosted performance. Theoretically, the paper also derived bounds on the neural network size needed for spectral clustering. Experiments showed SpectralNet successfully clustered challenging non-convex datasets that other methods failed on. It achieved state-of-the-art clustering results on datasets like MNIST handwritten digits and Reuters news articles.
