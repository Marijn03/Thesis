[
    {
        "id": 487,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nThe capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks. Out-of-distribution (OOD) detection, also known as novelty detection, refers to the task of identifying the samples that differ in some respect from the training samples. Recently, deep neural networks (DNNs) turned out to show unpredictable behaviors in case of mismatch between the training and testing data distributions; for example, they tend to make high confidence prediction for the samples that are drawn from OOD or belong to unseen classes (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017) . For this reason, accurately measuring the distributional uncertainty (Malinin & Gales, 2018) of DNNs becomes one of the important challenges in many real-world applications where we can hardly control the testing data distribution. Several recent studies have tried to simply detect OOD samples using the confidence score defined by softmax probability (Hendrycks & Gimpel, 2017; Liang et al., 2018) or Mahalanobis distance from class means (Lee et al., 2018) , and they showed promising results even without re-training the model. However, all of them employ the DNNs designed for a discriminative (or softmax) classifier, which has limited power to locate OOD samples distinguishable with in-distribution (ID) samples in their latent space. To be specific, the softmax classifier is optimized to learn the discriminative latent space where the training samples are aligned along their corresponding class weight vectors, maximizing the softmax probability for the target classes. As pointed out in (Hendrycks & Gimpel, 2017) , OOD samples are more likely to have small values of the softmax probability for all known classes, which means that their latent vectors get closer to the origin. As a result, there could be a large overlap between two sets of ID and OOD samples in the latent space (Figure 1 ), which eventually reduces the gap between their confidence scores and degrades the performance as well. In addition, most of existing confidence scores adopt additional calibration techniques Hinton et al., 2015) to enhance the reliability of the detection, but they include several hyperparameters whose optimal values vary depending on the testing data distribution. In this situation, they utilized a small portion of each test set (containing both ID and OOD samples) for validation, and reported the results evaluated on the rest by using the optimal hyperparameter values for each test case. Considering the motivation of OOD detection that prior knowledge of test distributions is not available before we encounter them, such process of tuning the hyperparameters for each test case is not practical when deploying the DNNs in practice. In this paper, we propose a novel objective to train DNNs with a generative (or distance) classifier which is capable of effectively identifying OOD test samples. The main difference of our deep generative classifier is to learn separable class-conditional distributions in the latent space, by explicitly modeling them as a DNN layer. The generative classifier places OOD samples further apart from the distributions of all given classes, without utilizing OOD samples for its validation. Thus, based on the Euclidean distance between a test sample and the centers of the obtained class-conditional distributions, we can calculate how likely and how confidently the sample belongs to each class. This can be interpreted as a multi-class extension of unsupervised anomaly detection (Ruff et al., 2018) , and Gaussian discriminant analysis provides the theoretical background for incorporating the generative classifier into the DNNs. Our extensive experiments on images and tabular data demonstrate that the proposed classifier distinguishes OOD samples more accurately than the state-of-the-art method, while maintaining the classification accuracy for ID samples. This paper introduces a deep learning objective to learn the multi-class generative classifier, by fusing the concept of Gaussian discriminant analysis with DNNs. Unlike the conventional softmax classifier, our generative (or distance) classifier learns the class-conditional distributions to be separated from each other and follow the Gaussian distribution at the same time, thus it is able to effectively distinguish OOD samples from ID samples. We empirically show that our confidence score beats other competing methods in detecting both OOD tabular data and OOD images, and also the distance classifier can be easily combined with various types of DNNs to further improve their performances.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the importance of detecting out-of-distribution (OOD) samples in machine learning models, which are samples that do not match the training data. The authors propose a new type of classifier called a generative classifier, which is better at detecting OOD samples than traditional classifiers. This classifier works by modeling the class-conditional distributions as separate Gaussian distributions in the latent space, allowing it to distinguish OOD samples from in-distribution (ID) samples. The authors test their classifier on various types of data, including images and tabular data, and show that it outperforms other methods in detecting OOD samples while maintaining classification accuracy for ID samples.\n\nThe important entities in this document are the classifier, the training data, and the testing data. The important dates mentioned are not applicable, as this is a research paper and does not contain specific dates. The events happening in this document are the development of the generative classifier and its testing on various types of data. The result of these events is the creation of a more accurate and reliable classifier that can detect OOD samples effectively.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nThe capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks. Out-of-distribution (OOD) detection, also known as novelty detection, refers to the task of identifying the samples that differ in some respect from the training samples. Recently, deep neural networks (DNNs) turned out to show unpredictable behaviors in case of mismatch between the training and testing data distributions; for example, they tend to make high confidence prediction for the samples that are drawn from OOD or belong to unseen classes (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017) . For this reason, accurately measuring the distributional uncertainty (Malinin & Gales, 2018) of DNNs becomes one of the important challenges in many real-world applications where we can hardly control the testing data distribution. Several recent studies have tried to simply detect OOD samples using the confidence score defined by softmax probability (Hendrycks & Gimpel, 2017; Liang et al., 2018) or Mahalanobis distance from class means (Lee et al., 2018) , and they showed promising results even without re-training the model. However, all of them employ the DNNs designed for a discriminative (or softmax) classifier, which has limited power to locate OOD samples distinguishable with in-distribution (ID) samples in their latent space. To be specific, the softmax classifier is optimized to learn the discriminative latent space where the training samples are aligned along their corresponding class weight vectors, maximizing the softmax probability for the target classes. As pointed out in (Hendrycks & Gimpel, 2017) , OOD samples are more likely to have small values of the softmax probability for all known classes, which means that their latent vectors get closer to the origin. As a result, there could be a large overlap between two sets of ID and OOD samples in the latent space (Figure 1 ), which eventually reduces the gap between their confidence scores and degrades the performance as well. In addition, most of existing confidence scores adopt additional calibration techniques Hinton et al., 2015) to enhance the reliability of the detection, but they include several hyperparameters whose optimal values vary depending on the testing data distribution. In this situation, they utilized a small portion of each test set (containing both ID and OOD samples) for validation, and reported the results evaluated on the rest by using the optimal hyperparameter values for each test case. Considering the motivation of OOD detection that prior knowledge of test distributions is not available before we encounter them, such process of tuning the hyperparameters for each test case is not practical when deploying the DNNs in practice. In this paper, we propose a novel objective to train DNNs with a generative (or distance) classifier which is capable of effectively identifying OOD test samples. The main difference of our deep generative classifier is to learn separable class-conditional distributions in the latent space, by explicitly modeling them as a DNN layer. The generative classifier places OOD samples further apart from the distributions of all given classes, without utilizing OOD samples for its validation. Thus, based on the Euclidean distance between a test sample and the centers of the obtained class-conditional distributions, we can calculate how likely and how confidently the sample belongs to each class. This can be interpreted as a multi-class extension of unsupervised anomaly detection (Ruff et al., 2018) , and Gaussian discriminant analysis provides the theoretical background for incorporating the generative classifier into the DNNs. Our extensive experiments on images and tabular data demonstrate that the proposed classifier distinguishes OOD samples more accurately than the state-of-the-art method, while maintaining the classification accuracy for ID samples. This paper introduces a deep learning objective to learn the multi-class generative classifier, by fusing the concept of Gaussian discriminant analysis with DNNs. Unlike the conventional softmax classifier, our generative (or distance) classifier learns the class-conditional distributions to be separated from each other and follow the Gaussian distribution at the same time, thus it is able to effectively distinguish OOD samples from ID samples. We empirically show that our confidence score beats other competing methods in detecting both OOD tabular data and OOD images, and also the distance classifier can be easily combined with various types of DNNs to further improve their performances.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the importance of detecting out-of-distribution (OOD) samples in machine learning models. OOD samples are data points that are different from the training data and can cause the model to make incorrect predictions. The authors propose a new type of classifier called a generative classifier that can detect OOD samples more accurately than existing methods. The generative classifier works by learning the class-conditional distributions of the data, which are the probability distributions of each class. The authors show that this approach can be used to detect OOD samples in both image and tabular data. The key dates mentioned in the document are the dates of previous research papers that have addressed the issue of OOD detection. The important events in the document are the development of the generative classifier and its application to OOD detection. The result of these events is a more accurate method for detecting OOD samples, which can be used to improve the performance of machine learning models in real-world applications.\n\nNote that the important entities in this document are the authors, the generative classifier, and the concept of OOD detection. The important dates are the dates of previous research papers that have addressed the issue of OOD detection. The events in the document are the development of the generative classifier and its application to OOD detection. The result of these events is a more accurate method for detecting OOD samples.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nThe capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks. Out-of-distribution (OOD) detection, also known as novelty detection, refers to the task of identifying the samples that differ in some respect from the training samples. Recently, deep neural networks (DNNs) turned out to show unpredictable behaviors in case of mismatch between the training and testing data distributions; for example, they tend to make high confidence prediction for the samples that are drawn from OOD or belong to unseen classes (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017) . For this reason, accurately measuring the distributional uncertainty (Malinin & Gales, 2018) of DNNs becomes one of the important challenges in many real-world applications where we can hardly control the testing data distribution. Several recent studies have tried to simply detect OOD samples using the confidence score defined by softmax probability (Hendrycks & Gimpel, 2017; Liang et al., 2018) or Mahalanobis distance from class means (Lee et al., 2018) , and they showed promising results even without re-training the model. However, all of them employ the DNNs designed for a discriminative (or softmax) classifier, which has limited power to locate OOD samples distinguishable with in-distribution (ID) samples in their latent space. To be specific, the softmax classifier is optimized to learn the discriminative latent space where the training samples are aligned along their corresponding class weight vectors, maximizing the softmax probability for the target classes. As pointed out in (Hendrycks & Gimpel, 2017) , OOD samples are more likely to have small values of the softmax probability for all known classes, which means that their latent vectors get closer to the origin. As a result, there could be a large overlap between two sets of ID and OOD samples in the latent space (Figure 1 ), which eventually reduces the gap between their confidence scores and degrades the performance as well. In addition, most of existing confidence scores adopt additional calibration techniques Hinton et al., 2015) to enhance the reliability of the detection, but they include several hyperparameters whose optimal values vary depending on the testing data distribution. In this situation, they utilized a small portion of each test set (containing both ID and OOD samples) for validation, and reported the results evaluated on the rest by using the optimal hyperparameter values for each test case. Considering the motivation of OOD detection that prior knowledge of test distributions is not available before we encounter them, such process of tuning the hyperparameters for each test case is not practical when deploying the DNNs in practice. In this paper, we propose a novel objective to train DNNs with a generative (or distance) classifier which is capable of effectively identifying OOD test samples. The main difference of our deep generative classifier is to learn separable class-conditional distributions in the latent space, by explicitly modeling them as a DNN layer. The generative classifier places OOD samples further apart from the distributions of all given classes, without utilizing OOD samples for its validation. Thus, based on the Euclidean distance between a test sample and the centers of the obtained class-conditional distributions, we can calculate how likely and how confidently the sample belongs to each class. This can be interpreted as a multi-class extension of unsupervised anomaly detection (Ruff et al., 2018) , and Gaussian discriminant analysis provides the theoretical background for incorporating the generative classifier into the DNNs. Our extensive experiments on images and tabular data demonstrate that the proposed classifier distinguishes OOD samples more accurately than the state-of-the-art method, while maintaining the classification accuracy for ID samples. This paper introduces a deep learning objective to learn the multi-class generative classifier, by fusing the concept of Gaussian discriminant analysis with DNNs. Unlike the conventional softmax classifier, our generative (or distance) classifier learns the class-conditional distributions to be separated from each other and follow the Gaussian distribution at the same time, thus it is able to effectively distinguish OOD samples from ID samples. We empirically show that our confidence score beats other competing methods in detecting both OOD tabular data and OOD images, and also the distance classifier can be easily combined with various types of DNNs to further improve their performances.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the importance of detecting out-of-distribution (OOD) samples in machine learning models. OOD samples are data points that are not part of the training data and can cause models to make incorrect predictions. The authors propose a new type of classifier called a generative classifier, which is better at detecting OOD samples than traditional classifiers. This classifier works by learning the patterns in the data and creating a model of what normal data looks like. It then uses this model to identify OOD samples by measuring how far they are from the normal patterns. The authors test their classifier on several types of data and show that it outperforms other methods in detecting OOD samples. They also show that their classifier can be used with different types of machine learning models to improve their performance.\n\nImportant entities: OOD samples, generative classifier, machine learning models, training data, test data\n\nImportant dates: Not applicable\n\nEvents: The authors propose a new type of classifier called a generative classifier, which is better at detecting OOD samples than traditional classifiers.\n\nResult: The generative classifier outperforms other methods in detecting OOD samples and can be used with different types of machine learning models to improve their performance.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nThe capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks. Out-of-distribution (OOD) detection, also known as novelty detection, refers to the task of identifying the samples that differ in some respect from the training samples. Recently, deep neural networks (DNNs) turned out to show unpredictable behaviors in case of mismatch between the training and testing data distributions; for example, they tend to make high confidence prediction for the samples that are drawn from OOD or belong to unseen classes (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017) . For this reason, accurately measuring the distributional uncertainty (Malinin & Gales, 2018) of DNNs becomes one of the important challenges in many real-world applications where we can hardly control the testing data distribution. Several recent studies have tried to simply detect OOD samples using the confidence score defined by softmax probability (Hendrycks & Gimpel, 2017; Liang et al., 2018) or Mahalanobis distance from class means (Lee et al., 2018) , and they showed promising results even without re-training the model. However, all of them employ the DNNs designed for a discriminative (or softmax) classifier, which has limited power to locate OOD samples distinguishable with in-distribution (ID) samples in their latent space. To be specific, the softmax classifier is optimized to learn the discriminative latent space where the training samples are aligned along their corresponding class weight vectors, maximizing the softmax probability for the target classes. As pointed out in (Hendrycks & Gimpel, 2017) , OOD samples are more likely to have small values of the softmax probability for all known classes, which means that their latent vectors get closer to the origin. As a result, there could be a large overlap between two sets of ID and OOD samples in the latent space (Figure 1 ), which eventually reduces the gap between their confidence scores and degrades the performance as well. In addition, most of existing confidence scores adopt additional calibration techniques Hinton et al., 2015) to enhance the reliability of the detection, but they include several hyperparameters whose optimal values vary depending on the testing data distribution. In this situation, they utilized a small portion of each test set (containing both ID and OOD samples) for validation, and reported the results evaluated on the rest by using the optimal hyperparameter values for each test case. Considering the motivation of OOD detection that prior knowledge of test distributions is not available before we encounter them, such process of tuning the hyperparameters for each test case is not practical when deploying the DNNs in practice. In this paper, we propose a novel objective to train DNNs with a generative (or distance) classifier which is capable of effectively identifying OOD test samples. The main difference of our deep generative classifier is to learn separable class-conditional distributions in the latent space, by explicitly modeling them as a DNN layer. The generative classifier places OOD samples further apart from the distributions of all given classes, without utilizing OOD samples for its validation. Thus, based on the Euclidean distance between a test sample and the centers of the obtained class-conditional distributions, we can calculate how likely and how confidently the sample belongs to each class. This can be interpreted as a multi-class extension of unsupervised anomaly detection (Ruff et al., 2018) , and Gaussian discriminant analysis provides the theoretical background for incorporating the generative classifier into the DNNs. Our extensive experiments on images and tabular data demonstrate that the proposed classifier distinguishes OOD samples more accurately than the state-of-the-art method, while maintaining the classification accuracy for ID samples. This paper introduces a deep learning objective to learn the multi-class generative classifier, by fusing the concept of Gaussian discriminant analysis with DNNs. Unlike the conventional softmax classifier, our generative (or distance) classifier learns the class-conditional distributions to be separated from each other and follow the Gaussian distribution at the same time, thus it is able to effectively distinguish OOD samples from ID samples. We empirically show that our confidence score beats other competing methods in detecting both OOD tabular data and OOD images, and also the distance classifier can be easily combined with various types of DNNs to further improve their performances.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nIn the real world, it's common for the data we test a machine learning model with to be different from the data the model was trained on. This is called an \"out-of-distribution\" (OOD) sample. To detect OOD samples, researchers have developed a new type of machine learning model called a generative classifier. This model is different from traditional classifiers because it doesn't just focus on the decision boundary between classes, but also tries to model the distribution of each class. This allows the model to calculate a confidence score for each sample, which can be used to detect OOD samples. The researchers tested their model on various types of data, including images and tables, and found that it outperformed other state-of-the-art methods in detecting OOD samples while maintaining good classification accuracy for in-distribution samples.\n\nThe important entities in this document are the machine learning models, the data they are trained and tested on, and the concept of out-of-distribution samples.\n\nThe important dates mentioned in the document are not explicitly stated, but the references provided at the end of the document are from 2014 to 2018.\n\nThe events happening in this document are the development and testing of a new machine learning model, the generative classifier, and its comparison to other state-of-the-art methods.\n\nThe result of these events is the creation of a more accurate and reliable machine learning model that can detect out-of-distribution samples, which has potential applications in various fields such as image and speech recognition, natural language processing, and more."
        }
    },
    {
        "id": 1213,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nThe backpropagation algorithm is the most popular algorithm training neural networks nowadays. However, it suffers from the forward locking, backward locking and update locking problems, especially when a neural network is so large that its layers are distributed across multiple devices. Existing solutions either can only handle one locking problem or lead to severe accuracy loss or memory inefficiency. Moreover, none of them consider the straggler problem among devices. In this paper, we propose \\textbf{Layer-wise Staleness} and a novel efficient training algorithm, \\textbf{Diversely Stale Parameters} (DSP), which can address all these challenges without loss of accuracy nor memory issue. We also analyze the convergence of DSP with two popular gradient-based methods and prove that both of them are guaranteed to converge to critical points for non-convex problems. Finally, extensive experimental results on training deep convolutional neural networks demonstrate that our proposed DSP algorithm can achieve significant training speedup with stronger robustness and better generalization than compared methods.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe backpropagation algorithm is a widely used method for training artificial neural networks, but it has some limitations. For example, when a network is very large and spread across multiple devices, it can get stuck in certain problems, which can slow down the training process. Researchers have proposed different solutions to address these issues, but they often come with trade-offs such as reduced accuracy or increased memory usage. A new approach called Diversely Stale Parameters (DSP) has been developed to overcome these challenges, and it has been tested on deep convolutional neural networks, showing significant improvements in training speed and robustness.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nThe backpropagation algorithm is the most popular algorithm training neural networks nowadays. However, it suffers from the forward locking, backward locking and update locking problems, especially when a neural network is so large that its layers are distributed across multiple devices. Existing solutions either can only handle one locking problem or lead to severe accuracy loss or memory inefficiency. Moreover, none of them consider the straggler problem among devices. In this paper, we propose \\textbf{Layer-wise Staleness} and a novel efficient training algorithm, \\textbf{Diversely Stale Parameters} (DSP), which can address all these challenges without loss of accuracy nor memory issue. We also analyze the convergence of DSP with two popular gradient-based methods and prove that both of them are guaranteed to converge to critical points for non-convex problems. Finally, extensive experimental results on training deep convolutional neural networks demonstrate that our proposed DSP algorithm can achieve significant training speedup with stronger robustness and better generalization than compared methods.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe backpropagation algorithm is a widely used method for training neural networks, but it has some major limitations. The proposed solution, Diversely Stale Parameters (DSP), addresses the problems of forward locking, backward locking, and update locking, which occur when training large neural networks across multiple devices.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nThe backpropagation algorithm is the most popular algorithm training neural networks nowadays. However, it suffers from the forward locking, backward locking and update locking problems, especially when a neural network is so large that its layers are distributed across multiple devices. Existing solutions either can only handle one locking problem or lead to severe accuracy loss or memory inefficiency. Moreover, none of them consider the straggler problem among devices. In this paper, we propose \\textbf{Layer-wise Staleness} and a novel efficient training algorithm, \\textbf{Diversely Stale Parameters} (DSP), which can address all these challenges without loss of accuracy nor memory issue. We also analyze the convergence of DSP with two popular gradient-based methods and prove that both of them are guaranteed to converge to critical points for non-convex problems. Finally, extensive experimental results on training deep convolutional neural networks demonstrate that our proposed DSP algorithm can achieve significant training speedup with stronger robustness and better generalization than compared methods.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the document for a high school student:\n\nThe backpropagation algorithm is a popular way to train neural networks, but it has some limitations. For example, when a neural network is very large and spread across multiple devices, it can get stuck in a \"locking\" problem, which slows down the training process. The authors of this paper propose a new algorithm called Diversely Stale Parameters (DSP) that can handle these issues without losing accuracy or using too much memory. They also prove that DSP is guaranteed to work for non-convex problems and show that it can train deep neural networks faster and more robustly than other methods.\n\nNote: The important entities in this document are the backpropagation algorithm, the neural network, and the proposed algorithm Diversely Stale Parameters (DSP). The important dates are not mentioned, as this is a theoretical paper without specific dates. The events happening in this document are the limitations of the backpropagation algorithm and the proposal of a new algorithm to address these limitations. The result of these events is the development of a new algorithm that can train neural networks more efficiently and robustly.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nThe backpropagation algorithm is the most popular algorithm training neural networks nowadays. However, it suffers from the forward locking, backward locking and update locking problems, especially when a neural network is so large that its layers are distributed across multiple devices. Existing solutions either can only handle one locking problem or lead to severe accuracy loss or memory inefficiency. Moreover, none of them consider the straggler problem among devices. In this paper, we propose \\textbf{Layer-wise Staleness} and a novel efficient training algorithm, \\textbf{Diversely Stale Parameters} (DSP), which can address all these challenges without loss of accuracy nor memory issue. We also analyze the convergence of DSP with two popular gradient-based methods and prove that both of them are guaranteed to converge to critical points for non-convex problems. Finally, extensive experimental results on training deep convolutional neural networks demonstrate that our proposed DSP algorithm can achieve significant training speedup with stronger robustness and better generalization than compared methods.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe backpropagation algorithm, a popular method for training neural networks, has some major limitations. When dealing with very large neural networks spread across multiple devices, it can get stuck in problems like \"forward locking,\" \"backward locking,\" and \"update locking,\" which slow down the training process. The authors of this paper propose a new solution called Diversely Stale Parameters (DSP) that can tackle all these challenges without sacrificing accuracy or using too much memory. They also prove that their method is guaranteed to work for non-convex problems and show through experiments that it can speed up the training process while improving the robustness and generalization of the neural network.\n\n(Note: The important entities are neural networks, devices, and algorithms. The important dates are not mentioned in this document as it is a theoretical paper. The events happening in this document are the challenges faced by the backpropagation algorithm and the proposal of a new solution called Diversely Stale Parameters. The result of these events is a new efficient training algorithm that can address the challenges without loss of accuracy or memory issue.)"
        }
    },
    {
        "id": 1114,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nFlies and mice are species separated by 600 million years of evolution, yet have evolved olfactory systems that share many similarities in their anatomic and functional organization. What functions do these shared anatomical and functional features serve, and are they optimal for odor sensing? In this study, we address the optimality of evolutionary design in olfactory circuits by studying artificial neural networks trained to sense odors. We found that artificial neural networks quantitatively recapitulate structures inherent in the olfactory system, including the formation of glomeruli onto a compression layer and sparse and random connectivity onto an expansion layer. Finally, we offer theoretical justifications for each result. Our work offers a framework to explain the evolutionary convergence of olfactory circuits, and gives insight and logic into the anatomic and functional structure of the olfactory system. Over the last two decades, both the anatomic and functional organization of the fly and mouse olfactory systems have been mapped to excruciating detail, affording knowledge of how odors are processed along the entirety of olfactory pathway. In both model organisms, the layout of the olfactory system is two layers deep and comprises of a compression layer and an expansion layer. Olfactory perception is initiated by the recognition of odorants by a large repertoire of receptors in the sensory epithelium (Buck & Axel, 1991) . In fruit flies, individual olfactory receptor neurons (ORNs) express only one of 50 different olfactory receptors (ORs), and all neurons (10 on average) that express the same receptor converge with precision onto a unique set of 2-4 projection neurons (PNs) through a specialized structure known as an olfactory glomerulus (Vosshall, Wong, & Axel, 2000) . This layout establishes a one-to-one mapping between ORs and PNs. Information is then conveyed to an expansion layer of 2,500 Kenyon Cells (KCs) through sparse and random connectivity to support a high dimensional representation of odor information before it is classified by 20 read-out neurons, the mushroom body output neurons (MBONs). Experiments reveal that synaptic plasticity at the KC-MBON synapse is necessary and causal in odor learning. The only major differences between the circuits of mice and flies appear to be numerical. Whereas the fly olfactory system consists of 50 ORs, 50 glomeruli, and 2500 KCs, the mouse olfactory system consists of 1500 ORs, 1500 glomeruli, and 1 million piriform neurons. The fact that evolution has evolved to hardwire the same architecture in flies, mice, and multiple other organisms suggests that such an architecture is optimal for the general task of odor sensing. Although we have a detailed anatomy of the olfactory system in both flies and mice, it is unclear why certain features are optimal for odor sensing. In particular, 1) why does every ORN express a single OR, 2) why is information preserved through a one-to-one mapping between ORs and PNs, and 3) why is connectivity onto the expansion layer sparse and random (Litwin-Kumar, Harris, Axel, Sompolinsky, & Abbott, 2017)? To study optimal circuit design, we use a goal-driven approach to train an artificial neural network to classify odors and then analyze the anatomical and functional structures that emerge after training. This approach has recently been used to study the functional profiles of the ventral stream in visual object processing (Yamins & DiCarlo, 2016) . The simplicity of the fly olfactory circuit and the exhaustive knowledge that we have of its anatomy provides constraints that can be used to gain insight into evolutionary design. We trained artificial neural networks using stochastic gradient descent to classify odors. We found that glomeruli emerged in PN layer, and sparse random connectivity emerges in the PN to KC connections. We then explored the sufficient conditions that enabled these features to emerge. We found that the formation of glomeruli did not depend on input noise but rather on the existence of an expansion layer downstream. In addition, we found that an expansion layer with a synaptic degree of 7 endows the olfactory system with robustness by allowing for large tolerances in synaptic efficacies without affecting task performance. Our work offers a framework to explain the Optimal K predicted by maximal weight robustness (red) and direct training (plus signs). The line is a power-law fit of the red dots.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nFlies and mice have evolved to have similar olfactory systems, despite being separated by 600 million years of evolution. This study explores why these systems are similar and whether they are optimal for sensing odors. Researchers used artificial neural networks to study the evolution of olfactory circuits and found that the networks quantitatively recapitulate the structures of the fly and mouse olfactory systems, including the formation of glomeruli and sparse and random connectivity. The study suggests that the shared features of these systems are optimal for odor sensing, and the architecture of the olfactory system is a result of evolution's optimal design. The researchers also found that the simplicity of the fly olfactory circuit and the exhaustive knowledge of its anatomy provided constraints that could be used to gain insight into evolutionary design.\n\nImportant entities in this document include flies, mice, olfactory systems, artificial neural networks, and odors.\n\nImportant dates mentioned in the document include the last two decades, when the anatomic and functional organization of the fly and mouse olfactory systems were mapped to excruciating detail.\n\nEvents happening in this document include the study of artificial neural networks to classify odors and the analysis of the anatomical and functional structures that emerge after training.\n\nThe result of these events is a framework to explain the evolutionary convergence of olfactory circuits and insight into the anatomic and functional structure of the olfactory system.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nFlies and mice are species separated by 600 million years of evolution, yet have evolved olfactory systems that share many similarities in their anatomic and functional organization. What functions do these shared anatomical and functional features serve, and are they optimal for odor sensing? In this study, we address the optimality of evolutionary design in olfactory circuits by studying artificial neural networks trained to sense odors. We found that artificial neural networks quantitatively recapitulate structures inherent in the olfactory system, including the formation of glomeruli onto a compression layer and sparse and random connectivity onto an expansion layer. Finally, we offer theoretical justifications for each result. Our work offers a framework to explain the evolutionary convergence of olfactory circuits, and gives insight and logic into the anatomic and functional structure of the olfactory system. Over the last two decades, both the anatomic and functional organization of the fly and mouse olfactory systems have been mapped to excruciating detail, affording knowledge of how odors are processed along the entirety of olfactory pathway. In both model organisms, the layout of the olfactory system is two layers deep and comprises of a compression layer and an expansion layer. Olfactory perception is initiated by the recognition of odorants by a large repertoire of receptors in the sensory epithelium (Buck & Axel, 1991) . In fruit flies, individual olfactory receptor neurons (ORNs) express only one of 50 different olfactory receptors (ORs), and all neurons (10 on average) that express the same receptor converge with precision onto a unique set of 2-4 projection neurons (PNs) through a specialized structure known as an olfactory glomerulus (Vosshall, Wong, & Axel, 2000) . This layout establishes a one-to-one mapping between ORs and PNs. Information is then conveyed to an expansion layer of 2,500 Kenyon Cells (KCs) through sparse and random connectivity to support a high dimensional representation of odor information before it is classified by 20 read-out neurons, the mushroom body output neurons (MBONs). Experiments reveal that synaptic plasticity at the KC-MBON synapse is necessary and causal in odor learning. The only major differences between the circuits of mice and flies appear to be numerical. Whereas the fly olfactory system consists of 50 ORs, 50 glomeruli, and 2500 KCs, the mouse olfactory system consists of 1500 ORs, 1500 glomeruli, and 1 million piriform neurons. The fact that evolution has evolved to hardwire the same architecture in flies, mice, and multiple other organisms suggests that such an architecture is optimal for the general task of odor sensing. Although we have a detailed anatomy of the olfactory system in both flies and mice, it is unclear why certain features are optimal for odor sensing. In particular, 1) why does every ORN express a single OR, 2) why is information preserved through a one-to-one mapping between ORs and PNs, and 3) why is connectivity onto the expansion layer sparse and random (Litwin-Kumar, Harris, Axel, Sompolinsky, & Abbott, 2017)? To study optimal circuit design, we use a goal-driven approach to train an artificial neural network to classify odors and then analyze the anatomical and functional structures that emerge after training. This approach has recently been used to study the functional profiles of the ventral stream in visual object processing (Yamins & DiCarlo, 2016) . The simplicity of the fly olfactory circuit and the exhaustive knowledge that we have of its anatomy provides constraints that can be used to gain insight into evolutionary design. We trained artificial neural networks using stochastic gradient descent to classify odors. We found that glomeruli emerged in PN layer, and sparse random connectivity emerges in the PN to KC connections. We then explored the sufficient conditions that enabled these features to emerge. We found that the formation of glomeruli did not depend on input noise but rather on the existence of an expansion layer downstream. In addition, we found that an expansion layer with a synaptic degree of 7 endows the olfactory system with robustness by allowing for large tolerances in synaptic efficacies without affecting task performance. Our work offers a framework to explain the Optimal K predicted by maximal weight robustness (red) and direct training (plus signs). The line is a power-law fit of the red dots.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe study explores the similarities between the olfactory systems of flies and mice, despite being separated by 600 million years of evolution. The researchers used artificial neural networks to understand the optimality of the design of these systems. They found that the networks quantitatively recapitulated the structures of the olfactory system, including the formation of glomeruli and sparse and random connectivity. The study suggests that these features are optimal for odor sensing and that the evolution of similar systems in different species is not a coincidence. The researchers also found that the formation of glomeruli and the sparse and random connectivity in the networks emerged naturally during the training process, without the need for specific input noise. This study provides a framework to explain the evolutionary convergence of olfactory circuits and offers insights into the anatomic and functional structure of the olfactory system.\n\nThe important entities in this document are the flies, mice, and artificial neural networks. The important dates mentioned are the last two decades, during which the anatomic and functional organization of the fly and mouse olfactory systems have been mapped to excruciating detail. The events happening in this document are the training of artificial neural networks to classify odors and the analysis of the anatomical and functional structures that emerge after training. The result of these events is the understanding of the optimality of the design of the olfactory systems and the explanation of the evolutionary convergence of olfactory circuits.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nFlies and mice are species separated by 600 million years of evolution, yet have evolved olfactory systems that share many similarities in their anatomic and functional organization. What functions do these shared anatomical and functional features serve, and are they optimal for odor sensing? In this study, we address the optimality of evolutionary design in olfactory circuits by studying artificial neural networks trained to sense odors. We found that artificial neural networks quantitatively recapitulate structures inherent in the olfactory system, including the formation of glomeruli onto a compression layer and sparse and random connectivity onto an expansion layer. Finally, we offer theoretical justifications for each result. Our work offers a framework to explain the evolutionary convergence of olfactory circuits, and gives insight and logic into the anatomic and functional structure of the olfactory system. Over the last two decades, both the anatomic and functional organization of the fly and mouse olfactory systems have been mapped to excruciating detail, affording knowledge of how odors are processed along the entirety of olfactory pathway. In both model organisms, the layout of the olfactory system is two layers deep and comprises of a compression layer and an expansion layer. Olfactory perception is initiated by the recognition of odorants by a large repertoire of receptors in the sensory epithelium (Buck & Axel, 1991) . In fruit flies, individual olfactory receptor neurons (ORNs) express only one of 50 different olfactory receptors (ORs), and all neurons (10 on average) that express the same receptor converge with precision onto a unique set of 2-4 projection neurons (PNs) through a specialized structure known as an olfactory glomerulus (Vosshall, Wong, & Axel, 2000) . This layout establishes a one-to-one mapping between ORs and PNs. Information is then conveyed to an expansion layer of 2,500 Kenyon Cells (KCs) through sparse and random connectivity to support a high dimensional representation of odor information before it is classified by 20 read-out neurons, the mushroom body output neurons (MBONs). Experiments reveal that synaptic plasticity at the KC-MBON synapse is necessary and causal in odor learning. The only major differences between the circuits of mice and flies appear to be numerical. Whereas the fly olfactory system consists of 50 ORs, 50 glomeruli, and 2500 KCs, the mouse olfactory system consists of 1500 ORs, 1500 glomeruli, and 1 million piriform neurons. The fact that evolution has evolved to hardwire the same architecture in flies, mice, and multiple other organisms suggests that such an architecture is optimal for the general task of odor sensing. Although we have a detailed anatomy of the olfactory system in both flies and mice, it is unclear why certain features are optimal for odor sensing. In particular, 1) why does every ORN express a single OR, 2) why is information preserved through a one-to-one mapping between ORs and PNs, and 3) why is connectivity onto the expansion layer sparse and random (Litwin-Kumar, Harris, Axel, Sompolinsky, & Abbott, 2017)? To study optimal circuit design, we use a goal-driven approach to train an artificial neural network to classify odors and then analyze the anatomical and functional structures that emerge after training. This approach has recently been used to study the functional profiles of the ventral stream in visual object processing (Yamins & DiCarlo, 2016) . The simplicity of the fly olfactory circuit and the exhaustive knowledge that we have of its anatomy provides constraints that can be used to gain insight into evolutionary design. We trained artificial neural networks using stochastic gradient descent to classify odors. We found that glomeruli emerged in PN layer, and sparse random connectivity emerges in the PN to KC connections. We then explored the sufficient conditions that enabled these features to emerge. We found that the formation of glomeruli did not depend on input noise but rather on the existence of an expansion layer downstream. In addition, we found that an expansion layer with a synaptic degree of 7 endows the olfactory system with robustness by allowing for large tolerances in synaptic efficacies without affecting task performance. Our work offers a framework to explain the Optimal K predicted by maximal weight robustness (red) and direct training (plus signs). The line is a power-law fit of the red dots.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document, written for a high school student:\n\nFlies and mice are very different species that have been around for 600 million years, but they both have similar ways of sensing smells. Scientists have studied the parts of their brains that help them do this and found that they have many similarities. They wanted to know why these similarities exist and if they are the best way for animals to sense smells. To answer this, they created artificial neural networks, which are like computer models of the brain, and trained them to recognize smells. They found that the models developed similar structures to the real brains, including the formation of \"glomeruli\" and \"sparse and random\" connections between different parts. They also found that these features are important for the brain to be able to recognize and classify smells correctly. The study suggests that these features are not just a coincidence, but are actually the best way for the brain to process smells.\n\nThe important entities in this document are the flies and mice, which are the species being studied, and the artificial neural networks, which are the computer models of the brain.\n\nThe important dates mentioned in the document are not specific dates, but rather references to studies and research conducted over the last two decades.\n\nThe events happening in this document are the study of the fly and mouse olfactory systems, the creation and training of artificial neural networks, and the analysis of the results.\n\nThe result of these events is a better understanding of how the brain processes smells and why certain features are optimal for odor sensing.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nFlies and mice are species separated by 600 million years of evolution, yet have evolved olfactory systems that share many similarities in their anatomic and functional organization. What functions do these shared anatomical and functional features serve, and are they optimal for odor sensing? In this study, we address the optimality of evolutionary design in olfactory circuits by studying artificial neural networks trained to sense odors. We found that artificial neural networks quantitatively recapitulate structures inherent in the olfactory system, including the formation of glomeruli onto a compression layer and sparse and random connectivity onto an expansion layer. Finally, we offer theoretical justifications for each result. Our work offers a framework to explain the evolutionary convergence of olfactory circuits, and gives insight and logic into the anatomic and functional structure of the olfactory system. Over the last two decades, both the anatomic and functional organization of the fly and mouse olfactory systems have been mapped to excruciating detail, affording knowledge of how odors are processed along the entirety of olfactory pathway. In both model organisms, the layout of the olfactory system is two layers deep and comprises of a compression layer and an expansion layer. Olfactory perception is initiated by the recognition of odorants by a large repertoire of receptors in the sensory epithelium (Buck & Axel, 1991) . In fruit flies, individual olfactory receptor neurons (ORNs) express only one of 50 different olfactory receptors (ORs), and all neurons (10 on average) that express the same receptor converge with precision onto a unique set of 2-4 projection neurons (PNs) through a specialized structure known as an olfactory glomerulus (Vosshall, Wong, & Axel, 2000) . This layout establishes a one-to-one mapping between ORs and PNs. Information is then conveyed to an expansion layer of 2,500 Kenyon Cells (KCs) through sparse and random connectivity to support a high dimensional representation of odor information before it is classified by 20 read-out neurons, the mushroom body output neurons (MBONs). Experiments reveal that synaptic plasticity at the KC-MBON synapse is necessary and causal in odor learning. The only major differences between the circuits of mice and flies appear to be numerical. Whereas the fly olfactory system consists of 50 ORs, 50 glomeruli, and 2500 KCs, the mouse olfactory system consists of 1500 ORs, 1500 glomeruli, and 1 million piriform neurons. The fact that evolution has evolved to hardwire the same architecture in flies, mice, and multiple other organisms suggests that such an architecture is optimal for the general task of odor sensing. Although we have a detailed anatomy of the olfactory system in both flies and mice, it is unclear why certain features are optimal for odor sensing. In particular, 1) why does every ORN express a single OR, 2) why is information preserved through a one-to-one mapping between ORs and PNs, and 3) why is connectivity onto the expansion layer sparse and random (Litwin-Kumar, Harris, Axel, Sompolinsky, & Abbott, 2017)? To study optimal circuit design, we use a goal-driven approach to train an artificial neural network to classify odors and then analyze the anatomical and functional structures that emerge after training. This approach has recently been used to study the functional profiles of the ventral stream in visual object processing (Yamins & DiCarlo, 2016) . The simplicity of the fly olfactory circuit and the exhaustive knowledge that we have of its anatomy provides constraints that can be used to gain insight into evolutionary design. We trained artificial neural networks using stochastic gradient descent to classify odors. We found that glomeruli emerged in PN layer, and sparse random connectivity emerges in the PN to KC connections. We then explored the sufficient conditions that enabled these features to emerge. We found that the formation of glomeruli did not depend on input noise but rather on the existence of an expansion layer downstream. In addition, we found that an expansion layer with a synaptic degree of 7 endows the olfactory system with robustness by allowing for large tolerances in synaptic efficacies without affecting task performance. Our work offers a framework to explain the Optimal K predicted by maximal weight robustness (red) and direct training (plus signs). The line is a power-law fit of the red dots.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe study compares the olfactory systems of flies and mice, which are separated by 600 million years of evolution. Despite this vast difference, their olfactory systems share many similarities in terms of their anatomy and function. The researchers wanted to understand why these similarities exist and whether they are optimal for odor sensing. To do this, they used artificial neural networks to study how they could be designed to sense odors. They found that these networks quantitatively recapitulated the structures found in the olfactory systems of flies and mice, including the formation of glomeruli and sparse and random connectivity. The study suggests that these features may be optimal for odor sensing, and provides a framework for understanding the evolutionary convergence of olfactory circuits.\n\nThe important entities in this document are the olfactory systems of flies and mice, as well as the artificial neural networks used to study them. The important dates mentioned are the last two decades, during which the anatomy and function of the olfactory systems of flies and mice have been mapped in detail.\n\nThe events described in the document include the evolution of the olfactory systems of flies and mice, the development of artificial neural networks to study odor sensing, and the comparison of the structures and functions of the olfactory systems of flies and mice with those of the artificial neural networks. The result of these events is a deeper understanding of the evolutionary convergence of olfactory circuits and the optimal design of odor sensing systems."
        }
    },
    {
        "id": 267,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 7 sentences.\n\nScholarly document\nConvolutional neural networks (CNNs) are commonly trained using a fixed spatial image size predetermined for a given model. Although trained on images of a specific size, it is well established that CNNs can be used to evaluate a wide range of image sizes at test time, by adjusting the size of intermediate feature maps. \n In this work, we describe and evaluate a novel mixed-size training regime that mixes several image sizes at training time. We demonstrate that models trained using our method are more resilient to image size changes and generalize well even on small images. This allows faster inference by using smaller images at test time. For instance, we receive a 76.43% top-1 accuracy using ResNet50 with an image size of 160, which matches the accuracy of the baseline model with 2x fewer computations.\n Furthermore, for a given image size used at test time, we show this method can be exploited either to accelerate training or the final test accuracy. For example, we are able to reach a 79.27% accuracy with a model evaluated at a 288 spatial size for a relative improvement of 14% over the baseline. Figure 1: Test accuracy per image size, models trained on specific sizes (ResNet50, ImageNet). Convolutional neural networks are successfully used to solve various tasks across multiple domains such as visual (Krizhevsky et al., 2012; Ren et al., 2015) , audio (van den Oord et al., 2016) , language (Gehring et al., 2017) and speech (Abdel-Hamid et al., 2014) . While scale-invariance is considered important for visual representations (Lowe, 1999) , convolutional networks are not scale invariant with respect to the spatial resolution of the image input, as a change in image dimension may lead to a non-linear change of their output. Even though CNNs are able to achieve state-of-theart results in many tasks and domains, their sensitivity to the image size is an inherent deficiency that limits practical use cases and requires evaluation inputs to match training image size. For example, Touvron et al. (2019) demonstrated that networks trained on specific image size, perform poorly on other image sizes at evaluation, as shown in Figure 1 . Several works attempted to achieve scale invariance by modifying the network structure (Xu et al., 2014; Takahashi et al., 2017) . However, the most common method is to artificially enlarge the dataset using a set of label-preserving transformations also known as \"data augmentation\" (Krizhevsky et al., 2012; Howard, 2013) . Several of these transformations scale and crop objects appearing within the data, thus increasing the network's robustness to inputs of different scale. Although not explicitly trained to handle varying image sizes, CNNs are commonly evaluated on multiple scales post training, such as in the case of detection (Lin et al., 2017; Redmon & Farhadi, 2018) and segmentation (He et al., 2017) tasks. In these tasks, a network that was pretrained with fixed image size for classification is used as the backbone of a larger model that is expected to adapt to a wide variety of image sizes. In this work, we will introduce a novel training regime, \"MixSize\" for convolutional networks that uses stochastic image and batch sizes. The main contributions of the MixSize regime are: \u2022 Reducing image size sensitivity. We show that the MixSize training regime can improve model performance on a wide range of sizes used at evaluation. \u2022 Faster inference. As our mixed-size models can be evaluated at smaller image sizes, we show up to 2\u00d7 reduction in computations required at inference to reach the same accuracy as the baseline model. \u2022 Faster training vs. high accuracy. We show that reducing the average image size at training leads to a trade-off between the time required to train the model and its final accuracy. 2 RELATED WORK\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document that embeds the answers to the questions within the summary:\n\nConvolutional neural networks (CNNs) are commonly trained on fixed image sizes, but they can be used to evaluate images of various sizes at test time. However, this can be a limitation, as CNNs are sensitive to image size changes. To address this issue, researchers have proposed various methods to make CNNs more scale-invariant, such as using data augmentation or modifying the network structure. In this study, the authors introduce a novel mixed-size training regime called \"MixSize\" that mixes several image sizes at training time. This regime allows CNNs to be more resilient to image size changes and generalize well even on small images. As a result, models trained using MixSize can be evaluated at smaller image sizes, reducing the computational requirements for inference. For example, a ResNet50 model trained using MixSize achieved a top-1 accuracy of 76.43% with an image size of 160, which is comparable to the accuracy of the baseline model with 2x fewer computations. The MixSize regime also allows for a trade-off between training time and final accuracy, making it a promising approach for practical applications.\n\nThe important entities in this document are convolutional neural networks (CNNs), image sizes, and the MixSize training regime. The important dates mentioned are not explicitly stated, but the references provided are from 2012 to 2019. The events described in the document are the development of the MixSize training regime and its evaluation on various image sizes. The result of these events is that the MixSize regime allows CNNs to be more resilient to image size changes and generalize well even on small images, making it a promising approach for practical applications.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 7 sentences.\n\nScholarly document\nConvolutional neural networks (CNNs) are commonly trained using a fixed spatial image size predetermined for a given model. Although trained on images of a specific size, it is well established that CNNs can be used to evaluate a wide range of image sizes at test time, by adjusting the size of intermediate feature maps. \n In this work, we describe and evaluate a novel mixed-size training regime that mixes several image sizes at training time. We demonstrate that models trained using our method are more resilient to image size changes and generalize well even on small images. This allows faster inference by using smaller images at test time. For instance, we receive a 76.43% top-1 accuracy using ResNet50 with an image size of 160, which matches the accuracy of the baseline model with 2x fewer computations.\n Furthermore, for a given image size used at test time, we show this method can be exploited either to accelerate training or the final test accuracy. For example, we are able to reach a 79.27% accuracy with a model evaluated at a 288 spatial size for a relative improvement of 14% over the baseline. Figure 1: Test accuracy per image size, models trained on specific sizes (ResNet50, ImageNet). Convolutional neural networks are successfully used to solve various tasks across multiple domains such as visual (Krizhevsky et al., 2012; Ren et al., 2015) , audio (van den Oord et al., 2016) , language (Gehring et al., 2017) and speech (Abdel-Hamid et al., 2014) . While scale-invariance is considered important for visual representations (Lowe, 1999) , convolutional networks are not scale invariant with respect to the spatial resolution of the image input, as a change in image dimension may lead to a non-linear change of their output. Even though CNNs are able to achieve state-of-theart results in many tasks and domains, their sensitivity to the image size is an inherent deficiency that limits practical use cases and requires evaluation inputs to match training image size. For example, Touvron et al. (2019) demonstrated that networks trained on specific image size, perform poorly on other image sizes at evaluation, as shown in Figure 1 . Several works attempted to achieve scale invariance by modifying the network structure (Xu et al., 2014; Takahashi et al., 2017) . However, the most common method is to artificially enlarge the dataset using a set of label-preserving transformations also known as \"data augmentation\" (Krizhevsky et al., 2012; Howard, 2013) . Several of these transformations scale and crop objects appearing within the data, thus increasing the network's robustness to inputs of different scale. Although not explicitly trained to handle varying image sizes, CNNs are commonly evaluated on multiple scales post training, such as in the case of detection (Lin et al., 2017; Redmon & Farhadi, 2018) and segmentation (He et al., 2017) tasks. In these tasks, a network that was pretrained with fixed image size for classification is used as the backbone of a larger model that is expected to adapt to a wide variety of image sizes. In this work, we will introduce a novel training regime, \"MixSize\" for convolutional networks that uses stochastic image and batch sizes. The main contributions of the MixSize regime are: \u2022 Reducing image size sensitivity. We show that the MixSize training regime can improve model performance on a wide range of sizes used at evaluation. \u2022 Faster inference. As our mixed-size models can be evaluated at smaller image sizes, we show up to 2\u00d7 reduction in computations required at inference to reach the same accuracy as the baseline model. \u2022 Faster training vs. high accuracy. We show that reducing the average image size at training leads to a trade-off between the time required to train the model and its final accuracy. 2 RELATED WORK\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses how Convolutional Neural Networks (CNNs) are typically trained on specific image sizes, but they can still work well on different image sizes at test time. However, this can be a problem because it means that the network is not scale-invariant, meaning it's not robust to changes in image size. To address this issue, researchers have tried different methods, such as artificially increasing the dataset using transformations or modifying the network structure. However, these methods have limitations. In this work, the researchers propose a new method called \"MixSize\" that trains the network on multiple image sizes at the same time. This allows the network to be more resilient to changes in image size and generalize well to small images. As a result, the network can be evaluated at smaller image sizes, which reduces the computational cost and allows for faster inference. For example, the researchers were able to achieve the same level of accuracy as the baseline model with 2x fewer computations. Additionally, the MixSize method can also be used to accelerate training or improve the final accuracy of the model. The researchers demonstrate the effectiveness of the MixSize method on various image sizes and tasks, showing that it can improve model performance and reduce computational cost.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 7 sentences.\n\nScholarly document\nConvolutional neural networks (CNNs) are commonly trained using a fixed spatial image size predetermined for a given model. Although trained on images of a specific size, it is well established that CNNs can be used to evaluate a wide range of image sizes at test time, by adjusting the size of intermediate feature maps. \n In this work, we describe and evaluate a novel mixed-size training regime that mixes several image sizes at training time. We demonstrate that models trained using our method are more resilient to image size changes and generalize well even on small images. This allows faster inference by using smaller images at test time. For instance, we receive a 76.43% top-1 accuracy using ResNet50 with an image size of 160, which matches the accuracy of the baseline model with 2x fewer computations.\n Furthermore, for a given image size used at test time, we show this method can be exploited either to accelerate training or the final test accuracy. For example, we are able to reach a 79.27% accuracy with a model evaluated at a 288 spatial size for a relative improvement of 14% over the baseline. Figure 1: Test accuracy per image size, models trained on specific sizes (ResNet50, ImageNet). Convolutional neural networks are successfully used to solve various tasks across multiple domains such as visual (Krizhevsky et al., 2012; Ren et al., 2015) , audio (van den Oord et al., 2016) , language (Gehring et al., 2017) and speech (Abdel-Hamid et al., 2014) . While scale-invariance is considered important for visual representations (Lowe, 1999) , convolutional networks are not scale invariant with respect to the spatial resolution of the image input, as a change in image dimension may lead to a non-linear change of their output. Even though CNNs are able to achieve state-of-theart results in many tasks and domains, their sensitivity to the image size is an inherent deficiency that limits practical use cases and requires evaluation inputs to match training image size. For example, Touvron et al. (2019) demonstrated that networks trained on specific image size, perform poorly on other image sizes at evaluation, as shown in Figure 1 . Several works attempted to achieve scale invariance by modifying the network structure (Xu et al., 2014; Takahashi et al., 2017) . However, the most common method is to artificially enlarge the dataset using a set of label-preserving transformations also known as \"data augmentation\" (Krizhevsky et al., 2012; Howard, 2013) . Several of these transformations scale and crop objects appearing within the data, thus increasing the network's robustness to inputs of different scale. Although not explicitly trained to handle varying image sizes, CNNs are commonly evaluated on multiple scales post training, such as in the case of detection (Lin et al., 2017; Redmon & Farhadi, 2018) and segmentation (He et al., 2017) tasks. In these tasks, a network that was pretrained with fixed image size for classification is used as the backbone of a larger model that is expected to adapt to a wide variety of image sizes. In this work, we will introduce a novel training regime, \"MixSize\" for convolutional networks that uses stochastic image and batch sizes. The main contributions of the MixSize regime are: \u2022 Reducing image size sensitivity. We show that the MixSize training regime can improve model performance on a wide range of sizes used at evaluation. \u2022 Faster inference. As our mixed-size models can be evaluated at smaller image sizes, we show up to 2\u00d7 reduction in computations required at inference to reach the same accuracy as the baseline model. \u2022 Faster training vs. high accuracy. We show that reducing the average image size at training leads to a trade-off between the time required to train the model and its final accuracy. 2 RELATED WORK\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nConvolutional neural networks (CNNs) are commonly used to analyze images, but they are sensitive to the size of the images they are given. This means that if an image is smaller or larger than the size it was trained on, the CNN might not work as well. Researchers have tried to make CNNs more robust to different image sizes by using techniques like artificially enlarging the training dataset or modifying the network structure. However, these methods have limitations. A new method called \"MixSize\" has been developed, which trains CNNs on multiple image sizes at the same time. This makes the models more resilient to changes in image size and allows them to generalize well even on small images. The MixSize method also enables faster inference, as the models can be evaluated at smaller image sizes, which reduces the number of computations required. Additionally, the method can be used to trade off between training time and accuracy. Overall, the MixSize method is an improvement over previous methods and has the potential to be used in various applications, such as image classification, object detection, and image segmentation.\n\nThe important entities in this document are Convolutional Neural Networks (CNNs), image sizes, and the MixSize method.\n\nThe important dates mentioned in the document are not specific, but they refer to various research papers and studies conducted by different authors.\n\nThe events happening in this document are the development and evaluation of the MixSize method, which is a new training regime for CNNs. The result of these events is the creation of a more robust and efficient method for training CNNs, which can be used in various applications.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 7 sentences.\n\nScholarly document\nConvolutional neural networks (CNNs) are commonly trained using a fixed spatial image size predetermined for a given model. Although trained on images of a specific size, it is well established that CNNs can be used to evaluate a wide range of image sizes at test time, by adjusting the size of intermediate feature maps. \n In this work, we describe and evaluate a novel mixed-size training regime that mixes several image sizes at training time. We demonstrate that models trained using our method are more resilient to image size changes and generalize well even on small images. This allows faster inference by using smaller images at test time. For instance, we receive a 76.43% top-1 accuracy using ResNet50 with an image size of 160, which matches the accuracy of the baseline model with 2x fewer computations.\n Furthermore, for a given image size used at test time, we show this method can be exploited either to accelerate training or the final test accuracy. For example, we are able to reach a 79.27% accuracy with a model evaluated at a 288 spatial size for a relative improvement of 14% over the baseline. Figure 1: Test accuracy per image size, models trained on specific sizes (ResNet50, ImageNet). Convolutional neural networks are successfully used to solve various tasks across multiple domains such as visual (Krizhevsky et al., 2012; Ren et al., 2015) , audio (van den Oord et al., 2016) , language (Gehring et al., 2017) and speech (Abdel-Hamid et al., 2014) . While scale-invariance is considered important for visual representations (Lowe, 1999) , convolutional networks are not scale invariant with respect to the spatial resolution of the image input, as a change in image dimension may lead to a non-linear change of their output. Even though CNNs are able to achieve state-of-theart results in many tasks and domains, their sensitivity to the image size is an inherent deficiency that limits practical use cases and requires evaluation inputs to match training image size. For example, Touvron et al. (2019) demonstrated that networks trained on specific image size, perform poorly on other image sizes at evaluation, as shown in Figure 1 . Several works attempted to achieve scale invariance by modifying the network structure (Xu et al., 2014; Takahashi et al., 2017) . However, the most common method is to artificially enlarge the dataset using a set of label-preserving transformations also known as \"data augmentation\" (Krizhevsky et al., 2012; Howard, 2013) . Several of these transformations scale and crop objects appearing within the data, thus increasing the network's robustness to inputs of different scale. Although not explicitly trained to handle varying image sizes, CNNs are commonly evaluated on multiple scales post training, such as in the case of detection (Lin et al., 2017; Redmon & Farhadi, 2018) and segmentation (He et al., 2017) tasks. In these tasks, a network that was pretrained with fixed image size for classification is used as the backbone of a larger model that is expected to adapt to a wide variety of image sizes. In this work, we will introduce a novel training regime, \"MixSize\" for convolutional networks that uses stochastic image and batch sizes. The main contributions of the MixSize regime are: \u2022 Reducing image size sensitivity. We show that the MixSize training regime can improve model performance on a wide range of sizes used at evaluation. \u2022 Faster inference. As our mixed-size models can be evaluated at smaller image sizes, we show up to 2\u00d7 reduction in computations required at inference to reach the same accuracy as the baseline model. \u2022 Faster training vs. high accuracy. We show that reducing the average image size at training leads to a trade-off between the time required to train the model and its final accuracy. 2 RELATED WORK\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nConvolutional Neural Networks (CNNs) are powerful tools used in many applications, such as image recognition, object detection, and more. However, these networks are typically trained on images of a specific size, which can limit their ability to work well with images of different sizes. Researchers have tried to address this issue by using techniques such as data augmentation, which involves artificially enlarging the dataset by applying transformations to the images. In this study, the researchers propose a new approach called \"MixSize\" that trains the network on multiple image sizes at the same time. They show that this approach improves the network's performance on a wide range of image sizes and allows for faster inference (i.e., processing time) at test time. Additionally, they demonstrate that this approach can be used to trade off between training time and final accuracy. The results show that the MixSize regime can achieve high accuracy with reduced computations, making it a promising approach for real-world applications."
        }
    },
    {
        "id": 1236,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nCapsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce Siamese Capsule Networks, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with l2-normalized capsule encoded pose features. We find that Siamese Capsule Networks perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects. Convolutional Neural networks (CNNs) have been a mainstay model for a wide variety of tasks in computer vision. CNNs are effective at detecting local features in the receptive field, although the spatial relationship between features is lost when crude routing operations are performed to achieve translation invariance, as is the case with max and average pooling. Essentially, pooling results in viewpoint invariance so that small perturbations in the input do not effect the output. This leads to a significant loss of information about the internal properties of present entities (e.g location, orientation, shape and pose) in an image and relationships between them. The issue is usually combated by having large amounts of annotated data from a wide variety of viewpoints, albeit redundant and less efficient in many cases. As noted by hinton1985shape, from a psychology perspective of human shape perception, pooling does not account for the coordinate frames imposed on objects when performing mental rotation to identify handedness BID20 ; BID16 BID10 . Hence, the scalar output activities from local kernel regions that summarize sets of local inputs are not sufficient for preserving reference frames that are used in human perception, since viewpoint information is discarded. Spatial Transformer Networks (STN) BID11 have acknowledged the issue by using dynamic spatial transformations on feature mappings to enhance the geometric invariance of the model, although this approach addresses changes in viewpoint by learning to remove rotational and scale variance, as opposed to viewpoint variance being reflected in the model activations. Instead of addressing translation invariance using pooling operations, BID6 have worked on achieving translation equivariance.The recently proposed Capsule Networks BID21 ; BID5 have shown encouraging results to address these challenges. Thus far, Capsule Networks have only been tested on datasets that have (1) a relatively sufficient number of instances per class to learn from and (2) utilized on tasks in the standard classification setup. This paper extends Capsule Networks to the pairwise learning setting to learn relationships between whole entity encodings, while also demonstrating their ability to learn from little data that can perform few-shot learning where instances from new classes arise during testing (i.e zero-shot prediction). The Siamese Capsule Network is trained using a contrastive loss with 2 -normalized encoded features and demonstrated on two face verification tasks. BID6 first introduced the idea of using whole vectors to represent internal properties (referred to as instantiation parameters that include pose) of an entity with an associated activation probability where each capsule represents a single instance of an entity within in an image. This differs from the single scalar outputs in conventional neural networks where pooling is used as a crude routing operation over filters. Pooling performs sub-sampling so that neurons are invariant to viewpoint change, instead capsules look to preserve the information to achieve equivariance, akin to perceptual systems. Hence, pooling is replaced with a dynamic routing scheme to send lowerlevel capsule (e.g nose, mouth, ears etc.) outputs as input to parent capsule (e.g face) that represent part-whole relationships to achieve translation equivariance and untangles the coordinate frame of an entity through linear transformations. The idea has its roots in computer graphics where images are rendered given an internal hierarchical representation, for this reason the brain is hypothesized to solve an inverse graphics problem where given an image the cortex deconstructs it to its latent hierarchical properties. The original paper by BID21 describes a dynamic routing scheme that represent these internal representations as vectors given a group of designated neurons called capsules, which consist of a pose vector u \u2208 R d and activation \u03b1 \u2208 [0, 1]. The architecture consists of two convolutional layers that are used as the initial input representations for the first capsule layer that are then routed to a final class capsule layer. The initial convolutional layers allow learned knowledge from local feature representations to be reused and replicated in other parts of the receptive field. The capsule inputs are determined using a Iterative Dynamic Routing scheme. A transformation W ij is made to output vector u i of capsule C L i . The length of the vector u i represents the probability that this lower-level capsule detected a given object and the direction corresponds to the state of the object (e.g orientation, position or relationship to upper capsule). The output vector u i is transformed into a prediction vector\u00fb j|i , where\u00fb j|i = W ij u i . Then,\u00fb j|i is weighted by a coupling coefficient c ij to obtain s j = i c ij\u00fbj|i , where coupling coefficients for each capsule j c ij = 1 and c ij is got by log prior probabilities b ij from a sigmoid function, followed by the softmax, c ij = e bij / k e b ik . If\u00fb L j|i has high scalar magnitude when multiplied by u L+1 j then the coupling coefficient c ij is increased and the remaining potential parent capsules coupling coefficients are decreased. Routing By Agreement is then performed using coincidence filtering to find tight clusters of nearby predictions. The entities output vector length is represented as the probability of an entity being present by using the nonlinear normalization shown in Equation 1 where vote v j is the output from total input s j , which is then used to compute the agreement a ij = v j\u00fbj|i that is added to the log prior b ij . This paper has introduced the Siamese Capsule Network, a novel architecture that extends Capsule Networks to the pairwise learning setting with a feature 2 -normalized contrastive loss that maximizes inter-class variance and minimizes intra-class variance. The results indicate Capsule Networks perform better at learning from only few examples and converge faster when a contrastive loss is used that takes face embeddings in the form of encoded capsule pose vectors. We find Siamese Capsule Networks to perform particularly well on the AT&T dataset in the few-shot learning setting, which is tested on unseen classes (i.e subjects) during testing, while competitive against baselines for the larger Labeled Faces In The Wild dataset.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a type of neural network called Capsule Networks, which have shown promising results in computer vision tasks. However, they have not been tested on tasks where the entities (like faces) have complex internal representations and there are few examples to learn from. This paper explores the use of Capsule Networks for face verification in controlled and uncontrolled settings. The authors introduce a new variant called Siamese Capsule Networks, which is trained using a contrastive loss function that maximizes the difference between different classes and minimizes the difference within the same class. The results show that Siamese Capsule Networks perform well on both controlled and uncontrolled datasets, especially in the few-shot learning setting where the model is tested on unseen subjects. The authors also discuss the limitations of traditional Convolutional Neural Networks (CNNs) and how Capsule Networks address these limitations by preserving the spatial relationships between features.\n\nThe important entities in this document are Capsule Networks, Siamese Capsule Networks, and Convolutional Neural Networks (CNNs).\n\nThe important dates mentioned in the document are not specific to a particular time period, but rather refer to the development of Capsule Networks and their applications.\n\nThe events happening in this document are the development and testing of Capsule Networks and Siamese Capsule Networks for face verification tasks.\n\nThe result of these events is the demonstration of the effectiveness of Siamese Capsule Networks in face verification tasks, especially in the few-shot learning setting.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nCapsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce Siamese Capsule Networks, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with l2-normalized capsule encoded pose features. We find that Siamese Capsule Networks perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects. Convolutional Neural networks (CNNs) have been a mainstay model for a wide variety of tasks in computer vision. CNNs are effective at detecting local features in the receptive field, although the spatial relationship between features is lost when crude routing operations are performed to achieve translation invariance, as is the case with max and average pooling. Essentially, pooling results in viewpoint invariance so that small perturbations in the input do not effect the output. This leads to a significant loss of information about the internal properties of present entities (e.g location, orientation, shape and pose) in an image and relationships between them. The issue is usually combated by having large amounts of annotated data from a wide variety of viewpoints, albeit redundant and less efficient in many cases. As noted by hinton1985shape, from a psychology perspective of human shape perception, pooling does not account for the coordinate frames imposed on objects when performing mental rotation to identify handedness BID20 ; BID16 BID10 . Hence, the scalar output activities from local kernel regions that summarize sets of local inputs are not sufficient for preserving reference frames that are used in human perception, since viewpoint information is discarded. Spatial Transformer Networks (STN) BID11 have acknowledged the issue by using dynamic spatial transformations on feature mappings to enhance the geometric invariance of the model, although this approach addresses changes in viewpoint by learning to remove rotational and scale variance, as opposed to viewpoint variance being reflected in the model activations. Instead of addressing translation invariance using pooling operations, BID6 have worked on achieving translation equivariance.The recently proposed Capsule Networks BID21 ; BID5 have shown encouraging results to address these challenges. Thus far, Capsule Networks have only been tested on datasets that have (1) a relatively sufficient number of instances per class to learn from and (2) utilized on tasks in the standard classification setup. This paper extends Capsule Networks to the pairwise learning setting to learn relationships between whole entity encodings, while also demonstrating their ability to learn from little data that can perform few-shot learning where instances from new classes arise during testing (i.e zero-shot prediction). The Siamese Capsule Network is trained using a contrastive loss with 2 -normalized encoded features and demonstrated on two face verification tasks. BID6 first introduced the idea of using whole vectors to represent internal properties (referred to as instantiation parameters that include pose) of an entity with an associated activation probability where each capsule represents a single instance of an entity within in an image. This differs from the single scalar outputs in conventional neural networks where pooling is used as a crude routing operation over filters. Pooling performs sub-sampling so that neurons are invariant to viewpoint change, instead capsules look to preserve the information to achieve equivariance, akin to perceptual systems. Hence, pooling is replaced with a dynamic routing scheme to send lowerlevel capsule (e.g nose, mouth, ears etc.) outputs as input to parent capsule (e.g face) that represent part-whole relationships to achieve translation equivariance and untangles the coordinate frame of an entity through linear transformations. The idea has its roots in computer graphics where images are rendered given an internal hierarchical representation, for this reason the brain is hypothesized to solve an inverse graphics problem where given an image the cortex deconstructs it to its latent hierarchical properties. The original paper by BID21 describes a dynamic routing scheme that represent these internal representations as vectors given a group of designated neurons called capsules, which consist of a pose vector u \u2208 R d and activation \u03b1 \u2208 [0, 1]. The architecture consists of two convolutional layers that are used as the initial input representations for the first capsule layer that are then routed to a final class capsule layer. The initial convolutional layers allow learned knowledge from local feature representations to be reused and replicated in other parts of the receptive field. The capsule inputs are determined using a Iterative Dynamic Routing scheme. A transformation W ij is made to output vector u i of capsule C L i . The length of the vector u i represents the probability that this lower-level capsule detected a given object and the direction corresponds to the state of the object (e.g orientation, position or relationship to upper capsule). The output vector u i is transformed into a prediction vector\u00fb j|i , where\u00fb j|i = W ij u i . Then,\u00fb j|i is weighted by a coupling coefficient c ij to obtain s j = i c ij\u00fbj|i , where coupling coefficients for each capsule j c ij = 1 and c ij is got by log prior probabilities b ij from a sigmoid function, followed by the softmax, c ij = e bij / k e b ik . If\u00fb L j|i has high scalar magnitude when multiplied by u L+1 j then the coupling coefficient c ij is increased and the remaining potential parent capsules coupling coefficients are decreased. Routing By Agreement is then performed using coincidence filtering to find tight clusters of nearby predictions. The entities output vector length is represented as the probability of an entity being present by using the nonlinear normalization shown in Equation 1 where vote v j is the output from total input s j , which is then used to compute the agreement a ij = v j\u00fbj|i that is added to the log prior b ij . This paper has introduced the Siamese Capsule Network, a novel architecture that extends Capsule Networks to the pairwise learning setting with a feature 2 -normalized contrastive loss that maximizes inter-class variance and minimizes intra-class variance. The results indicate Capsule Networks perform better at learning from only few examples and converge faster when a contrastive loss is used that takes face embeddings in the form of encoded capsule pose vectors. We find Siamese Capsule Networks to perform particularly well on the AT&T dataset in the few-shot learning setting, which is tested on unseen classes (i.e subjects) during testing, while competitive against baselines for the larger Labeled Faces In The Wild dataset.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document that embeds answers to the above questions:\n\nThe paper explores the application of Capsule Networks to pairwise learning tasks, particularly face verification, which is a challenging problem due to the complexity of internal representations of entities and the scarcity of instances per class. The authors introduce Siamese Capsule Networks, a new variant of Capsule Networks designed for pairwise learning tasks. They train the model using a contrastive loss with L2-normalized capsule-encoded pose features and find that it performs well on both controlled and uncontrolled settings. The model is particularly effective in the few-shot learning setting, where image pairs in the test set contain unseen subjects.\n\nThe entities in this document are Capsule Networks, Siamese Capsule Networks, and face verification datasets. The important dates are not explicitly mentioned, but the paper is likely to be a recent publication given the references to recent research in computer vision.\n\nThe events happening in this document are the development of Capsule Networks, the introduction of Siamese Capsule Networks, and the experimentation with these models on face verification tasks. The result of these events is the demonstration of the effectiveness of Siamese Capsule Networks in pairwise learning tasks, particularly in the few-shot learning setting.\n\nHere is the summary in 9 sentences:\n\nCapsule Networks have shown promising results on computer vision tasks, but their application to pairwise learning tasks is still unexplored. The authors introduce Siamese Capsule Networks, a new variant designed for pairwise learning tasks, and train it using a contrastive loss with L2-normalized capsule-encoded pose features. The model is tested on face verification tasks in both controlled and uncontrolled settings. The results show that Siamese Capsule Networks perform well on both datasets, particularly in the few-shot learning setting where image pairs in the test set contain unseen subjects. The model is effective in learning from few examples and converges faster when using a contrastive loss. The authors also compare their results to strong baselines and find that Siamese Capsule Networks perform competitively. The paper explores the application of Capsule Networks to pairwise learning tasks and demonstrates the effectiveness of Siamese Capsule Networks in this domain. The results have implications for the development of computer vision models that can learn from few examples and generalize well to unseen classes. Overall, the paper contributes to the advancement of computer vision research and provides a new approach to pairwise learning tasks.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nCapsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce Siamese Capsule Networks, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with l2-normalized capsule encoded pose features. We find that Siamese Capsule Networks perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects. Convolutional Neural networks (CNNs) have been a mainstay model for a wide variety of tasks in computer vision. CNNs are effective at detecting local features in the receptive field, although the spatial relationship between features is lost when crude routing operations are performed to achieve translation invariance, as is the case with max and average pooling. Essentially, pooling results in viewpoint invariance so that small perturbations in the input do not effect the output. This leads to a significant loss of information about the internal properties of present entities (e.g location, orientation, shape and pose) in an image and relationships between them. The issue is usually combated by having large amounts of annotated data from a wide variety of viewpoints, albeit redundant and less efficient in many cases. As noted by hinton1985shape, from a psychology perspective of human shape perception, pooling does not account for the coordinate frames imposed on objects when performing mental rotation to identify handedness BID20 ; BID16 BID10 . Hence, the scalar output activities from local kernel regions that summarize sets of local inputs are not sufficient for preserving reference frames that are used in human perception, since viewpoint information is discarded. Spatial Transformer Networks (STN) BID11 have acknowledged the issue by using dynamic spatial transformations on feature mappings to enhance the geometric invariance of the model, although this approach addresses changes in viewpoint by learning to remove rotational and scale variance, as opposed to viewpoint variance being reflected in the model activations. Instead of addressing translation invariance using pooling operations, BID6 have worked on achieving translation equivariance.The recently proposed Capsule Networks BID21 ; BID5 have shown encouraging results to address these challenges. Thus far, Capsule Networks have only been tested on datasets that have (1) a relatively sufficient number of instances per class to learn from and (2) utilized on tasks in the standard classification setup. This paper extends Capsule Networks to the pairwise learning setting to learn relationships between whole entity encodings, while also demonstrating their ability to learn from little data that can perform few-shot learning where instances from new classes arise during testing (i.e zero-shot prediction). The Siamese Capsule Network is trained using a contrastive loss with 2 -normalized encoded features and demonstrated on two face verification tasks. BID6 first introduced the idea of using whole vectors to represent internal properties (referred to as instantiation parameters that include pose) of an entity with an associated activation probability where each capsule represents a single instance of an entity within in an image. This differs from the single scalar outputs in conventional neural networks where pooling is used as a crude routing operation over filters. Pooling performs sub-sampling so that neurons are invariant to viewpoint change, instead capsules look to preserve the information to achieve equivariance, akin to perceptual systems. Hence, pooling is replaced with a dynamic routing scheme to send lowerlevel capsule (e.g nose, mouth, ears etc.) outputs as input to parent capsule (e.g face) that represent part-whole relationships to achieve translation equivariance and untangles the coordinate frame of an entity through linear transformations. The idea has its roots in computer graphics where images are rendered given an internal hierarchical representation, for this reason the brain is hypothesized to solve an inverse graphics problem where given an image the cortex deconstructs it to its latent hierarchical properties. The original paper by BID21 describes a dynamic routing scheme that represent these internal representations as vectors given a group of designated neurons called capsules, which consist of a pose vector u \u2208 R d and activation \u03b1 \u2208 [0, 1]. The architecture consists of two convolutional layers that are used as the initial input representations for the first capsule layer that are then routed to a final class capsule layer. The initial convolutional layers allow learned knowledge from local feature representations to be reused and replicated in other parts of the receptive field. The capsule inputs are determined using a Iterative Dynamic Routing scheme. A transformation W ij is made to output vector u i of capsule C L i . The length of the vector u i represents the probability that this lower-level capsule detected a given object and the direction corresponds to the state of the object (e.g orientation, position or relationship to upper capsule). The output vector u i is transformed into a prediction vector\u00fb j|i , where\u00fb j|i = W ij u i . Then,\u00fb j|i is weighted by a coupling coefficient c ij to obtain s j = i c ij\u00fbj|i , where coupling coefficients for each capsule j c ij = 1 and c ij is got by log prior probabilities b ij from a sigmoid function, followed by the softmax, c ij = e bij / k e b ik . If\u00fb L j|i has high scalar magnitude when multiplied by u L+1 j then the coupling coefficient c ij is increased and the remaining potential parent capsules coupling coefficients are decreased. Routing By Agreement is then performed using coincidence filtering to find tight clusters of nearby predictions. The entities output vector length is represented as the probability of an entity being present by using the nonlinear normalization shown in Equation 1 where vote v j is the output from total input s j , which is then used to compute the agreement a ij = v j\u00fbj|i that is added to the log prior b ij . This paper has introduced the Siamese Capsule Network, a novel architecture that extends Capsule Networks to the pairwise learning setting with a feature 2 -normalized contrastive loss that maximizes inter-class variance and minimizes intra-class variance. The results indicate Capsule Networks perform better at learning from only few examples and converge faster when a contrastive loss is used that takes face embeddings in the form of encoded capsule pose vectors. We find Siamese Capsule Networks to perform particularly well on the AT&T dataset in the few-shot learning setting, which is tested on unseen classes (i.e subjects) during testing, while competitive against baselines for the larger Labeled Faces In The Wild dataset.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the document for a high school student:\n\nThe document discusses a new type of computer network called Capsule Networks, which are designed to learn from a small number of examples and perform well on tasks where the entities detected have complex internal representations. The authors introduce a new variant called Siamese Capsule Networks, which is designed for pairwise learning tasks, such as face verification. The model is trained using a contrastive loss function that maximizes the difference between classes and minimizes the difference within classes. The authors test the model on two face verification datasets and find that it performs well, especially in the few-shot learning setting where the model is tested on unseen subjects.\n\nThe document also discusses the limitations of traditional convolutional neural networks (CNNs), which are effective at detecting local features but lose information about the internal properties of entities and relationships between them. Capsule Networks address this issue by using a dynamic routing scheme to preserve the information and achieve translation equivariance.\n\nIn summary, the document presents a new type of computer network called Siamese Capsule Networks that is designed for pairwise learning tasks and performs well on face verification datasets, especially in the few-shot learning setting. The model is a variant of Capsule Networks, which address the limitations of traditional CNNs by preserving information about internal properties and relationships between entities.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nCapsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce Siamese Capsule Networks, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with l2-normalized capsule encoded pose features. We find that Siamese Capsule Networks perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects. Convolutional Neural networks (CNNs) have been a mainstay model for a wide variety of tasks in computer vision. CNNs are effective at detecting local features in the receptive field, although the spatial relationship between features is lost when crude routing operations are performed to achieve translation invariance, as is the case with max and average pooling. Essentially, pooling results in viewpoint invariance so that small perturbations in the input do not effect the output. This leads to a significant loss of information about the internal properties of present entities (e.g location, orientation, shape and pose) in an image and relationships between them. The issue is usually combated by having large amounts of annotated data from a wide variety of viewpoints, albeit redundant and less efficient in many cases. As noted by hinton1985shape, from a psychology perspective of human shape perception, pooling does not account for the coordinate frames imposed on objects when performing mental rotation to identify handedness BID20 ; BID16 BID10 . Hence, the scalar output activities from local kernel regions that summarize sets of local inputs are not sufficient for preserving reference frames that are used in human perception, since viewpoint information is discarded. Spatial Transformer Networks (STN) BID11 have acknowledged the issue by using dynamic spatial transformations on feature mappings to enhance the geometric invariance of the model, although this approach addresses changes in viewpoint by learning to remove rotational and scale variance, as opposed to viewpoint variance being reflected in the model activations. Instead of addressing translation invariance using pooling operations, BID6 have worked on achieving translation equivariance.The recently proposed Capsule Networks BID21 ; BID5 have shown encouraging results to address these challenges. Thus far, Capsule Networks have only been tested on datasets that have (1) a relatively sufficient number of instances per class to learn from and (2) utilized on tasks in the standard classification setup. This paper extends Capsule Networks to the pairwise learning setting to learn relationships between whole entity encodings, while also demonstrating their ability to learn from little data that can perform few-shot learning where instances from new classes arise during testing (i.e zero-shot prediction). The Siamese Capsule Network is trained using a contrastive loss with 2 -normalized encoded features and demonstrated on two face verification tasks. BID6 first introduced the idea of using whole vectors to represent internal properties (referred to as instantiation parameters that include pose) of an entity with an associated activation probability where each capsule represents a single instance of an entity within in an image. This differs from the single scalar outputs in conventional neural networks where pooling is used as a crude routing operation over filters. Pooling performs sub-sampling so that neurons are invariant to viewpoint change, instead capsules look to preserve the information to achieve equivariance, akin to perceptual systems. Hence, pooling is replaced with a dynamic routing scheme to send lowerlevel capsule (e.g nose, mouth, ears etc.) outputs as input to parent capsule (e.g face) that represent part-whole relationships to achieve translation equivariance and untangles the coordinate frame of an entity through linear transformations. The idea has its roots in computer graphics where images are rendered given an internal hierarchical representation, for this reason the brain is hypothesized to solve an inverse graphics problem where given an image the cortex deconstructs it to its latent hierarchical properties. The original paper by BID21 describes a dynamic routing scheme that represent these internal representations as vectors given a group of designated neurons called capsules, which consist of a pose vector u \u2208 R d and activation \u03b1 \u2208 [0, 1]. The architecture consists of two convolutional layers that are used as the initial input representations for the first capsule layer that are then routed to a final class capsule layer. The initial convolutional layers allow learned knowledge from local feature representations to be reused and replicated in other parts of the receptive field. The capsule inputs are determined using a Iterative Dynamic Routing scheme. A transformation W ij is made to output vector u i of capsule C L i . The length of the vector u i represents the probability that this lower-level capsule detected a given object and the direction corresponds to the state of the object (e.g orientation, position or relationship to upper capsule). The output vector u i is transformed into a prediction vector\u00fb j|i , where\u00fb j|i = W ij u i . Then,\u00fb j|i is weighted by a coupling coefficient c ij to obtain s j = i c ij\u00fbj|i , where coupling coefficients for each capsule j c ij = 1 and c ij is got by log prior probabilities b ij from a sigmoid function, followed by the softmax, c ij = e bij / k e b ik . If\u00fb L j|i has high scalar magnitude when multiplied by u L+1 j then the coupling coefficient c ij is increased and the remaining potential parent capsules coupling coefficients are decreased. Routing By Agreement is then performed using coincidence filtering to find tight clusters of nearby predictions. The entities output vector length is represented as the probability of an entity being present by using the nonlinear normalization shown in Equation 1 where vote v j is the output from total input s j , which is then used to compute the agreement a ij = v j\u00fbj|i that is added to the log prior b ij . This paper has introduced the Siamese Capsule Network, a novel architecture that extends Capsule Networks to the pairwise learning setting with a feature 2 -normalized contrastive loss that maximizes inter-class variance and minimizes intra-class variance. The results indicate Capsule Networks perform better at learning from only few examples and converge faster when a contrastive loss is used that takes face embeddings in the form of encoded capsule pose vectors. We find Siamese Capsule Networks to perform particularly well on the AT&T dataset in the few-shot learning setting, which is tested on unseen classes (i.e subjects) during testing, while competitive against baselines for the larger Labeled Faces In The Wild dataset.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a new type of neural network called Siamese Capsule Networks, which is designed to learn relationships between whole entities, such as faces, and can perform well even with limited data. The authors of the paper test this new network on two face verification tasks and compare its performance to other state-of-the-art models. The results show that Siamese Capsule Networks can learn from only a few examples and perform well even when tested on unseen classes.\n\nThe document also discusses the limitations of traditional convolutional neural networks (CNNs), which are commonly used in computer vision tasks. CNNs are effective at detecting local features, but they lose spatial information and can't account for the internal properties of objects, such as their pose or orientation. The authors argue that this is a problem because it makes it difficult for the network to understand the relationships between objects.\n\nThe authors propose the use of Capsule Networks, which are a type of neural network that was introduced recently. Capsule Networks are designed to preserve spatial information and can learn to recognize objects even when they are viewed from different angles. The authors modify the Capsule Network architecture to create a Siamese Capsule Network, which is designed specifically for pairwise learning tasks.\n\nThe results of the experiments show that the Siamese Capsule Network performs well on both face verification tasks, especially in the few-shot learning setting, where the network is tested on unseen classes during testing. Overall, the authors conclude that Siamese Capsule Networks are a promising approach for learning relationships between whole entities and can perform well even with limited data.\n\nThe important entities in this document are the Siamese Capsule Networks, Capsule Networks, and Convolutional Neural Networks (CNNs). The important dates are not mentioned in the document. The events happening in this document are the introduction of the Siamese Capsule Network architecture, the testing of this architecture on two face verification tasks, and the comparison of its performance to other state-of-the-art models. The result of these events is the conclusion that Siamese Capsule Networks are a promising approach for learning relationships between whole entities and can perform well even with limited data."
        }
    },
    {
        "id": 1189,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nKnowledge Distillation (KD) is a common method for transferring the ``knowledge'' learned by one machine learning model (the teacher) into another model (the student), where typically, the teacher has a greater capacity (e.g., more parameters or higher bit-widths). To our knowledge, existing methods overlook the fact that although the student absorbs extra knowledge from the teacher, both models share the same input data -- and this data is the only medium by which the teacher's knowledge can be demonstrated. Due to the difference in model capacities, the student may not benefit fully from the same data points on which the teacher is trained. On the other hand, a human teacher may demonstrate a piece of knowledge with individualized examples adapted to a particular student, for instance, in terms of her cultural background and interests. Inspired by this behavior, we design data augmentation agents with distinct roles to facilitate knowledge distillation. Our data augmentation agents generate distinct training data for the teacher and student, respectively. We focus specifically on KD when the teacher network has greater precision (bit-width) than the student network.\n\n We find empirically that specially tailored data points enable the teacher's knowledge to be demonstrated more effectively to the student. We compare our approach with existing KD methods on training popular neural architectures and demonstrate that role-wise data augmentation improves the effectiveness of KD over strong prior approaches. The code for reproducing our results will be made publicly available. Background and Motivation. In the educational psychology literature, it is generally considered beneficial if teachers can adapt curricula based upon students' prior experiences (Bandura, 2002; Brumfiel, 2005; Gurlitt et al., 2006; Slotta & Chi, 2006) . These vary widely depending on students' cultural backgrounds, previous educational experiences, interests, and motivations. To help students with different prior experiences to comprehend, memorise, and consolidate a piece of knowledge, teachers may provide extra and customized teaching material during their teaching processes. For instance, when teaching the concept of the color pink, a teacher may choose flamingos, sakura (cherry blossoms), or ice cream cones as the example, depending on a student's background. Knowledge distillation (KD) (Bucilua et al., 2006; Hinton et al., 2014 ) is a common framework for training machine learning models. It works by transferring knowledge from a higher-capacity teacher model to a lower-capacity student model. Most KD methods can be categorized by how they define the knowledge stored in the teacher (i.e., the \"soft targets\" of training as defined in existing literature). For instance, Hinton et al. (2014) originally proposed KD for neural networks, and they define the output class probabilities (i.e., soft labels) generated by the teacher as the targets for assisting the training of students. In a follow up work, Romero et al. (2015) defined the soft targets via the feature maps in the teacher model's hidden layers. To train a student network with KD effectively, it is important to distill as much knowledge from the teacher as possible. However, previous methods overlook the importance of the medium by which the teacher's knowledge is demonstrated: the training data points. We conjecture that there exist examples, not necessarily seen and ingested by the teacher, that might make it easier for the student to absorb the teacher's knowledge. Blindly adding more training examples may not be beneficial because it may slow down training and introduce unnecessary biases (Ho et al., 2019) . The analogy with how human teachers adjust their teaching to their students' particular situations (e.g., with the feedback gathered from the students during teaching) suggests that a reasonable yet uninvestigated approach might be to augment the training data for both the teacher and student according to distinct policies. In this paper, we study whether and how adaptive data augmentation and knowledge distillation can be leveraged synergistically to better train student networks. We propose a two-stage, rolewise data augmentation process for KD. This process consists of: (1) training a teacher network till convergence while learning a schedule of policies to augment the training data specifically for the teacher; (2) distilling the knowledge from the teacher into a student network while learning another schedule of policies to augment the training data specifically for the student. It is worth noting that this two-stage framework is orthogonal to existing methods for KD, which focus on how the knowledge to be distilled is defined; thus, our approach can be combined with previous methods straighforwardly. Although our proposed method can in principle be applied to any models trained via KD, we focus specifically on how to use it to transfer the knowledge from a full-precision teacher network into a student network with lower bit-width. Network quantization is crucial when deploying trained models on embedded devices, or in data centers to reduce energy consumption (Strubell et al., 2019) . KD-based quantization (Zhuang et al., 2018; Polino et al., 2018) jointly trains a full-precision model, which acts as the teacher, alongside a low-precision model, which acts as the student. Previous work has shown that distilling a full-precision teacher's knowledge into a low-precision student, followed by fine-tuning, incurs noticeable performance degradation, especially when the bit-widths are below four (Zhuang et al., 2018; Polino et al., 2018) . We show that it is advantageous to use adaptive data augmentation to generate more training data for the low-precision network based on its specific weaknesses. For example, low-precision networks may have difficulties learning rotationrelated patterns, 1 and the data augmentation agent should be aware of this and generate more such data points. One positive side-effect for demonstrating the effectiveness of our method is that the improvement brought by our proposed method is more significant compared to the experiments on all full-precision models.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a method called Knowledge Distillation (KD) that helps transfer knowledge from a more advanced machine learning model (the teacher) to a less advanced model (the student). The teacher model has more capacity to learn and process information, but the student model may not benefit fully from the same data points that the teacher is trained on. The authors of the document propose a new approach to KD, inspired by how human teachers adapt their teaching to individual students' backgrounds and interests. They design data augmentation agents that generate distinct training data for the teacher and student models, respectively. The agents learn to create customized data points that help the teacher demonstrate its knowledge more effectively to the student. The authors test their approach on popular neural architectures and show that it improves the effectiveness of KD over existing methods. They also discuss the importance of adaptive data augmentation in knowledge distillation, particularly when the teacher model has higher precision (bit-width) than the student model.\n\nThe important entities in this document are the machine learning models (teacher and student), the data augmentation agents, and the neural architectures.\n\nThe important dates mentioned in the document are not explicitly stated, but the references provided are from 2002 to 2019, indicating the time period in which the research was conducted.\n\nThe events happening in this document are the development of the Knowledge Distillation method, the design of data augmentation agents, and the testing of the approach on various neural architectures.\n\nThe result of these events is the improvement of the effectiveness of Knowledge Distillation, enabling the transfer of knowledge from a more advanced model to a less advanced model, which is particularly important for deploying trained models on embedded devices or in data centers to reduce energy consumption.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nKnowledge Distillation (KD) is a common method for transferring the ``knowledge'' learned by one machine learning model (the teacher) into another model (the student), where typically, the teacher has a greater capacity (e.g., more parameters or higher bit-widths). To our knowledge, existing methods overlook the fact that although the student absorbs extra knowledge from the teacher, both models share the same input data -- and this data is the only medium by which the teacher's knowledge can be demonstrated. Due to the difference in model capacities, the student may not benefit fully from the same data points on which the teacher is trained. On the other hand, a human teacher may demonstrate a piece of knowledge with individualized examples adapted to a particular student, for instance, in terms of her cultural background and interests. Inspired by this behavior, we design data augmentation agents with distinct roles to facilitate knowledge distillation. Our data augmentation agents generate distinct training data for the teacher and student, respectively. We focus specifically on KD when the teacher network has greater precision (bit-width) than the student network.\n\n We find empirically that specially tailored data points enable the teacher's knowledge to be demonstrated more effectively to the student. We compare our approach with existing KD methods on training popular neural architectures and demonstrate that role-wise data augmentation improves the effectiveness of KD over strong prior approaches. The code for reproducing our results will be made publicly available. Background and Motivation. In the educational psychology literature, it is generally considered beneficial if teachers can adapt curricula based upon students' prior experiences (Bandura, 2002; Brumfiel, 2005; Gurlitt et al., 2006; Slotta & Chi, 2006) . These vary widely depending on students' cultural backgrounds, previous educational experiences, interests, and motivations. To help students with different prior experiences to comprehend, memorise, and consolidate a piece of knowledge, teachers may provide extra and customized teaching material during their teaching processes. For instance, when teaching the concept of the color pink, a teacher may choose flamingos, sakura (cherry blossoms), or ice cream cones as the example, depending on a student's background. Knowledge distillation (KD) (Bucilua et al., 2006; Hinton et al., 2014 ) is a common framework for training machine learning models. It works by transferring knowledge from a higher-capacity teacher model to a lower-capacity student model. Most KD methods can be categorized by how they define the knowledge stored in the teacher (i.e., the \"soft targets\" of training as defined in existing literature). For instance, Hinton et al. (2014) originally proposed KD for neural networks, and they define the output class probabilities (i.e., soft labels) generated by the teacher as the targets for assisting the training of students. In a follow up work, Romero et al. (2015) defined the soft targets via the feature maps in the teacher model's hidden layers. To train a student network with KD effectively, it is important to distill as much knowledge from the teacher as possible. However, previous methods overlook the importance of the medium by which the teacher's knowledge is demonstrated: the training data points. We conjecture that there exist examples, not necessarily seen and ingested by the teacher, that might make it easier for the student to absorb the teacher's knowledge. Blindly adding more training examples may not be beneficial because it may slow down training and introduce unnecessary biases (Ho et al., 2019) . The analogy with how human teachers adjust their teaching to their students' particular situations (e.g., with the feedback gathered from the students during teaching) suggests that a reasonable yet uninvestigated approach might be to augment the training data for both the teacher and student according to distinct policies. In this paper, we study whether and how adaptive data augmentation and knowledge distillation can be leveraged synergistically to better train student networks. We propose a two-stage, rolewise data augmentation process for KD. This process consists of: (1) training a teacher network till convergence while learning a schedule of policies to augment the training data specifically for the teacher; (2) distilling the knowledge from the teacher into a student network while learning another schedule of policies to augment the training data specifically for the student. It is worth noting that this two-stage framework is orthogonal to existing methods for KD, which focus on how the knowledge to be distilled is defined; thus, our approach can be combined with previous methods straighforwardly. Although our proposed method can in principle be applied to any models trained via KD, we focus specifically on how to use it to transfer the knowledge from a full-precision teacher network into a student network with lower bit-width. Network quantization is crucial when deploying trained models on embedded devices, or in data centers to reduce energy consumption (Strubell et al., 2019) . KD-based quantization (Zhuang et al., 2018; Polino et al., 2018) jointly trains a full-precision model, which acts as the teacher, alongside a low-precision model, which acts as the student. Previous work has shown that distilling a full-precision teacher's knowledge into a low-precision student, followed by fine-tuning, incurs noticeable performance degradation, especially when the bit-widths are below four (Zhuang et al., 2018; Polino et al., 2018) . We show that it is advantageous to use adaptive data augmentation to generate more training data for the low-precision network based on its specific weaknesses. For example, low-precision networks may have difficulties learning rotationrelated patterns, 1 and the data augmentation agent should be aware of this and generate more such data points. One positive side-effect for demonstrating the effectiveness of our method is that the improvement brought by our proposed method is more significant compared to the experiments on all full-precision models.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe article discusses a new approach to teaching machine learning models, called Knowledge Distillation (KD). In KD, a more powerful model (the teacher) teaches a less powerful model (the student) by sharing its knowledge. However, the teacher and student models are trained on the same data, which can make it difficult for the student to learn from the teacher. The authors propose a new method called data augmentation agents, which create different training data for the teacher and student. This helps the teacher to demonstrate its knowledge more effectively to the student. The authors tested their approach on popular neural networks and found that it improved the effectiveness of KD. They also compared their method to existing approaches and found that it outperformed them.\n\nThe important entities in this document are the machine learning models, the teacher and student, and the data augmentation agents.\n\nThe important dates mentioned in the document are not explicitly stated, but the authors reference several research papers and studies from the 2000s and 2010s.\n\nThe events happening in this document are the development of the Knowledge Distillation method, the creation of data augmentation agents, and the testing of these methods on various neural networks.\n\nThe result of these events is the development of a new and improved method for teaching machine learning models, which can help to improve the performance of these models and enable them to learn more effectively.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nKnowledge Distillation (KD) is a common method for transferring the ``knowledge'' learned by one machine learning model (the teacher) into another model (the student), where typically, the teacher has a greater capacity (e.g., more parameters or higher bit-widths). To our knowledge, existing methods overlook the fact that although the student absorbs extra knowledge from the teacher, both models share the same input data -- and this data is the only medium by which the teacher's knowledge can be demonstrated. Due to the difference in model capacities, the student may not benefit fully from the same data points on which the teacher is trained. On the other hand, a human teacher may demonstrate a piece of knowledge with individualized examples adapted to a particular student, for instance, in terms of her cultural background and interests. Inspired by this behavior, we design data augmentation agents with distinct roles to facilitate knowledge distillation. Our data augmentation agents generate distinct training data for the teacher and student, respectively. We focus specifically on KD when the teacher network has greater precision (bit-width) than the student network.\n\n We find empirically that specially tailored data points enable the teacher's knowledge to be demonstrated more effectively to the student. We compare our approach with existing KD methods on training popular neural architectures and demonstrate that role-wise data augmentation improves the effectiveness of KD over strong prior approaches. The code for reproducing our results will be made publicly available. Background and Motivation. In the educational psychology literature, it is generally considered beneficial if teachers can adapt curricula based upon students' prior experiences (Bandura, 2002; Brumfiel, 2005; Gurlitt et al., 2006; Slotta & Chi, 2006) . These vary widely depending on students' cultural backgrounds, previous educational experiences, interests, and motivations. To help students with different prior experiences to comprehend, memorise, and consolidate a piece of knowledge, teachers may provide extra and customized teaching material during their teaching processes. For instance, when teaching the concept of the color pink, a teacher may choose flamingos, sakura (cherry blossoms), or ice cream cones as the example, depending on a student's background. Knowledge distillation (KD) (Bucilua et al., 2006; Hinton et al., 2014 ) is a common framework for training machine learning models. It works by transferring knowledge from a higher-capacity teacher model to a lower-capacity student model. Most KD methods can be categorized by how they define the knowledge stored in the teacher (i.e., the \"soft targets\" of training as defined in existing literature). For instance, Hinton et al. (2014) originally proposed KD for neural networks, and they define the output class probabilities (i.e., soft labels) generated by the teacher as the targets for assisting the training of students. In a follow up work, Romero et al. (2015) defined the soft targets via the feature maps in the teacher model's hidden layers. To train a student network with KD effectively, it is important to distill as much knowledge from the teacher as possible. However, previous methods overlook the importance of the medium by which the teacher's knowledge is demonstrated: the training data points. We conjecture that there exist examples, not necessarily seen and ingested by the teacher, that might make it easier for the student to absorb the teacher's knowledge. Blindly adding more training examples may not be beneficial because it may slow down training and introduce unnecessary biases (Ho et al., 2019) . The analogy with how human teachers adjust their teaching to their students' particular situations (e.g., with the feedback gathered from the students during teaching) suggests that a reasonable yet uninvestigated approach might be to augment the training data for both the teacher and student according to distinct policies. In this paper, we study whether and how adaptive data augmentation and knowledge distillation can be leveraged synergistically to better train student networks. We propose a two-stage, rolewise data augmentation process for KD. This process consists of: (1) training a teacher network till convergence while learning a schedule of policies to augment the training data specifically for the teacher; (2) distilling the knowledge from the teacher into a student network while learning another schedule of policies to augment the training data specifically for the student. It is worth noting that this two-stage framework is orthogonal to existing methods for KD, which focus on how the knowledge to be distilled is defined; thus, our approach can be combined with previous methods straighforwardly. Although our proposed method can in principle be applied to any models trained via KD, we focus specifically on how to use it to transfer the knowledge from a full-precision teacher network into a student network with lower bit-width. Network quantization is crucial when deploying trained models on embedded devices, or in data centers to reduce energy consumption (Strubell et al., 2019) . KD-based quantization (Zhuang et al., 2018; Polino et al., 2018) jointly trains a full-precision model, which acts as the teacher, alongside a low-precision model, which acts as the student. Previous work has shown that distilling a full-precision teacher's knowledge into a low-precision student, followed by fine-tuning, incurs noticeable performance degradation, especially when the bit-widths are below four (Zhuang et al., 2018; Polino et al., 2018) . We show that it is advantageous to use adaptive data augmentation to generate more training data for the low-precision network based on its specific weaknesses. For example, low-precision networks may have difficulties learning rotationrelated patterns, 1 and the data augmentation agent should be aware of this and generate more such data points. One positive side-effect for demonstrating the effectiveness of our method is that the improvement brought by our proposed method is more significant compared to the experiments on all full-precision models.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a method called Knowledge Distillation (KD) where a more advanced machine learning model (the teacher) helps train a less advanced model (the student). The teacher has more parameters or higher bit-widths, which means it can learn more complex information. However, the student may not fully benefit from the same data points that the teacher was trained on. Inspired by how human teachers adapt their teaching to individual students, the authors propose a new approach called data augmentation agents. These agents generate distinct training data for the teacher and student, taking into account the student's specific strengths and weaknesses. The authors tested this approach on popular neural networks and found that it improved the effectiveness of KD, especially when the teacher has a higher precision than the student. The authors also note that this approach can be used to train models with lower bit-widths, which is important for deploying models on devices with limited resources.\n\nThe important entities in this document are the machine learning models (teacher and student), data augmentation agents, and neural networks. The important dates mentioned are not specific to this document, but rather refer to the dates of previous studies and publications.\n\nThe events happening in this document are the development of the data augmentation agents, the testing of the approach on popular neural networks, and the comparison of the results with previous methods. The result of these events is the improvement in the effectiveness of Knowledge Distillation and the ability to train models with lower bit-widths.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nKnowledge Distillation (KD) is a common method for transferring the ``knowledge'' learned by one machine learning model (the teacher) into another model (the student), where typically, the teacher has a greater capacity (e.g., more parameters or higher bit-widths). To our knowledge, existing methods overlook the fact that although the student absorbs extra knowledge from the teacher, both models share the same input data -- and this data is the only medium by which the teacher's knowledge can be demonstrated. Due to the difference in model capacities, the student may not benefit fully from the same data points on which the teacher is trained. On the other hand, a human teacher may demonstrate a piece of knowledge with individualized examples adapted to a particular student, for instance, in terms of her cultural background and interests. Inspired by this behavior, we design data augmentation agents with distinct roles to facilitate knowledge distillation. Our data augmentation agents generate distinct training data for the teacher and student, respectively. We focus specifically on KD when the teacher network has greater precision (bit-width) than the student network.\n\n We find empirically that specially tailored data points enable the teacher's knowledge to be demonstrated more effectively to the student. We compare our approach with existing KD methods on training popular neural architectures and demonstrate that role-wise data augmentation improves the effectiveness of KD over strong prior approaches. The code for reproducing our results will be made publicly available. Background and Motivation. In the educational psychology literature, it is generally considered beneficial if teachers can adapt curricula based upon students' prior experiences (Bandura, 2002; Brumfiel, 2005; Gurlitt et al., 2006; Slotta & Chi, 2006) . These vary widely depending on students' cultural backgrounds, previous educational experiences, interests, and motivations. To help students with different prior experiences to comprehend, memorise, and consolidate a piece of knowledge, teachers may provide extra and customized teaching material during their teaching processes. For instance, when teaching the concept of the color pink, a teacher may choose flamingos, sakura (cherry blossoms), or ice cream cones as the example, depending on a student's background. Knowledge distillation (KD) (Bucilua et al., 2006; Hinton et al., 2014 ) is a common framework for training machine learning models. It works by transferring knowledge from a higher-capacity teacher model to a lower-capacity student model. Most KD methods can be categorized by how they define the knowledge stored in the teacher (i.e., the \"soft targets\" of training as defined in existing literature). For instance, Hinton et al. (2014) originally proposed KD for neural networks, and they define the output class probabilities (i.e., soft labels) generated by the teacher as the targets for assisting the training of students. In a follow up work, Romero et al. (2015) defined the soft targets via the feature maps in the teacher model's hidden layers. To train a student network with KD effectively, it is important to distill as much knowledge from the teacher as possible. However, previous methods overlook the importance of the medium by which the teacher's knowledge is demonstrated: the training data points. We conjecture that there exist examples, not necessarily seen and ingested by the teacher, that might make it easier for the student to absorb the teacher's knowledge. Blindly adding more training examples may not be beneficial because it may slow down training and introduce unnecessary biases (Ho et al., 2019) . The analogy with how human teachers adjust their teaching to their students' particular situations (e.g., with the feedback gathered from the students during teaching) suggests that a reasonable yet uninvestigated approach might be to augment the training data for both the teacher and student according to distinct policies. In this paper, we study whether and how adaptive data augmentation and knowledge distillation can be leveraged synergistically to better train student networks. We propose a two-stage, rolewise data augmentation process for KD. This process consists of: (1) training a teacher network till convergence while learning a schedule of policies to augment the training data specifically for the teacher; (2) distilling the knowledge from the teacher into a student network while learning another schedule of policies to augment the training data specifically for the student. It is worth noting that this two-stage framework is orthogonal to existing methods for KD, which focus on how the knowledge to be distilled is defined; thus, our approach can be combined with previous methods straighforwardly. Although our proposed method can in principle be applied to any models trained via KD, we focus specifically on how to use it to transfer the knowledge from a full-precision teacher network into a student network with lower bit-width. Network quantization is crucial when deploying trained models on embedded devices, or in data centers to reduce energy consumption (Strubell et al., 2019) . KD-based quantization (Zhuang et al., 2018; Polino et al., 2018) jointly trains a full-precision model, which acts as the teacher, alongside a low-precision model, which acts as the student. Previous work has shown that distilling a full-precision teacher's knowledge into a low-precision student, followed by fine-tuning, incurs noticeable performance degradation, especially when the bit-widths are below four (Zhuang et al., 2018; Polino et al., 2018) . We show that it is advantageous to use adaptive data augmentation to generate more training data for the low-precision network based on its specific weaknesses. For example, low-precision networks may have difficulties learning rotationrelated patterns, 1 and the data augmentation agent should be aware of this and generate more such data points. One positive side-effect for demonstrating the effectiveness of our method is that the improvement brought by our proposed method is more significant compared to the experiments on all full-precision models.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nImagine you're trying to learn a new concept, like a new language or a new skill. A good teacher would tailor their teaching to your individual needs and background, right? For example, if you're interested in music, they might use music-related examples to help you understand the concept better. In the same way, when machine learning models are trying to learn from each other, it would be helpful if they could tailor their teaching to the specific needs of the student model. This is exactly what the researchers in this study did. They designed a new approach called \"data augmentation agents\" that generate training data specifically for the teacher and student models, taking into account the differences in their capacities and abilities. By doing so, they found that the student model can learn more effectively from the teacher model, and the results are more accurate. This approach is particularly useful when the teacher model has a higher capacity (more parameters or higher bit-widths) than the student model. The researchers tested their approach on popular neural architectures and found that it improves the effectiveness of knowledge distillation, which is a common method for transferring knowledge from one model to another."
        }
    },
    {
        "id": 1240,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nAn unintended consequence of feature sharing is the model fitting to correlated tasks within the dataset, termed negative transfer.   In this paper, we revisit the problem of negative transfer in multitask setting and find that its corrosive effects are applicable to a wide range of linear and non-linear models, including neural networks. We first study the effects of negative transfer in a principled way and show that previously proposed counter-measures are insufficient, particularly for trainable features. We propose an adversarial training approach to mitigate the effects of negative transfer by viewing the problem in a domain adaptation setting. Finally, empirical results on attribute prediction multi-task on AWA and CUB datasets further validate the need for correcting negative sharing in an end-to-end manner. Advances in machine learning have led to proficient supervised learning models with powerful representations in various prediction tasks. We now expect an ideal classification model to restrict itself to a pertinent set of evidences available to it from the input for prediction. Further, we expect the model to disregard any unrelated evidences in the data to enable better generalization. Figure 1: A supervised classifier 'cheetah vs. snow-leopard' that uses unrelated evidence (of habitat) over relevant evidence (of fur patterns). As shown by the pixel importance maps, the model suffers from the negative transfer prevalent in a typical animal image dataset skewed towards the animal's typical habitat and fails to generalize to rare samples.Let us consider the task of training an animal classifier \"cheetah vs. snow-leopards\" from a dataset of images of these animals, such as those illustrated in Figure 1 -a task which ideally should focus on the animal's appearance features. However, a large portion of these images also contain various cues of the typical habitat of the animals in the background, i.e., tall grass and snow (see Figures 1 (a) and (b)) which are, in principle, unrelated to the animal's appearance. An archetypal model is deceived by the co-occurrence of such unrelated, yet easily detectable cues of habitat over the animal's appearance features such as complex fur patterns. However, a proficient supervised learning model must identify relevant evidences for the label of interest and at the same time discard various unrelated evidences such as presence of snow, even though it tends to co-occur frequently with snow-leopard. Consequently, it would be more likely that such a model would perform better on rare-instances (such as those in Figures 1 (c) and (d)) and generalize better to unseen instances.This phenomenon of co-occurring but unrelated evidences being present in training data and thereby having a debilitating effect on model performance has been described in literature BID8 ; BID16 ; BID9 ; BID13 ; BID15 ). These techniques utilize the easy availability of labels for unrelated evidences (e.g. background habitat labels above), called negative labels which constitutes an auxiliary task, and seek to mitigate its debilitating performance on the primary task (e.g. animal classification above) with techniques referred to as negative-sharing or negative-transfer.While all of these techniques have tackled this problem utilizing various forms of regularization, we describe several shortcomings of this class of approaches, most notable of which is their inapplicability to the popular paradigm of trainable features obtained via neural representation learning. Motivated by these limitations, in this paper we depart from the direction of regularization-based approaches and examine methods inspired from a domain-adaptation viewpoint to propose an adversarial training-based formulation. We uniquely view such a scenario as an instance of adversarial multi-task learning, where the classification tasks are either the primary task of interest (i.e., predicting the presence of fur pattern and color) or the auxiliary negative tasks (i.e., characteristics of habitat) to be avoided. Since the 2 tasks are unrelated, any label correlation between primary and auxiliary labels in the training data is only by chance and therefore from a domain-adaptation perspective, we envision a target-domain as possibly having a different correlation between the primary and auxiliary labels. The effects of negative transfer are hence mitigated when the classification task is trained in this domain.We discuss advantages of our proposed formulation, inspired from domain-adaptation, to alleviate the negative transfer over existing techniques, including ready applicability to neural networks in an end-to-end fashion. It must be noted that, while the formulation of the problem is motivated with multi-task learning, negative-transfer is a disposition of any supervised learning task from simple binary classification to recent popular supervised tasks such as image detection, captioning, or visual dialog. We present motivating literature that prelude this work next. In this work, we show that adversarial learning is the natural answer to prevent negative transfer. This leads to potential improvement in any supervised learning of natural data that is seeking generalization. We find that even in relatively straight-forward linear models presented above, co-occurrence of unrelated labels hampers performance and must be explicitly treated. We address the problem of negative transfer in a multi-task scenario, and also show the applicability of our solution in any supervised task. Supervised learning practitioners can utilize domain expertise to acquire and leverage additional negative labels for this purpose. Recent work in explainability of machine learning models can also be appropriately leveraged to facilitate this task.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a problem in machine learning called \"negative transfer,\" where a model learns to focus on irrelevant information in the training data instead of the relevant information. This can happen when there are multiple tasks or features in the data that are related to each other, but not directly related to the task the model is trying to learn. The document shows that this problem can occur even with simple models, such as linear models, and that it can be a major obstacle to achieving good performance on the task.\n\nThe authors of the document propose a new approach to mitigate negative transfer, called adversarial training, which involves training the model to ignore the irrelevant information. They show that this approach can be effective in a variety of tasks, including image classification and object detection.\n\nThe document also discusses the importance of domain expertise and explainability in machine learning, and how these can be used to help mitigate negative transfer.\n\nOverall, the document highlights the importance of careful consideration of the training data and the need for robust models that can generalize well to new data.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nAn unintended consequence of feature sharing is the model fitting to correlated tasks within the dataset, termed negative transfer.   In this paper, we revisit the problem of negative transfer in multitask setting and find that its corrosive effects are applicable to a wide range of linear and non-linear models, including neural networks. We first study the effects of negative transfer in a principled way and show that previously proposed counter-measures are insufficient, particularly for trainable features. We propose an adversarial training approach to mitigate the effects of negative transfer by viewing the problem in a domain adaptation setting. Finally, empirical results on attribute prediction multi-task on AWA and CUB datasets further validate the need for correcting negative sharing in an end-to-end manner. Advances in machine learning have led to proficient supervised learning models with powerful representations in various prediction tasks. We now expect an ideal classification model to restrict itself to a pertinent set of evidences available to it from the input for prediction. Further, we expect the model to disregard any unrelated evidences in the data to enable better generalization. Figure 1: A supervised classifier 'cheetah vs. snow-leopard' that uses unrelated evidence (of habitat) over relevant evidence (of fur patterns). As shown by the pixel importance maps, the model suffers from the negative transfer prevalent in a typical animal image dataset skewed towards the animal's typical habitat and fails to generalize to rare samples.Let us consider the task of training an animal classifier \"cheetah vs. snow-leopards\" from a dataset of images of these animals, such as those illustrated in Figure 1 -a task which ideally should focus on the animal's appearance features. However, a large portion of these images also contain various cues of the typical habitat of the animals in the background, i.e., tall grass and snow (see Figures 1 (a) and (b)) which are, in principle, unrelated to the animal's appearance. An archetypal model is deceived by the co-occurrence of such unrelated, yet easily detectable cues of habitat over the animal's appearance features such as complex fur patterns. However, a proficient supervised learning model must identify relevant evidences for the label of interest and at the same time discard various unrelated evidences such as presence of snow, even though it tends to co-occur frequently with snow-leopard. Consequently, it would be more likely that such a model would perform better on rare-instances (such as those in Figures 1 (c) and (d)) and generalize better to unseen instances.This phenomenon of co-occurring but unrelated evidences being present in training data and thereby having a debilitating effect on model performance has been described in literature BID8 ; BID16 ; BID9 ; BID13 ; BID15 ). These techniques utilize the easy availability of labels for unrelated evidences (e.g. background habitat labels above), called negative labels which constitutes an auxiliary task, and seek to mitigate its debilitating performance on the primary task (e.g. animal classification above) with techniques referred to as negative-sharing or negative-transfer.While all of these techniques have tackled this problem utilizing various forms of regularization, we describe several shortcomings of this class of approaches, most notable of which is their inapplicability to the popular paradigm of trainable features obtained via neural representation learning. Motivated by these limitations, in this paper we depart from the direction of regularization-based approaches and examine methods inspired from a domain-adaptation viewpoint to propose an adversarial training-based formulation. We uniquely view such a scenario as an instance of adversarial multi-task learning, where the classification tasks are either the primary task of interest (i.e., predicting the presence of fur pattern and color) or the auxiliary negative tasks (i.e., characteristics of habitat) to be avoided. Since the 2 tasks are unrelated, any label correlation between primary and auxiliary labels in the training data is only by chance and therefore from a domain-adaptation perspective, we envision a target-domain as possibly having a different correlation between the primary and auxiliary labels. The effects of negative transfer are hence mitigated when the classification task is trained in this domain.We discuss advantages of our proposed formulation, inspired from domain-adaptation, to alleviate the negative transfer over existing techniques, including ready applicability to neural networks in an end-to-end fashion. It must be noted that, while the formulation of the problem is motivated with multi-task learning, negative-transfer is a disposition of any supervised learning task from simple binary classification to recent popular supervised tasks such as image detection, captioning, or visual dialog. We present motivating literature that prelude this work next. In this work, we show that adversarial learning is the natural answer to prevent negative transfer. This leads to potential improvement in any supervised learning of natural data that is seeking generalization. We find that even in relatively straight-forward linear models presented above, co-occurrence of unrelated labels hampers performance and must be explicitly treated. We address the problem of negative transfer in a multi-task scenario, and also show the applicability of our solution in any supervised task. Supervised learning practitioners can utilize domain expertise to acquire and leverage additional negative labels for this purpose. Recent work in explainability of machine learning models can also be appropriately leveraged to facilitate this task.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nIn machine learning, a problem called \"negative transfer\" occurs when a model is trained on multiple tasks and becomes biased towards one task over others. This can lead to poor performance on tasks that are not well-represented in the training data. The authors of this paper explore this problem in a scenario where a model is trained to classify animals based on their appearance, but the training data also contains images of the animals' habitats. The model may become biased towards the habitat features instead of the animal's appearance, which can lead to poor performance on rare or unseen instances.\n\nThe authors propose an adversarial training approach to mitigate the effects of negative transfer by viewing the problem as a domain adaptation problem. This approach involves training the model to ignore the unrelated features in the training data and focus on the relevant features. The authors demonstrate the effectiveness of this approach on two datasets and show that it can improve the model's performance on rare or unseen instances.\n\nThe authors also discuss the limitations of previous approaches to negative transfer, which rely on regularization techniques, and argue that their approach is more effective and applicable to a wider range of models, including neural networks. Overall, the paper presents a solution to a common problem in machine learning and demonstrates the importance of considering the relationships between different tasks and features in the training data.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nAn unintended consequence of feature sharing is the model fitting to correlated tasks within the dataset, termed negative transfer.   In this paper, we revisit the problem of negative transfer in multitask setting and find that its corrosive effects are applicable to a wide range of linear and non-linear models, including neural networks. We first study the effects of negative transfer in a principled way and show that previously proposed counter-measures are insufficient, particularly for trainable features. We propose an adversarial training approach to mitigate the effects of negative transfer by viewing the problem in a domain adaptation setting. Finally, empirical results on attribute prediction multi-task on AWA and CUB datasets further validate the need for correcting negative sharing in an end-to-end manner. Advances in machine learning have led to proficient supervised learning models with powerful representations in various prediction tasks. We now expect an ideal classification model to restrict itself to a pertinent set of evidences available to it from the input for prediction. Further, we expect the model to disregard any unrelated evidences in the data to enable better generalization. Figure 1: A supervised classifier 'cheetah vs. snow-leopard' that uses unrelated evidence (of habitat) over relevant evidence (of fur patterns). As shown by the pixel importance maps, the model suffers from the negative transfer prevalent in a typical animal image dataset skewed towards the animal's typical habitat and fails to generalize to rare samples.Let us consider the task of training an animal classifier \"cheetah vs. snow-leopards\" from a dataset of images of these animals, such as those illustrated in Figure 1 -a task which ideally should focus on the animal's appearance features. However, a large portion of these images also contain various cues of the typical habitat of the animals in the background, i.e., tall grass and snow (see Figures 1 (a) and (b)) which are, in principle, unrelated to the animal's appearance. An archetypal model is deceived by the co-occurrence of such unrelated, yet easily detectable cues of habitat over the animal's appearance features such as complex fur patterns. However, a proficient supervised learning model must identify relevant evidences for the label of interest and at the same time discard various unrelated evidences such as presence of snow, even though it tends to co-occur frequently with snow-leopard. Consequently, it would be more likely that such a model would perform better on rare-instances (such as those in Figures 1 (c) and (d)) and generalize better to unseen instances.This phenomenon of co-occurring but unrelated evidences being present in training data and thereby having a debilitating effect on model performance has been described in literature BID8 ; BID16 ; BID9 ; BID13 ; BID15 ). These techniques utilize the easy availability of labels for unrelated evidences (e.g. background habitat labels above), called negative labels which constitutes an auxiliary task, and seek to mitigate its debilitating performance on the primary task (e.g. animal classification above) with techniques referred to as negative-sharing or negative-transfer.While all of these techniques have tackled this problem utilizing various forms of regularization, we describe several shortcomings of this class of approaches, most notable of which is their inapplicability to the popular paradigm of trainable features obtained via neural representation learning. Motivated by these limitations, in this paper we depart from the direction of regularization-based approaches and examine methods inspired from a domain-adaptation viewpoint to propose an adversarial training-based formulation. We uniquely view such a scenario as an instance of adversarial multi-task learning, where the classification tasks are either the primary task of interest (i.e., predicting the presence of fur pattern and color) or the auxiliary negative tasks (i.e., characteristics of habitat) to be avoided. Since the 2 tasks are unrelated, any label correlation between primary and auxiliary labels in the training data is only by chance and therefore from a domain-adaptation perspective, we envision a target-domain as possibly having a different correlation between the primary and auxiliary labels. The effects of negative transfer are hence mitigated when the classification task is trained in this domain.We discuss advantages of our proposed formulation, inspired from domain-adaptation, to alleviate the negative transfer over existing techniques, including ready applicability to neural networks in an end-to-end fashion. It must be noted that, while the formulation of the problem is motivated with multi-task learning, negative-transfer is a disposition of any supervised learning task from simple binary classification to recent popular supervised tasks such as image detection, captioning, or visual dialog. We present motivating literature that prelude this work next. In this work, we show that adversarial learning is the natural answer to prevent negative transfer. This leads to potential improvement in any supervised learning of natural data that is seeking generalization. We find that even in relatively straight-forward linear models presented above, co-occurrence of unrelated labels hampers performance and must be explicitly treated. We address the problem of negative transfer in a multi-task scenario, and also show the applicability of our solution in any supervised task. Supervised learning practitioners can utilize domain expertise to acquire and leverage additional negative labels for this purpose. Recent work in explainability of machine learning models can also be appropriately leveraged to facilitate this task.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nIn this paper, researchers studied how machine learning models can be affected by \"negative transfer\" when learning from multiple tasks at once. Negative transfer occurs when a model is trained on a dataset that contains unrelated information, which can cause the model to focus on the wrong features and perform poorly on new, unseen data. The researchers found that this problem is not unique to complex models like neural networks, but can also affect simpler models like linear models.\n\nTo address this issue, the researchers proposed a new approach called \"adversarial training\" that uses a domain adaptation perspective to mitigate the effects of negative transfer. They showed that this approach can be effective in improving the performance of machine learning models on a variety of tasks, including image classification and object detection.\n\nThe researchers also highlighted the importance of considering the relationship between the tasks being learned and the features present in the data. They suggested that domain expertise and explainability techniques can be used to identify and leverage additional negative labels, which can help to improve the performance of machine learning models.\n\nOverall, this research highlights the importance of considering the potential negative effects of negative transfer when designing machine learning models and the need for new approaches to mitigate these effects.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 8 sentences.\n\nScholarly document\nAn unintended consequence of feature sharing is the model fitting to correlated tasks within the dataset, termed negative transfer.   In this paper, we revisit the problem of negative transfer in multitask setting and find that its corrosive effects are applicable to a wide range of linear and non-linear models, including neural networks. We first study the effects of negative transfer in a principled way and show that previously proposed counter-measures are insufficient, particularly for trainable features. We propose an adversarial training approach to mitigate the effects of negative transfer by viewing the problem in a domain adaptation setting. Finally, empirical results on attribute prediction multi-task on AWA and CUB datasets further validate the need for correcting negative sharing in an end-to-end manner. Advances in machine learning have led to proficient supervised learning models with powerful representations in various prediction tasks. We now expect an ideal classification model to restrict itself to a pertinent set of evidences available to it from the input for prediction. Further, we expect the model to disregard any unrelated evidences in the data to enable better generalization. Figure 1: A supervised classifier 'cheetah vs. snow-leopard' that uses unrelated evidence (of habitat) over relevant evidence (of fur patterns). As shown by the pixel importance maps, the model suffers from the negative transfer prevalent in a typical animal image dataset skewed towards the animal's typical habitat and fails to generalize to rare samples.Let us consider the task of training an animal classifier \"cheetah vs. snow-leopards\" from a dataset of images of these animals, such as those illustrated in Figure 1 -a task which ideally should focus on the animal's appearance features. However, a large portion of these images also contain various cues of the typical habitat of the animals in the background, i.e., tall grass and snow (see Figures 1 (a) and (b)) which are, in principle, unrelated to the animal's appearance. An archetypal model is deceived by the co-occurrence of such unrelated, yet easily detectable cues of habitat over the animal's appearance features such as complex fur patterns. However, a proficient supervised learning model must identify relevant evidences for the label of interest and at the same time discard various unrelated evidences such as presence of snow, even though it tends to co-occur frequently with snow-leopard. Consequently, it would be more likely that such a model would perform better on rare-instances (such as those in Figures 1 (c) and (d)) and generalize better to unseen instances.This phenomenon of co-occurring but unrelated evidences being present in training data and thereby having a debilitating effect on model performance has been described in literature BID8 ; BID16 ; BID9 ; BID13 ; BID15 ). These techniques utilize the easy availability of labels for unrelated evidences (e.g. background habitat labels above), called negative labels which constitutes an auxiliary task, and seek to mitigate its debilitating performance on the primary task (e.g. animal classification above) with techniques referred to as negative-sharing or negative-transfer.While all of these techniques have tackled this problem utilizing various forms of regularization, we describe several shortcomings of this class of approaches, most notable of which is their inapplicability to the popular paradigm of trainable features obtained via neural representation learning. Motivated by these limitations, in this paper we depart from the direction of regularization-based approaches and examine methods inspired from a domain-adaptation viewpoint to propose an adversarial training-based formulation. We uniquely view such a scenario as an instance of adversarial multi-task learning, where the classification tasks are either the primary task of interest (i.e., predicting the presence of fur pattern and color) or the auxiliary negative tasks (i.e., characteristics of habitat) to be avoided. Since the 2 tasks are unrelated, any label correlation between primary and auxiliary labels in the training data is only by chance and therefore from a domain-adaptation perspective, we envision a target-domain as possibly having a different correlation between the primary and auxiliary labels. The effects of negative transfer are hence mitigated when the classification task is trained in this domain.We discuss advantages of our proposed formulation, inspired from domain-adaptation, to alleviate the negative transfer over existing techniques, including ready applicability to neural networks in an end-to-end fashion. It must be noted that, while the formulation of the problem is motivated with multi-task learning, negative-transfer is a disposition of any supervised learning task from simple binary classification to recent popular supervised tasks such as image detection, captioning, or visual dialog. We present motivating literature that prelude this work next. In this work, we show that adversarial learning is the natural answer to prevent negative transfer. This leads to potential improvement in any supervised learning of natural data that is seeking generalization. We find that even in relatively straight-forward linear models presented above, co-occurrence of unrelated labels hampers performance and must be explicitly treated. We address the problem of negative transfer in a multi-task scenario, and also show the applicability of our solution in any supervised task. Supervised learning practitioners can utilize domain expertise to acquire and leverage additional negative labels for this purpose. Recent work in explainability of machine learning models can also be appropriately leveraged to facilitate this task.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nIn machine learning, a problem called negative transfer occurs when a model is trained on multiple tasks at the same time, but it ends up learning irrelevant information from the training data. This can cause the model to perform poorly on new, unseen data. The authors of this paper studied this problem and found that it affects a wide range of models, including neural networks. They also showed that previous methods for addressing negative transfer are not effective, especially when the model is trained on features that can be adjusted during training. The authors proposed a new approach called adversarial training, which involves training the model to ignore irrelevant information. They tested their approach on two datasets and found that it improved the model's performance. This means that the model was better able to generalize to new, unseen data. The authors believe that their approach can be used to improve the performance of any supervised learning model.\n\nImportant entities in this document include models, training data, and neural networks.\n\nImportant dates are not mentioned in this document.\n\nEvents happening in this document include the study of negative transfer, the development of a new approach to address negative transfer, and the testing of this approach on two datasets.\n\nThe result of these events is the development of a new approach to address negative transfer, which can be used to improve the performance of any supervised learning model."
        }
    },
    {
        "id": 26,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nDropout is a simple yet effective technique to improve generalization performance and prevent overfitting in deep neural networks (DNNs). In this paper, we discuss three novel observations about dropout to better understand the generalization of DNNs with rectified linear unit (ReLU) activations: 1) dropout is a smoothing technique that encourages each local linear model of a DNN to be trained on data points from nearby regions; 2) a constant dropout rate can result in effective neural-deactivation rates that are significantly different for layers with different fractions of activated neurons; and 3) the rescaling factor of dropout causes an inconsistency to occur between the normalization during training and testing conditions when batch normalization is also used.   The above leads to three simple but nontrivial improvements to dropout resulting in our proposed method \"Jumpout. \" Jumpout samples the dropout rate using a monotone decreasing distribution (such as the right part of a truncated Gaussian), so the local linear model at each data point is trained, with high probability, to work better for data points from nearby than from more distant regions. Instead of tuning a dropout rate for each layer and applying it to all samples, jumpout moreover adaptively normalizes the dropout rate at each layer and every training sample/batch, so the effective dropout rate applied to the activated neurons are kept the same. Moreover, we rescale the outputs of jumpout for a better trade-off that keeps both the variance and mean of neurons more consistent between training and test phases, which mitigates the incompatibility between dropout and batch normalization. Compared to the original dropout, jumpout shows significantly improved performance on CIFAR10, CIFAR100, Fashion- MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and computation costs. Deep learning has achieved remarkable success on a variety of machine learning tasks BID15 BID14 . Deep neural networks (DNN), however, are often able to fit the training data perfectly -this can result in the overfitting problem, thereby weakening the generalization performance on unseen data. Dropout BID17 BID7 is a simple yet effective technique to mitigate such problems by randomly setting the activations of hidden neurons to 0, a strategy that reduces co-adaptation amongst neurons. Dropout applies to any layer in a DNN without causing significant additional computational overhead.Dropout, however, has several drawbacks. Firstly, dropout rates, constituting extra hyper-parameters at each layer, need to be tuned to get optimal performance. Too high a dropout rate can slow the convergence rate of the model, and often hurt final performance. Too low a rate yields few or no improvements on generalization performance. Ideally, dropout rates should be tuned separately for each layer and also during various training stages. In practice, to reduce computation, we often tune a single dropout rate and keep it constant for all dropout layers and throughout the training process.If we treat dropout as a type of perturbation on each training sample, it acts to generalize the DNN to noisy samples having that specific expected amount of perturbation (due to the fixed dropout rate) with high probability. The fixed rate rules out samples typical having less perturbation, i.e., those potentially more likely to be closer to the original samples and thus that are potentially more helpful to improve generalization. Also, when a constant dropout rate is applied to layers and samples having different fractions of activated neurons, the effective dropout rate (i.e., the proportion of the activated neurons that are deactivated by dropout) varies, which might result in too much perturbation for some layers and samples and too little perturbation for others.Another deficiency of dropout lies in its incompatibility with batch normalization (BN) BID8 (more empirical evidence of this is shown in Section 3.3). As dropout randomly shuts down activated neurons, it needs to rescale the undropped neurons to match the original overall activation gain of the layer. Unfortunately, such rescaling breaks the consistency of the normalization parameters required between training and test phases 1 and may cause poor behavior when used with BN. Since BN, and its variants BID0 BID18 BID20 , has become an almost indispensable component of modern DNN architectures to keep the training stable and to accelerate convergence, dropout itself often gets dropped out in the choice between these two non-complementary options and has recently become less popular.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a technique called dropout, which is used to improve the performance of deep neural networks (DNNs) and prevent them from overfitting. Overfitting occurs when a DNN is too good at fitting the training data, but not well enough at generalizing to new, unseen data. Dropout works by randomly setting some of the neurons in the network to zero, which helps to prevent the network from relying too heavily on any one set of neurons. The document presents three novel observations about dropout and proposes a new method called \"Jumpout\" that improves upon the original dropout technique. Jumpout uses a monotone decreasing distribution to sample the dropout rate, which helps to ensure that the network is trained on data points from nearby regions. The document also discusses the limitations of dropout, including the need to tune the dropout rate for each layer and the incompatibility of dropout with batch normalization.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nDropout is a simple yet effective technique to improve generalization performance and prevent overfitting in deep neural networks (DNNs). In this paper, we discuss three novel observations about dropout to better understand the generalization of DNNs with rectified linear unit (ReLU) activations: 1) dropout is a smoothing technique that encourages each local linear model of a DNN to be trained on data points from nearby regions; 2) a constant dropout rate can result in effective neural-deactivation rates that are significantly different for layers with different fractions of activated neurons; and 3) the rescaling factor of dropout causes an inconsistency to occur between the normalization during training and testing conditions when batch normalization is also used.   The above leads to three simple but nontrivial improvements to dropout resulting in our proposed method \"Jumpout. \" Jumpout samples the dropout rate using a monotone decreasing distribution (such as the right part of a truncated Gaussian), so the local linear model at each data point is trained, with high probability, to work better for data points from nearby than from more distant regions. Instead of tuning a dropout rate for each layer and applying it to all samples, jumpout moreover adaptively normalizes the dropout rate at each layer and every training sample/batch, so the effective dropout rate applied to the activated neurons are kept the same. Moreover, we rescale the outputs of jumpout for a better trade-off that keeps both the variance and mean of neurons more consistent between training and test phases, which mitigates the incompatibility between dropout and batch normalization. Compared to the original dropout, jumpout shows significantly improved performance on CIFAR10, CIFAR100, Fashion- MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and computation costs. Deep learning has achieved remarkable success on a variety of machine learning tasks BID15 BID14 . Deep neural networks (DNN), however, are often able to fit the training data perfectly -this can result in the overfitting problem, thereby weakening the generalization performance on unseen data. Dropout BID17 BID7 is a simple yet effective technique to mitigate such problems by randomly setting the activations of hidden neurons to 0, a strategy that reduces co-adaptation amongst neurons. Dropout applies to any layer in a DNN without causing significant additional computational overhead.Dropout, however, has several drawbacks. Firstly, dropout rates, constituting extra hyper-parameters at each layer, need to be tuned to get optimal performance. Too high a dropout rate can slow the convergence rate of the model, and often hurt final performance. Too low a rate yields few or no improvements on generalization performance. Ideally, dropout rates should be tuned separately for each layer and also during various training stages. In practice, to reduce computation, we often tune a single dropout rate and keep it constant for all dropout layers and throughout the training process.If we treat dropout as a type of perturbation on each training sample, it acts to generalize the DNN to noisy samples having that specific expected amount of perturbation (due to the fixed dropout rate) with high probability. The fixed rate rules out samples typical having less perturbation, i.e., those potentially more likely to be closer to the original samples and thus that are potentially more helpful to improve generalization. Also, when a constant dropout rate is applied to layers and samples having different fractions of activated neurons, the effective dropout rate (i.e., the proportion of the activated neurons that are deactivated by dropout) varies, which might result in too much perturbation for some layers and samples and too little perturbation for others.Another deficiency of dropout lies in its incompatibility with batch normalization (BN) BID8 (more empirical evidence of this is shown in Section 3.3). As dropout randomly shuts down activated neurons, it needs to rescale the undropped neurons to match the original overall activation gain of the layer. Unfortunately, such rescaling breaks the consistency of the normalization parameters required between training and test phases 1 and may cause poor behavior when used with BN. Since BN, and its variants BID0 BID18 BID20 , has become an almost indispensable component of modern DNN architectures to keep the training stable and to accelerate convergence, dropout itself often gets dropped out in the choice between these two non-complementary options and has recently become less popular.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a technique called dropout, which is used to improve the performance of deep neural networks (DNNs) by preventing overfitting. The authors of the paper identify three key issues with dropout: it can lead to inconsistent normalization between training and testing conditions, it doesn't adapt to different layers and samples, and it can be incompatible with batch normalization. To address these issues, the authors propose a new method called Jumpout, which samples the dropout rate using a decreasing distribution to encourage the model to learn from nearby data points. Jumpout also adaptively normalizes the dropout rate and rescales the outputs to maintain consistency between training and testing. The results show that Jumpout performs significantly better than the original dropout method on various datasets, while introducing minimal additional memory and computation costs.\n\nThe important entities in this document are deep neural networks, dropout, and batch normalization. The important dates mentioned are not applicable, as this is a scholarly paper discussing ongoing research. The events happening in this document are the authors' observations about the limitations of dropout and their proposal of the Jumpout method. The result of these events is the development of a new and improved method for improving the performance of deep neural networks.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nDropout is a simple yet effective technique to improve generalization performance and prevent overfitting in deep neural networks (DNNs). In this paper, we discuss three novel observations about dropout to better understand the generalization of DNNs with rectified linear unit (ReLU) activations: 1) dropout is a smoothing technique that encourages each local linear model of a DNN to be trained on data points from nearby regions; 2) a constant dropout rate can result in effective neural-deactivation rates that are significantly different for layers with different fractions of activated neurons; and 3) the rescaling factor of dropout causes an inconsistency to occur between the normalization during training and testing conditions when batch normalization is also used.   The above leads to three simple but nontrivial improvements to dropout resulting in our proposed method \"Jumpout. \" Jumpout samples the dropout rate using a monotone decreasing distribution (such as the right part of a truncated Gaussian), so the local linear model at each data point is trained, with high probability, to work better for data points from nearby than from more distant regions. Instead of tuning a dropout rate for each layer and applying it to all samples, jumpout moreover adaptively normalizes the dropout rate at each layer and every training sample/batch, so the effective dropout rate applied to the activated neurons are kept the same. Moreover, we rescale the outputs of jumpout for a better trade-off that keeps both the variance and mean of neurons more consistent between training and test phases, which mitigates the incompatibility between dropout and batch normalization. Compared to the original dropout, jumpout shows significantly improved performance on CIFAR10, CIFAR100, Fashion- MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and computation costs. Deep learning has achieved remarkable success on a variety of machine learning tasks BID15 BID14 . Deep neural networks (DNN), however, are often able to fit the training data perfectly -this can result in the overfitting problem, thereby weakening the generalization performance on unseen data. Dropout BID17 BID7 is a simple yet effective technique to mitigate such problems by randomly setting the activations of hidden neurons to 0, a strategy that reduces co-adaptation amongst neurons. Dropout applies to any layer in a DNN without causing significant additional computational overhead.Dropout, however, has several drawbacks. Firstly, dropout rates, constituting extra hyper-parameters at each layer, need to be tuned to get optimal performance. Too high a dropout rate can slow the convergence rate of the model, and often hurt final performance. Too low a rate yields few or no improvements on generalization performance. Ideally, dropout rates should be tuned separately for each layer and also during various training stages. In practice, to reduce computation, we often tune a single dropout rate and keep it constant for all dropout layers and throughout the training process.If we treat dropout as a type of perturbation on each training sample, it acts to generalize the DNN to noisy samples having that specific expected amount of perturbation (due to the fixed dropout rate) with high probability. The fixed rate rules out samples typical having less perturbation, i.e., those potentially more likely to be closer to the original samples and thus that are potentially more helpful to improve generalization. Also, when a constant dropout rate is applied to layers and samples having different fractions of activated neurons, the effective dropout rate (i.e., the proportion of the activated neurons that are deactivated by dropout) varies, which might result in too much perturbation for some layers and samples and too little perturbation for others.Another deficiency of dropout lies in its incompatibility with batch normalization (BN) BID8 (more empirical evidence of this is shown in Section 3.3). As dropout randomly shuts down activated neurons, it needs to rescale the undropped neurons to match the original overall activation gain of the layer. Unfortunately, such rescaling breaks the consistency of the normalization parameters required between training and test phases 1 and may cause poor behavior when used with BN. Since BN, and its variants BID0 BID18 BID20 , has become an almost indispensable component of modern DNN architectures to keep the training stable and to accelerate convergence, dropout itself often gets dropped out in the choice between these two non-complementary options and has recently become less popular.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nDeep neural networks (DNNs) are great at solving many problems, but they can also be prone to overfitting, which means they perform well on the training data but poorly on new, unseen data. To combat this, researchers have developed a technique called dropout, which randomly sets some of the neurons in the network to zero during training. This helps the network generalize better to new data. However, there are some limitations to dropout. For example, the dropout rate (the proportion of neurons that are set to zero) needs to be carefully tuned, and it can be incompatible with another important technique called batch normalization. The researchers in this paper propose a new version of dropout called \"Jumpout\" that addresses these limitations. Jumpout uses a different way of setting the dropout rate and rescaling the output of the network, which helps the network generalize even better and perform well on a variety of tasks. Overall, Jumpout is a simple but effective way to improve the performance of DNNs and help them generalize better to new data.\n\nThe important entities in this document are deep neural networks, dropout, and batch normalization. The important dates are not explicitly mentioned, but the paper is likely referring to the development of deep learning and the introduction of dropout and batch normalization as techniques in the field. The events in this document are the development of dropout and batch normalization, the limitations of these techniques, and the proposal of a new version of dropout called Jumpout. The result of these events is a more effective and generalizable deep learning technique.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 6 sentences.\n\nScholarly document\nDropout is a simple yet effective technique to improve generalization performance and prevent overfitting in deep neural networks (DNNs). In this paper, we discuss three novel observations about dropout to better understand the generalization of DNNs with rectified linear unit (ReLU) activations: 1) dropout is a smoothing technique that encourages each local linear model of a DNN to be trained on data points from nearby regions; 2) a constant dropout rate can result in effective neural-deactivation rates that are significantly different for layers with different fractions of activated neurons; and 3) the rescaling factor of dropout causes an inconsistency to occur between the normalization during training and testing conditions when batch normalization is also used.   The above leads to three simple but nontrivial improvements to dropout resulting in our proposed method \"Jumpout. \" Jumpout samples the dropout rate using a monotone decreasing distribution (such as the right part of a truncated Gaussian), so the local linear model at each data point is trained, with high probability, to work better for data points from nearby than from more distant regions. Instead of tuning a dropout rate for each layer and applying it to all samples, jumpout moreover adaptively normalizes the dropout rate at each layer and every training sample/batch, so the effective dropout rate applied to the activated neurons are kept the same. Moreover, we rescale the outputs of jumpout for a better trade-off that keeps both the variance and mean of neurons more consistent between training and test phases, which mitigates the incompatibility between dropout and batch normalization. Compared to the original dropout, jumpout shows significantly improved performance on CIFAR10, CIFAR100, Fashion- MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and computation costs. Deep learning has achieved remarkable success on a variety of machine learning tasks BID15 BID14 . Deep neural networks (DNN), however, are often able to fit the training data perfectly -this can result in the overfitting problem, thereby weakening the generalization performance on unseen data. Dropout BID17 BID7 is a simple yet effective technique to mitigate such problems by randomly setting the activations of hidden neurons to 0, a strategy that reduces co-adaptation amongst neurons. Dropout applies to any layer in a DNN without causing significant additional computational overhead.Dropout, however, has several drawbacks. Firstly, dropout rates, constituting extra hyper-parameters at each layer, need to be tuned to get optimal performance. Too high a dropout rate can slow the convergence rate of the model, and often hurt final performance. Too low a rate yields few or no improvements on generalization performance. Ideally, dropout rates should be tuned separately for each layer and also during various training stages. In practice, to reduce computation, we often tune a single dropout rate and keep it constant for all dropout layers and throughout the training process.If we treat dropout as a type of perturbation on each training sample, it acts to generalize the DNN to noisy samples having that specific expected amount of perturbation (due to the fixed dropout rate) with high probability. The fixed rate rules out samples typical having less perturbation, i.e., those potentially more likely to be closer to the original samples and thus that are potentially more helpful to improve generalization. Also, when a constant dropout rate is applied to layers and samples having different fractions of activated neurons, the effective dropout rate (i.e., the proportion of the activated neurons that are deactivated by dropout) varies, which might result in too much perturbation for some layers and samples and too little perturbation for others.Another deficiency of dropout lies in its incompatibility with batch normalization (BN) BID8 (more empirical evidence of this is shown in Section 3.3). As dropout randomly shuts down activated neurons, it needs to rescale the undropped neurons to match the original overall activation gain of the layer. Unfortunately, such rescaling breaks the consistency of the normalization parameters required between training and test phases 1 and may cause poor behavior when used with BN. Since BN, and its variants BID0 BID18 BID20 , has become an almost indispensable component of modern DNN architectures to keep the training stable and to accelerate convergence, dropout itself often gets dropped out in the choice between these two non-complementary options and has recently become less popular.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document that embeds the answers to the above questions:\n\nThe paper discusses the limitations of dropout, a technique used to improve generalization performance and prevent overfitting in deep neural networks (DNNs). The authors identify three key observations about dropout: it acts as a smoothing technique, encourages local linear models to be trained on nearby data points, and has inconsistent normalization between training and testing conditions when used with batch normalization. To address these issues, the authors propose a new method called \"Jumpout,\" which samples the dropout rate using a monotone decreasing distribution, adapts the dropout rate to each layer and sample, and rescales the outputs to maintain consistency between training and testing phases. The results show that Jumpout significantly improves performance on various datasets while introducing negligible additional memory and computation costs. The authors also highlight the limitations of dropout, including the need to tune dropout rates, potential for over-perturbation, and incompatibility with batch normalization."
        }
    },
    {
        "id": 960,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nSolving long-horizon sequential decision making tasks in environments with sparse rewards is a longstanding problem in reinforcement learning (RL) research. Hierarchical Reinforcement Learning (HRL) has held the promise to enhance the capabilities of RL agents via operation on different levels of temporal abstraction. Despite the success of recent works in dealing with inherent nonstationarity and sample complexity, it remains difficult to generalize to unseen environments and to transfer different layers of the policy to other agents. In this paper, we propose a novel HRL architecture, Hierarchical Decompositional Reinforcement Learning (HiDe), which allows decomposition of the hierarchical layers into independent subtasks, yet allows for joint training of all layers in end-to-end manner. The main insight is to combine a control policy on a lower level with an image-based planning policy on a higher level. We evaluate our method on various complex continuous control tasks for navigation, demonstrating that generalization across environments and transfer of higher level policies can be achieved. See videos https://sites.google.com/view/hide-rl Reinforcement learning (RL) has been succesfully applied to sequential-decision making tasks, such as learning how to play video games in Atari (Mnih et al., 2013) , mastering the game of Go or continuous control in robotics (Lillicrap et al., 2015; Levine et al., 2015; . However, despite the success of RL agents in learning control policies for myopic tasks, such as reaching a nearby target, they lack the ability to effectively reason over extended horizons. In this paper, we consider continuous control tasks that require planning over long horizons in navigation environments with sparse rewards. The task becomes particularly challenging with sparse and delayed rewards since an agent needs to infer which actions caused the reward in a domain where most samples give no signal at all. Common techniques to mitigate the issue of sparse rewards include learning from demonstrations (Schaal, 1999; Peng et al., 2018) or using enhanced exploration strategies (Bellemare et al., 2016; Pathak et al., 2017; Andrychowicz et al., 2017) . Hierarchical Reinforcement Learning (HRL) has been proposed in part to solve such tasks. Typically, a sequential decision making task is split into several simpler subtasks of different temporal and functional abstraction levels (Sutton et al., 1999; Andre & Russell, 2002) . Although the hierarchies would ideally be learned in parallel, most methods resort to curriculum learning (Frans et al., 2017; Florensa et al., 2017; Bacon et al., 2016; Vezhnevets et al., 2017) . Recent goal-conditioned hierarchical architectures have successfully trained policies jointly via off-policy learning (Levy et al., 2019; Nachum et al., 2018; . However, these methods often do not generalize to unseen environments as we show in Section 5.1. We argue that this is due to a lack of true separation of planning and low-level control across the hierarchy. In this paper, we consider two main problems, namely functional decomposition of HRL architectures in navigation-based domains and generalization of RL agents to unseen environments (figure 1). To address these issues, we propose a novel multi-level HRL architecture that enables both functional decomposition and temporal abstraction. We introduce a 3-level hierarchy that decouples the major roles in a complex navigation task, namely planning and low-level control. The benefit of a modular design is twofold. First, layers have access to only task-relevant information for a predefined task, which significantly improves the generalization ability of the overall policy. Hence, this enables policies learned on a single task to solve randomly configured environments. Second, Figure 1 : Navigation environments. The red sphere indicates the goal an agent needs to reach, with the starting point at the opposite end of the maze. The agent is trained on environment a). To test generalization, we use the environments with b) reversed starting and goal positions, c) mirrored maze with reversed starting and goal positions and d) randomly generated mazes. the planning and control layers are modular and thus allow for composition of cross-agent architectures. We empirically show that the planning layer of the hierarchy can be transferred successfully to new agents. During training we provide global environment information only to the planning layer, whereas the full internal state of the agent is only accessible by the control layer. The actions of the top and middle layers are in the form of displacement in space. Similarly, the goals of the middle and lowest layers are relative to the current position. This prevents the policies from overfitting to the global position in an environment and hence encourages generalization to new environments. In our framework (see figure 2), the planner (i.e., the highest level policy \u03c0 2 ) learns to find a trajectory leading the agent to the goal. Specifically, we learn a value map of the environment by means of a value propagation network (Nardelli et al., 2019) . To prevent the policy from issuing too ambitious subgoals, an attention network estimates the range of the lower level policy \u03c0 0 (i.e., the agent). This attention mask also ensures that the planning considers the agent performance. The action of \u03c0 2 is the position which maximizes the masked value map, which serves as goal input to the policy \u03c0 1 . The middle layer implements an interface between the upper planner and lower control layer, which refines the coarser subgoals into shorter and reachable targets for the agent. The middle layer is crucial in functionally decoupling the abstract task of planning (\u03c0 2 ) from agent specific continuous control. The lowest layer learns a control policy \u03c0 0 to steer the agent to intermediate goals. While the policies are functionally decoupled, they are trained together and must learn to cooperate. In this work, we focus on solving long-horizon tasks with sparse rewards in complex continuous navigation domains. We first show in a maze environment that generalization causes challenges for state-of-the-art approaches. We then demonstrate that training with the same environment configuration (i.e., fixed start and goal positions) can generalize to randomly configured environments. Lastly, we show the benefits of functional decomposition via transfer of individual layers between different agents. In particular, we train our method with a simple 2DoF ball agent in a maze environment to learn the planning layer which is later used to steer a more complex agent. The results indicate that the proposed decomposition of policy layers is effective and can generalize to unseen environments. In summary our main contributions include: \u2022 A novel multi-layer HRL architecture that allows functional decomposition and temporal abstraction for navigation tasks. \u2022 This architecture enables generalization beyond training conditions and environments. \u2022 Functional decomposition that allows transfer of individual layers across different agents. In this paper, we introduce a novel HRL architecture that can solve complex navigation tasks in 3D-based maze environments. The architecture consists of a planning layer which learns an explicit value map and is connected with a subgoal refinement layer and a low-level control layer. The framework can be trained end-to-end. While training with a fixed starting and goal position, our method is able to generalize to previously unseen settings and environments. Furthermore, we demonstrate that transfer of planners between different agents can be achieved, enabling us to transfer a planner trained with a simplistic agent such as a ball to a more complex agent such as an ant or humanoid. In future work, we want to consider integration of a more general planner that is not restricted to navigation-based environments.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a problem in artificial intelligence called reinforcement learning, where machines learn to make decisions by trial and error. The main challenge is to solve long-term problems where the rewards are rare and delayed. The authors propose a new approach called Hierarchical Decompositional Reinforcement Learning (HiDe) that breaks down the problem into smaller, more manageable tasks. This approach allows for better generalization to new environments and the transfer of learned skills to different agents.\n\nThe authors use a maze navigation task to test their approach, where the goal is to reach a target while avoiding obstacles. They show that their method can generalize to new environments and transfer the learned skills to different agents. The authors also highlight the importance of separating the planning and control layers in the hierarchy to achieve better generalization.\n\nThe main entities in this document are the reinforcement learning agents, the maze environments, and the hierarchical layers. The important dates mentioned are not explicitly stated, but the authors refer to previous works in the field of reinforcement learning.\n\nThe events happening in this document are the development of the Hierarchical Decompositional Reinforcement Learning (HiDe) approach, the testing of this approach on a maze navigation task, and the demonstration of its generalization and transfer capabilities.\n\nThe result of these events is the development of a new approach to reinforcement learning that can solve long-term problems with sparse rewards and generalize to new environments.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nSolving long-horizon sequential decision making tasks in environments with sparse rewards is a longstanding problem in reinforcement learning (RL) research. Hierarchical Reinforcement Learning (HRL) has held the promise to enhance the capabilities of RL agents via operation on different levels of temporal abstraction. Despite the success of recent works in dealing with inherent nonstationarity and sample complexity, it remains difficult to generalize to unseen environments and to transfer different layers of the policy to other agents. In this paper, we propose a novel HRL architecture, Hierarchical Decompositional Reinforcement Learning (HiDe), which allows decomposition of the hierarchical layers into independent subtasks, yet allows for joint training of all layers in end-to-end manner. The main insight is to combine a control policy on a lower level with an image-based planning policy on a higher level. We evaluate our method on various complex continuous control tasks for navigation, demonstrating that generalization across environments and transfer of higher level policies can be achieved. See videos https://sites.google.com/view/hide-rl Reinforcement learning (RL) has been succesfully applied to sequential-decision making tasks, such as learning how to play video games in Atari (Mnih et al., 2013) , mastering the game of Go or continuous control in robotics (Lillicrap et al., 2015; Levine et al., 2015; . However, despite the success of RL agents in learning control policies for myopic tasks, such as reaching a nearby target, they lack the ability to effectively reason over extended horizons. In this paper, we consider continuous control tasks that require planning over long horizons in navigation environments with sparse rewards. The task becomes particularly challenging with sparse and delayed rewards since an agent needs to infer which actions caused the reward in a domain where most samples give no signal at all. Common techniques to mitigate the issue of sparse rewards include learning from demonstrations (Schaal, 1999; Peng et al., 2018) or using enhanced exploration strategies (Bellemare et al., 2016; Pathak et al., 2017; Andrychowicz et al., 2017) . Hierarchical Reinforcement Learning (HRL) has been proposed in part to solve such tasks. Typically, a sequential decision making task is split into several simpler subtasks of different temporal and functional abstraction levels (Sutton et al., 1999; Andre & Russell, 2002) . Although the hierarchies would ideally be learned in parallel, most methods resort to curriculum learning (Frans et al., 2017; Florensa et al., 2017; Bacon et al., 2016; Vezhnevets et al., 2017) . Recent goal-conditioned hierarchical architectures have successfully trained policies jointly via off-policy learning (Levy et al., 2019; Nachum et al., 2018; . However, these methods often do not generalize to unseen environments as we show in Section 5.1. We argue that this is due to a lack of true separation of planning and low-level control across the hierarchy. In this paper, we consider two main problems, namely functional decomposition of HRL architectures in navigation-based domains and generalization of RL agents to unseen environments (figure 1). To address these issues, we propose a novel multi-level HRL architecture that enables both functional decomposition and temporal abstraction. We introduce a 3-level hierarchy that decouples the major roles in a complex navigation task, namely planning and low-level control. The benefit of a modular design is twofold. First, layers have access to only task-relevant information for a predefined task, which significantly improves the generalization ability of the overall policy. Hence, this enables policies learned on a single task to solve randomly configured environments. Second, Figure 1 : Navigation environments. The red sphere indicates the goal an agent needs to reach, with the starting point at the opposite end of the maze. The agent is trained on environment a). To test generalization, we use the environments with b) reversed starting and goal positions, c) mirrored maze with reversed starting and goal positions and d) randomly generated mazes. the planning and control layers are modular and thus allow for composition of cross-agent architectures. We empirically show that the planning layer of the hierarchy can be transferred successfully to new agents. During training we provide global environment information only to the planning layer, whereas the full internal state of the agent is only accessible by the control layer. The actions of the top and middle layers are in the form of displacement in space. Similarly, the goals of the middle and lowest layers are relative to the current position. This prevents the policies from overfitting to the global position in an environment and hence encourages generalization to new environments. In our framework (see figure 2), the planner (i.e., the highest level policy \u03c0 2 ) learns to find a trajectory leading the agent to the goal. Specifically, we learn a value map of the environment by means of a value propagation network (Nardelli et al., 2019) . To prevent the policy from issuing too ambitious subgoals, an attention network estimates the range of the lower level policy \u03c0 0 (i.e., the agent). This attention mask also ensures that the planning considers the agent performance. The action of \u03c0 2 is the position which maximizes the masked value map, which serves as goal input to the policy \u03c0 1 . The middle layer implements an interface between the upper planner and lower control layer, which refines the coarser subgoals into shorter and reachable targets for the agent. The middle layer is crucial in functionally decoupling the abstract task of planning (\u03c0 2 ) from agent specific continuous control. The lowest layer learns a control policy \u03c0 0 to steer the agent to intermediate goals. While the policies are functionally decoupled, they are trained together and must learn to cooperate. In this work, we focus on solving long-horizon tasks with sparse rewards in complex continuous navigation domains. We first show in a maze environment that generalization causes challenges for state-of-the-art approaches. We then demonstrate that training with the same environment configuration (i.e., fixed start and goal positions) can generalize to randomly configured environments. Lastly, we show the benefits of functional decomposition via transfer of individual layers between different agents. In particular, we train our method with a simple 2DoF ball agent in a maze environment to learn the planning layer which is later used to steer a more complex agent. The results indicate that the proposed decomposition of policy layers is effective and can generalize to unseen environments. In summary our main contributions include: \u2022 A novel multi-layer HRL architecture that allows functional decomposition and temporal abstraction for navigation tasks. \u2022 This architecture enables generalization beyond training conditions and environments. \u2022 Functional decomposition that allows transfer of individual layers across different agents. In this paper, we introduce a novel HRL architecture that can solve complex navigation tasks in 3D-based maze environments. The architecture consists of a planning layer which learns an explicit value map and is connected with a subgoal refinement layer and a low-level control layer. The framework can be trained end-to-end. While training with a fixed starting and goal position, our method is able to generalize to previously unseen settings and environments. Furthermore, we demonstrate that transfer of planners between different agents can be achieved, enabling us to transfer a planner trained with a simplistic agent such as a ball to a more complex agent such as an ant or humanoid. In future work, we want to consider integration of a more general planner that is not restricted to navigation-based environments.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nReinforcement learning (RL) is a way for computers to learn how to make decisions by trial and error. One challenge in RL is solving long-horizon tasks, which are tasks that require planning and decision-making over a long period of time. Hierarchical Reinforcement Learning (HRL) is a type of RL that tries to solve this problem by breaking down a complex task into smaller, simpler subtasks.\n\nThe authors of this paper propose a new HRL architecture called Hierarchical Decompositional Reinforcement Learning (HiDe). HiDe is designed to solve complex navigation tasks, such as finding a path through a maze. The architecture consists of three layers: a planning layer, a subgoal refinement layer, and a low-level control layer. Each layer has a specific role in solving the task.\n\nThe planning layer learns to plan a path to the goal, while the subgoal refinement layer refines the plan into smaller, more specific goals. The low-level control layer then controls the agent to achieve these goals. The authors show that their HiDe architecture can generalize to new environments and transfer between different agents.\n\nIn other words, the HiDe architecture can learn to solve a task in one environment and then apply that knowledge to solve the same task in a different environment. This is important because it allows the agent to adapt to new situations and learn from its experiences.\n\nThe authors also show that the planning layer can be transferred between different agents, which means that a planner learned by one agent can be used by another agent. This is useful because it allows agents to share knowledge and learn from each other.\n\nOverall, the HiDe architecture is a promising approach to solving complex navigation tasks and has the potential to be used in a wide range of applications, such as robotics and video games.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nSolving long-horizon sequential decision making tasks in environments with sparse rewards is a longstanding problem in reinforcement learning (RL) research. Hierarchical Reinforcement Learning (HRL) has held the promise to enhance the capabilities of RL agents via operation on different levels of temporal abstraction. Despite the success of recent works in dealing with inherent nonstationarity and sample complexity, it remains difficult to generalize to unseen environments and to transfer different layers of the policy to other agents. In this paper, we propose a novel HRL architecture, Hierarchical Decompositional Reinforcement Learning (HiDe), which allows decomposition of the hierarchical layers into independent subtasks, yet allows for joint training of all layers in end-to-end manner. The main insight is to combine a control policy on a lower level with an image-based planning policy on a higher level. We evaluate our method on various complex continuous control tasks for navigation, demonstrating that generalization across environments and transfer of higher level policies can be achieved. See videos https://sites.google.com/view/hide-rl Reinforcement learning (RL) has been succesfully applied to sequential-decision making tasks, such as learning how to play video games in Atari (Mnih et al., 2013) , mastering the game of Go or continuous control in robotics (Lillicrap et al., 2015; Levine et al., 2015; . However, despite the success of RL agents in learning control policies for myopic tasks, such as reaching a nearby target, they lack the ability to effectively reason over extended horizons. In this paper, we consider continuous control tasks that require planning over long horizons in navigation environments with sparse rewards. The task becomes particularly challenging with sparse and delayed rewards since an agent needs to infer which actions caused the reward in a domain where most samples give no signal at all. Common techniques to mitigate the issue of sparse rewards include learning from demonstrations (Schaal, 1999; Peng et al., 2018) or using enhanced exploration strategies (Bellemare et al., 2016; Pathak et al., 2017; Andrychowicz et al., 2017) . Hierarchical Reinforcement Learning (HRL) has been proposed in part to solve such tasks. Typically, a sequential decision making task is split into several simpler subtasks of different temporal and functional abstraction levels (Sutton et al., 1999; Andre & Russell, 2002) . Although the hierarchies would ideally be learned in parallel, most methods resort to curriculum learning (Frans et al., 2017; Florensa et al., 2017; Bacon et al., 2016; Vezhnevets et al., 2017) . Recent goal-conditioned hierarchical architectures have successfully trained policies jointly via off-policy learning (Levy et al., 2019; Nachum et al., 2018; . However, these methods often do not generalize to unseen environments as we show in Section 5.1. We argue that this is due to a lack of true separation of planning and low-level control across the hierarchy. In this paper, we consider two main problems, namely functional decomposition of HRL architectures in navigation-based domains and generalization of RL agents to unseen environments (figure 1). To address these issues, we propose a novel multi-level HRL architecture that enables both functional decomposition and temporal abstraction. We introduce a 3-level hierarchy that decouples the major roles in a complex navigation task, namely planning and low-level control. The benefit of a modular design is twofold. First, layers have access to only task-relevant information for a predefined task, which significantly improves the generalization ability of the overall policy. Hence, this enables policies learned on a single task to solve randomly configured environments. Second, Figure 1 : Navigation environments. The red sphere indicates the goal an agent needs to reach, with the starting point at the opposite end of the maze. The agent is trained on environment a). To test generalization, we use the environments with b) reversed starting and goal positions, c) mirrored maze with reversed starting and goal positions and d) randomly generated mazes. the planning and control layers are modular and thus allow for composition of cross-agent architectures. We empirically show that the planning layer of the hierarchy can be transferred successfully to new agents. During training we provide global environment information only to the planning layer, whereas the full internal state of the agent is only accessible by the control layer. The actions of the top and middle layers are in the form of displacement in space. Similarly, the goals of the middle and lowest layers are relative to the current position. This prevents the policies from overfitting to the global position in an environment and hence encourages generalization to new environments. In our framework (see figure 2), the planner (i.e., the highest level policy \u03c0 2 ) learns to find a trajectory leading the agent to the goal. Specifically, we learn a value map of the environment by means of a value propagation network (Nardelli et al., 2019) . To prevent the policy from issuing too ambitious subgoals, an attention network estimates the range of the lower level policy \u03c0 0 (i.e., the agent). This attention mask also ensures that the planning considers the agent performance. The action of \u03c0 2 is the position which maximizes the masked value map, which serves as goal input to the policy \u03c0 1 . The middle layer implements an interface between the upper planner and lower control layer, which refines the coarser subgoals into shorter and reachable targets for the agent. The middle layer is crucial in functionally decoupling the abstract task of planning (\u03c0 2 ) from agent specific continuous control. The lowest layer learns a control policy \u03c0 0 to steer the agent to intermediate goals. While the policies are functionally decoupled, they are trained together and must learn to cooperate. In this work, we focus on solving long-horizon tasks with sparse rewards in complex continuous navigation domains. We first show in a maze environment that generalization causes challenges for state-of-the-art approaches. We then demonstrate that training with the same environment configuration (i.e., fixed start and goal positions) can generalize to randomly configured environments. Lastly, we show the benefits of functional decomposition via transfer of individual layers between different agents. In particular, we train our method with a simple 2DoF ball agent in a maze environment to learn the planning layer which is later used to steer a more complex agent. The results indicate that the proposed decomposition of policy layers is effective and can generalize to unseen environments. In summary our main contributions include: \u2022 A novel multi-layer HRL architecture that allows functional decomposition and temporal abstraction for navigation tasks. \u2022 This architecture enables generalization beyond training conditions and environments. \u2022 Functional decomposition that allows transfer of individual layers across different agents. In this paper, we introduce a novel HRL architecture that can solve complex navigation tasks in 3D-based maze environments. The architecture consists of a planning layer which learns an explicit value map and is connected with a subgoal refinement layer and a low-level control layer. The framework can be trained end-to-end. While training with a fixed starting and goal position, our method is able to generalize to previously unseen settings and environments. Furthermore, we demonstrate that transfer of planners between different agents can be achieved, enabling us to transfer a planner trained with a simplistic agent such as a ball to a more complex agent such as an ant or humanoid. In future work, we want to consider integration of a more general planner that is not restricted to navigation-based environments.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a problem in artificial intelligence called reinforcement learning, where a computer program learns to make decisions by trial and error. The goal is to solve complex tasks that require planning ahead, but the program struggles with making decisions when the rewards are sparse and delayed. To address this issue, the authors propose a new approach called Hierarchical Decompositional Reinforcement Learning (HiDe). This method breaks down the decision-making process into multiple levels, allowing the program to plan and control its actions more effectively.\n\nThe authors test their method on several complex navigation tasks, such as navigating a maze, and demonstrate that it can generalize to unseen environments and transfer to different agents. This means that the program can learn to navigate a maze in one environment and then apply that knowledge to navigate a similar maze in a different environment.\n\nThe key insight behind HiDe is to separate the planning and control layers, allowing each layer to focus on its specific task. The planning layer learns to create a plan to reach the goal, while the control layer refines the plan and controls the agent's movements. This modular design enables the program to generalize and transfer knowledge more effectively.\n\nThe authors also show that the planning layer can be transferred between different agents, allowing a planner trained on a simple agent to be used on a more complex agent. This is a significant breakthrough in reinforcement learning, as it enables the program to learn and adapt more efficiently.\n\nOverall, the HiDe approach has the potential to revolutionize the field of reinforcement learning and enable computers to solve complex problems that require planning and decision-making.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nSolving long-horizon sequential decision making tasks in environments with sparse rewards is a longstanding problem in reinforcement learning (RL) research. Hierarchical Reinforcement Learning (HRL) has held the promise to enhance the capabilities of RL agents via operation on different levels of temporal abstraction. Despite the success of recent works in dealing with inherent nonstationarity and sample complexity, it remains difficult to generalize to unseen environments and to transfer different layers of the policy to other agents. In this paper, we propose a novel HRL architecture, Hierarchical Decompositional Reinforcement Learning (HiDe), which allows decomposition of the hierarchical layers into independent subtasks, yet allows for joint training of all layers in end-to-end manner. The main insight is to combine a control policy on a lower level with an image-based planning policy on a higher level. We evaluate our method on various complex continuous control tasks for navigation, demonstrating that generalization across environments and transfer of higher level policies can be achieved. See videos https://sites.google.com/view/hide-rl Reinforcement learning (RL) has been succesfully applied to sequential-decision making tasks, such as learning how to play video games in Atari (Mnih et al., 2013) , mastering the game of Go or continuous control in robotics (Lillicrap et al., 2015; Levine et al., 2015; . However, despite the success of RL agents in learning control policies for myopic tasks, such as reaching a nearby target, they lack the ability to effectively reason over extended horizons. In this paper, we consider continuous control tasks that require planning over long horizons in navigation environments with sparse rewards. The task becomes particularly challenging with sparse and delayed rewards since an agent needs to infer which actions caused the reward in a domain where most samples give no signal at all. Common techniques to mitigate the issue of sparse rewards include learning from demonstrations (Schaal, 1999; Peng et al., 2018) or using enhanced exploration strategies (Bellemare et al., 2016; Pathak et al., 2017; Andrychowicz et al., 2017) . Hierarchical Reinforcement Learning (HRL) has been proposed in part to solve such tasks. Typically, a sequential decision making task is split into several simpler subtasks of different temporal and functional abstraction levels (Sutton et al., 1999; Andre & Russell, 2002) . Although the hierarchies would ideally be learned in parallel, most methods resort to curriculum learning (Frans et al., 2017; Florensa et al., 2017; Bacon et al., 2016; Vezhnevets et al., 2017) . Recent goal-conditioned hierarchical architectures have successfully trained policies jointly via off-policy learning (Levy et al., 2019; Nachum et al., 2018; . However, these methods often do not generalize to unseen environments as we show in Section 5.1. We argue that this is due to a lack of true separation of planning and low-level control across the hierarchy. In this paper, we consider two main problems, namely functional decomposition of HRL architectures in navigation-based domains and generalization of RL agents to unseen environments (figure 1). To address these issues, we propose a novel multi-level HRL architecture that enables both functional decomposition and temporal abstraction. We introduce a 3-level hierarchy that decouples the major roles in a complex navigation task, namely planning and low-level control. The benefit of a modular design is twofold. First, layers have access to only task-relevant information for a predefined task, which significantly improves the generalization ability of the overall policy. Hence, this enables policies learned on a single task to solve randomly configured environments. Second, Figure 1 : Navigation environments. The red sphere indicates the goal an agent needs to reach, with the starting point at the opposite end of the maze. The agent is trained on environment a). To test generalization, we use the environments with b) reversed starting and goal positions, c) mirrored maze with reversed starting and goal positions and d) randomly generated mazes. the planning and control layers are modular and thus allow for composition of cross-agent architectures. We empirically show that the planning layer of the hierarchy can be transferred successfully to new agents. During training we provide global environment information only to the planning layer, whereas the full internal state of the agent is only accessible by the control layer. The actions of the top and middle layers are in the form of displacement in space. Similarly, the goals of the middle and lowest layers are relative to the current position. This prevents the policies from overfitting to the global position in an environment and hence encourages generalization to new environments. In our framework (see figure 2), the planner (i.e., the highest level policy \u03c0 2 ) learns to find a trajectory leading the agent to the goal. Specifically, we learn a value map of the environment by means of a value propagation network (Nardelli et al., 2019) . To prevent the policy from issuing too ambitious subgoals, an attention network estimates the range of the lower level policy \u03c0 0 (i.e., the agent). This attention mask also ensures that the planning considers the agent performance. The action of \u03c0 2 is the position which maximizes the masked value map, which serves as goal input to the policy \u03c0 1 . The middle layer implements an interface between the upper planner and lower control layer, which refines the coarser subgoals into shorter and reachable targets for the agent. The middle layer is crucial in functionally decoupling the abstract task of planning (\u03c0 2 ) from agent specific continuous control. The lowest layer learns a control policy \u03c0 0 to steer the agent to intermediate goals. While the policies are functionally decoupled, they are trained together and must learn to cooperate. In this work, we focus on solving long-horizon tasks with sparse rewards in complex continuous navigation domains. We first show in a maze environment that generalization causes challenges for state-of-the-art approaches. We then demonstrate that training with the same environment configuration (i.e., fixed start and goal positions) can generalize to randomly configured environments. Lastly, we show the benefits of functional decomposition via transfer of individual layers between different agents. In particular, we train our method with a simple 2DoF ball agent in a maze environment to learn the planning layer which is later used to steer a more complex agent. The results indicate that the proposed decomposition of policy layers is effective and can generalize to unseen environments. In summary our main contributions include: \u2022 A novel multi-layer HRL architecture that allows functional decomposition and temporal abstraction for navigation tasks. \u2022 This architecture enables generalization beyond training conditions and environments. \u2022 Functional decomposition that allows transfer of individual layers across different agents. In this paper, we introduce a novel HRL architecture that can solve complex navigation tasks in 3D-based maze environments. The architecture consists of a planning layer which learns an explicit value map and is connected with a subgoal refinement layer and a low-level control layer. The framework can be trained end-to-end. While training with a fixed starting and goal position, our method is able to generalize to previously unseen settings and environments. Furthermore, we demonstrate that transfer of planners between different agents can be achieved, enabling us to transfer a planner trained with a simplistic agent such as a ball to a more complex agent such as an ant or humanoid. In future work, we want to consider integration of a more general planner that is not restricted to navigation-based environments.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a challenge in artificial intelligence called \"long-horizon sequential decision making\" where a robot or agent needs to make decisions over a long period of time to achieve a goal. This is a difficult problem because the agent may not receive rewards or feedback until much later, making it hard to learn what actions to take.\n\nThe authors propose a new approach called Hierarchical Decompositional Reinforcement Learning (HiDe) that breaks down the problem into smaller, more manageable tasks. This allows the agent to focus on one task at a time and learn from its experiences more efficiently.\n\nThe HiDe architecture consists of three layers: a planning layer, a subgoal refinement layer, and a low-level control layer. The planning layer learns to find a path to the goal, the subgoal refinement layer refines the plan into smaller, more specific goals, and the low-level control layer controls the agent's movements.\n\nThe authors test their approach on various complex navigation tasks and show that it can generalize to new environments and transfer knowledge between different agents. This means that an agent trained on one task can be used to solve similar tasks with different settings or environments.\n\nIn summary, the authors have developed a new approach to long-horizon sequential decision making that uses hierarchical decomposition to break down the problem into smaller tasks. This approach allows the agent to learn more efficiently and generalize to new environments, making it a promising solution for complex tasks like navigation.\n\nThe important entities in this document are:\n\n* Hierarchical Reinforcement Learning (HRL)\n* Hierarchical Decompositional Reinforcement Learning (HiDe)\n* Planning layer\n* Subgoal refinement layer\n* Low-level control layer\n* Navigation tasks\n\nThe important dates in this document are not explicitly mentioned, but the authors refer to previous works in the field of reinforcement learning, such as Mnih et al. (2013), Lillicrap et al. (2015), and Levine et al. (2015).\n\nThe events in this document are the development and testing of the HiDe architecture, which involves breaking down the problem of long-horizon sequential decision making into smaller tasks and using hierarchical decomposition to learn more efficiently.\n\nThe result of these events is a new approach to long-horizon sequential decision making that can generalize to new environments and transfer knowledge between different agents."
        }
    },
    {
        "id": 479,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nTraining agents to operate in one environment often yields overfitted models that are unable to generalize to the changes in that environment. However, due to the numerous variations that can occur in the real-world, the agent is often required to be robust in order to be useful. This has not been the case for agents trained with reinforcement learning (RL) algorithms. In this paper, we investigate the overfitting of RL agents to the training environments in visual navigation tasks. Our experiments show that deep RL agents can overfit even when trained on multiple environments simultaneously. \n We propose a regularization method which combines RL with supervised learning methods by adding a term to the RL objective that would encourage the invariance of a policy to variations in the observations that ought not to affect the action taken. The results of this method, called invariance regularization, show an improvement in the generalization of policies to environments not seen during training.\n Learning control policies from high-dimensional sensory input has been gaining more traction lately due to the popularity of deep reinforcement learning (DRL) Mnih et al. (2015) ; ; Zhang et al. (2018b) ; Rakelly et al. (2019) , which enables learning the perception and control modules simultaneously. However, most of the work done in RL chooses to evaluate the learned policies in the same environment in which training occurred Cobbe et al. (2018) . Using the same environments to train and test agents does not give any insight into the generalization abilities of the learned policy. There could be a number of changes in the environment at test time that would degrade the agent's performance. Variations could appear in the visual aspects that determine the agent's observation, the physical structure that determines the agent's state and even some aspects that are related to the agent's goal (Figure 1 ). For example, different observations of the same room are encountered at different times of the day (different lighting conditions). New obstacles could be present. Levels of a game could be different, yet playing a few levels should often be enough to figure out how to play the rest. Such variations might result in a new environment where the control model that defined the training environment has changed. A robust policy should generalize from its experience and perform the same skills in the presence of these variations. DRL agents have been notorious for overfitting to their training environments Cobbe et al. (2018) . An agent could have drastically different performance on testing environments even if it manages to maximize the reward during training Zhang et al. (2018a) . Supervised learning algorithms have been shown to have some generalization guarantees when adding proper regularization Mohri et al. (2018) . However, these guarantees are weakened in reinforcement learning algorithms where the source of the data is not i.i.d.. In order to make use of the progress of DRL algorithms in practice we need policies that are robust to possible changes in the sensory inputs, surrounding structure and even some aspects of the task. In this paper we study the notion of generalization that is appropriate for visual navigation control policies that are learned with DRL. We present: (1) a study of the generalization of visual control policies to certain changes in the underlying dynamical system; (2) an alternative training method that combines DRL with supervised learning, thus using DRL to learn a controller while leveraging the generalization properties of supervised learning. In our experiments we use the VizDoom platform Kempka et al. (2016) which is easily customizable and enables the generation of numerous variants of a given environment. We present a study of the generalization capabilities of visual navigation agents trained with deep reinforcement learning algorithms. We formalize what it means to generalize in the context of a POMDP. We find that the tendency of RL agent to overfit even when exposed to large training sets is quite visible. We show that using domain randomization with RL, without adding invariant features to the input such as the depth maps, is not enough to generalize. In the second part, we proposed Invariance Regularization (IR), a method that attempts to regularize the RL model with a supervised learning loss. It improves the generalization success and displays stable performance across different seeds. In this work, we focused our experimentation on generalization to changes in the input observation. However, it is also interesting to generalize the learned skills to different architectural designs of the environment, just as one one wishes to generalize to different levels of the game as proposed in the retro competition Nichol et al. (2018) . Another avenue of future work is to explore the appropriate transformation function T of the observations.One might consider an adaptive form of T learned with data augmentation Cubuk et al. (2018) or adversarial examples Goodfellow et al. (2015 The first part consists of training RL on the observations of the original training environment, while the second part can be seen as a supervised learning objective on the transformed observations, as shown in Algorithm 1. The first step trains RL on one environment and then use the actions that the trained policy would have taken in that environment to tune the model with supervised learning on the textured environments. In the reported experiments using the split version, the model is trained with one iteration of the algorithm. Therefore, the training process has two stages, train RL then train with a supervised learning setup, without iterating between both.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the issue of overfitting in artificial intelligence (AI) agents, specifically in visual navigation tasks. Overfitting occurs when an AI agent becomes too specialized to a specific environment and struggles to adapt to changes or new environments. The authors investigate this problem using reinforcement learning (RL) algorithms, which are commonly used to train AI agents to perform tasks. They find that even when trained on multiple environments, RL agents can still overfit to the training environments. To address this issue, the authors propose a new method called invariance regularization, which combines RL with supervised learning to encourage the AI agent to be more robust to changes in the environment. The results show that this method improves the generalization of the AI agent to new environments. The authors also discuss the importance of generalization in AI, as it allows agents to adapt to new situations and environments, making them more useful in real-world applications.\n\nEntities: AI agents, reinforcement learning, visual navigation, environments, invariance regularization.\n\nDates: Not mentioned.\n\nEvents: The authors investigate the problem of overfitting in RL agents, propose a new method called invariance regularization, and test its effectiveness in improving generalization.\n\nResult: The invariance regularization method improves the generalization of RL agents to new environments, making them more robust and useful in real-world applications.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nTraining agents to operate in one environment often yields overfitted models that are unable to generalize to the changes in that environment. However, due to the numerous variations that can occur in the real-world, the agent is often required to be robust in order to be useful. This has not been the case for agents trained with reinforcement learning (RL) algorithms. In this paper, we investigate the overfitting of RL agents to the training environments in visual navigation tasks. Our experiments show that deep RL agents can overfit even when trained on multiple environments simultaneously. \n We propose a regularization method which combines RL with supervised learning methods by adding a term to the RL objective that would encourage the invariance of a policy to variations in the observations that ought not to affect the action taken. The results of this method, called invariance regularization, show an improvement in the generalization of policies to environments not seen during training.\n Learning control policies from high-dimensional sensory input has been gaining more traction lately due to the popularity of deep reinforcement learning (DRL) Mnih et al. (2015) ; ; Zhang et al. (2018b) ; Rakelly et al. (2019) , which enables learning the perception and control modules simultaneously. However, most of the work done in RL chooses to evaluate the learned policies in the same environment in which training occurred Cobbe et al. (2018) . Using the same environments to train and test agents does not give any insight into the generalization abilities of the learned policy. There could be a number of changes in the environment at test time that would degrade the agent's performance. Variations could appear in the visual aspects that determine the agent's observation, the physical structure that determines the agent's state and even some aspects that are related to the agent's goal (Figure 1 ). For example, different observations of the same room are encountered at different times of the day (different lighting conditions). New obstacles could be present. Levels of a game could be different, yet playing a few levels should often be enough to figure out how to play the rest. Such variations might result in a new environment where the control model that defined the training environment has changed. A robust policy should generalize from its experience and perform the same skills in the presence of these variations. DRL agents have been notorious for overfitting to their training environments Cobbe et al. (2018) . An agent could have drastically different performance on testing environments even if it manages to maximize the reward during training Zhang et al. (2018a) . Supervised learning algorithms have been shown to have some generalization guarantees when adding proper regularization Mohri et al. (2018) . However, these guarantees are weakened in reinforcement learning algorithms where the source of the data is not i.i.d.. In order to make use of the progress of DRL algorithms in practice we need policies that are robust to possible changes in the sensory inputs, surrounding structure and even some aspects of the task. In this paper we study the notion of generalization that is appropriate for visual navigation control policies that are learned with DRL. We present: (1) a study of the generalization of visual control policies to certain changes in the underlying dynamical system; (2) an alternative training method that combines DRL with supervised learning, thus using DRL to learn a controller while leveraging the generalization properties of supervised learning. In our experiments we use the VizDoom platform Kempka et al. (2016) which is easily customizable and enables the generation of numerous variants of a given environment. We present a study of the generalization capabilities of visual navigation agents trained with deep reinforcement learning algorithms. We formalize what it means to generalize in the context of a POMDP. We find that the tendency of RL agent to overfit even when exposed to large training sets is quite visible. We show that using domain randomization with RL, without adding invariant features to the input such as the depth maps, is not enough to generalize. In the second part, we proposed Invariance Regularization (IR), a method that attempts to regularize the RL model with a supervised learning loss. It improves the generalization success and displays stable performance across different seeds. In this work, we focused our experimentation on generalization to changes in the input observation. However, it is also interesting to generalize the learned skills to different architectural designs of the environment, just as one one wishes to generalize to different levels of the game as proposed in the retro competition Nichol et al. (2018) . Another avenue of future work is to explore the appropriate transformation function T of the observations.One might consider an adaptive form of T learned with data augmentation Cubuk et al. (2018) or adversarial examples Goodfellow et al. (2015 The first part consists of training RL on the observations of the original training environment, while the second part can be seen as a supervised learning objective on the transformed observations, as shown in Algorithm 1. The first step trains RL on one environment and then use the actions that the trained policy would have taken in that environment to tune the model with supervised learning on the textured environments. In the reported experiments using the split version, the model is trained with one iteration of the algorithm. Therefore, the training process has two stages, train RL then train with a supervised learning setup, without iterating between both.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses how artificial intelligence agents, like those used in video games or robots, can be trained to perform tasks in one environment but struggle to adapt to changes in that environment. This is called overfitting. The authors of the document propose a new way to train these agents, called invariance regularization, which helps them learn to perform tasks in different environments by adding a new step to the training process. This new step encourages the agent to ignore irrelevant details in the environment and focus on the important features. The authors tested this method on a video game platform called VizDoom and found that it improved the agent's ability to generalize to new environments.\n\nThe important entities in this document are the artificial intelligence agents, the environments they are trained in, and the method of invariance regularization.\n\nThe important dates mentioned in the document are not specific dates, but rather references to previous research papers and publications.\n\nThe events happening in this document are the training of artificial intelligence agents, the testing of the agents in different environments, and the development of the invariance regularization method.\n\nThe result of these events is the improvement of the agent's ability to generalize to new environments, making it more robust and useful in real-world applications.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nTraining agents to operate in one environment often yields overfitted models that are unable to generalize to the changes in that environment. However, due to the numerous variations that can occur in the real-world, the agent is often required to be robust in order to be useful. This has not been the case for agents trained with reinforcement learning (RL) algorithms. In this paper, we investigate the overfitting of RL agents to the training environments in visual navigation tasks. Our experiments show that deep RL agents can overfit even when trained on multiple environments simultaneously. \n We propose a regularization method which combines RL with supervised learning methods by adding a term to the RL objective that would encourage the invariance of a policy to variations in the observations that ought not to affect the action taken. The results of this method, called invariance regularization, show an improvement in the generalization of policies to environments not seen during training.\n Learning control policies from high-dimensional sensory input has been gaining more traction lately due to the popularity of deep reinforcement learning (DRL) Mnih et al. (2015) ; ; Zhang et al. (2018b) ; Rakelly et al. (2019) , which enables learning the perception and control modules simultaneously. However, most of the work done in RL chooses to evaluate the learned policies in the same environment in which training occurred Cobbe et al. (2018) . Using the same environments to train and test agents does not give any insight into the generalization abilities of the learned policy. There could be a number of changes in the environment at test time that would degrade the agent's performance. Variations could appear in the visual aspects that determine the agent's observation, the physical structure that determines the agent's state and even some aspects that are related to the agent's goal (Figure 1 ). For example, different observations of the same room are encountered at different times of the day (different lighting conditions). New obstacles could be present. Levels of a game could be different, yet playing a few levels should often be enough to figure out how to play the rest. Such variations might result in a new environment where the control model that defined the training environment has changed. A robust policy should generalize from its experience and perform the same skills in the presence of these variations. DRL agents have been notorious for overfitting to their training environments Cobbe et al. (2018) . An agent could have drastically different performance on testing environments even if it manages to maximize the reward during training Zhang et al. (2018a) . Supervised learning algorithms have been shown to have some generalization guarantees when adding proper regularization Mohri et al. (2018) . However, these guarantees are weakened in reinforcement learning algorithms where the source of the data is not i.i.d.. In order to make use of the progress of DRL algorithms in practice we need policies that are robust to possible changes in the sensory inputs, surrounding structure and even some aspects of the task. In this paper we study the notion of generalization that is appropriate for visual navigation control policies that are learned with DRL. We present: (1) a study of the generalization of visual control policies to certain changes in the underlying dynamical system; (2) an alternative training method that combines DRL with supervised learning, thus using DRL to learn a controller while leveraging the generalization properties of supervised learning. In our experiments we use the VizDoom platform Kempka et al. (2016) which is easily customizable and enables the generation of numerous variants of a given environment. We present a study of the generalization capabilities of visual navigation agents trained with deep reinforcement learning algorithms. We formalize what it means to generalize in the context of a POMDP. We find that the tendency of RL agent to overfit even when exposed to large training sets is quite visible. We show that using domain randomization with RL, without adding invariant features to the input such as the depth maps, is not enough to generalize. In the second part, we proposed Invariance Regularization (IR), a method that attempts to regularize the RL model with a supervised learning loss. It improves the generalization success and displays stable performance across different seeds. In this work, we focused our experimentation on generalization to changes in the input observation. However, it is also interesting to generalize the learned skills to different architectural designs of the environment, just as one one wishes to generalize to different levels of the game as proposed in the retro competition Nichol et al. (2018) . Another avenue of future work is to explore the appropriate transformation function T of the observations.One might consider an adaptive form of T learned with data augmentation Cubuk et al. (2018) or adversarial examples Goodfellow et al. (2015 The first part consists of training RL on the observations of the original training environment, while the second part can be seen as a supervised learning objective on the transformed observations, as shown in Algorithm 1. The first step trains RL on one environment and then use the actions that the trained policy would have taken in that environment to tune the model with supervised learning on the textured environments. In the reported experiments using the split version, the model is trained with one iteration of the algorithm. Therefore, the training process has two stages, train RL then train with a supervised learning setup, without iterating between both.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the limitations of training artificial agents to operate in one environment, which often leads to overfitting and inability to generalize to changes in the environment. The authors investigate this issue in the context of visual navigation tasks and propose a new method called invariance regularization to improve the generalization of policies to unseen environments. The method combines reinforcement learning with supervised learning to encourage the policy to be invariant to variations in observations that do not affect the action taken. The authors show that deep reinforcement learning agents can overfit even when trained on multiple environments simultaneously, and that their proposed method improves the generalization of policies to environments not seen during training. The authors also discuss the importance of generalization in reinforcement learning, as agents may encounter changes in the environment that can degrade their performance. The document highlights the need for robust policies that can generalize to different environments and scenarios, and proposes future directions for research in this area.\n\nThe important entities in this document are the artificial agents, reinforcement learning algorithms, and the environments in which they operate. The important dates mentioned are not explicitly stated, but the document references several research papers published in the years 2015-2019. The events described in the document are the training of artificial agents in different environments, the overfitting of these agents, and the proposed method of invariance regularization to improve generalization. The result of these events is the development of more robust and generalizable policies that can operate effectively in a variety of environments.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 9 sentences.\n\nScholarly document\nTraining agents to operate in one environment often yields overfitted models that are unable to generalize to the changes in that environment. However, due to the numerous variations that can occur in the real-world, the agent is often required to be robust in order to be useful. This has not been the case for agents trained with reinforcement learning (RL) algorithms. In this paper, we investigate the overfitting of RL agents to the training environments in visual navigation tasks. Our experiments show that deep RL agents can overfit even when trained on multiple environments simultaneously. \n We propose a regularization method which combines RL with supervised learning methods by adding a term to the RL objective that would encourage the invariance of a policy to variations in the observations that ought not to affect the action taken. The results of this method, called invariance regularization, show an improvement in the generalization of policies to environments not seen during training.\n Learning control policies from high-dimensional sensory input has been gaining more traction lately due to the popularity of deep reinforcement learning (DRL) Mnih et al. (2015) ; ; Zhang et al. (2018b) ; Rakelly et al. (2019) , which enables learning the perception and control modules simultaneously. However, most of the work done in RL chooses to evaluate the learned policies in the same environment in which training occurred Cobbe et al. (2018) . Using the same environments to train and test agents does not give any insight into the generalization abilities of the learned policy. There could be a number of changes in the environment at test time that would degrade the agent's performance. Variations could appear in the visual aspects that determine the agent's observation, the physical structure that determines the agent's state and even some aspects that are related to the agent's goal (Figure 1 ). For example, different observations of the same room are encountered at different times of the day (different lighting conditions). New obstacles could be present. Levels of a game could be different, yet playing a few levels should often be enough to figure out how to play the rest. Such variations might result in a new environment where the control model that defined the training environment has changed. A robust policy should generalize from its experience and perform the same skills in the presence of these variations. DRL agents have been notorious for overfitting to their training environments Cobbe et al. (2018) . An agent could have drastically different performance on testing environments even if it manages to maximize the reward during training Zhang et al. (2018a) . Supervised learning algorithms have been shown to have some generalization guarantees when adding proper regularization Mohri et al. (2018) . However, these guarantees are weakened in reinforcement learning algorithms where the source of the data is not i.i.d.. In order to make use of the progress of DRL algorithms in practice we need policies that are robust to possible changes in the sensory inputs, surrounding structure and even some aspects of the task. In this paper we study the notion of generalization that is appropriate for visual navigation control policies that are learned with DRL. We present: (1) a study of the generalization of visual control policies to certain changes in the underlying dynamical system; (2) an alternative training method that combines DRL with supervised learning, thus using DRL to learn a controller while leveraging the generalization properties of supervised learning. In our experiments we use the VizDoom platform Kempka et al. (2016) which is easily customizable and enables the generation of numerous variants of a given environment. We present a study of the generalization capabilities of visual navigation agents trained with deep reinforcement learning algorithms. We formalize what it means to generalize in the context of a POMDP. We find that the tendency of RL agent to overfit even when exposed to large training sets is quite visible. We show that using domain randomization with RL, without adding invariant features to the input such as the depth maps, is not enough to generalize. In the second part, we proposed Invariance Regularization (IR), a method that attempts to regularize the RL model with a supervised learning loss. It improves the generalization success and displays stable performance across different seeds. In this work, we focused our experimentation on generalization to changes in the input observation. However, it is also interesting to generalize the learned skills to different architectural designs of the environment, just as one one wishes to generalize to different levels of the game as proposed in the retro competition Nichol et al. (2018) . Another avenue of future work is to explore the appropriate transformation function T of the observations.One might consider an adaptive form of T learned with data augmentation Cubuk et al. (2018) or adversarial examples Goodfellow et al. (2015 The first part consists of training RL on the observations of the original training environment, while the second part can be seen as a supervised learning objective on the transformed observations, as shown in Algorithm 1. The first step trains RL on one environment and then use the actions that the trained policy would have taken in that environment to tune the model with supervised learning on the textured environments. In the reported experiments using the split version, the model is trained with one iteration of the algorithm. Therefore, the training process has two stages, train RL then train with a supervised learning setup, without iterating between both.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nDeep learning agents, like those used in video games, are trained to perform well in one specific environment. However, in the real world, environments can change, and the agents need to be able to adapt. The problem is that these agents often overfit to their training environment, meaning they don't generalize well to new situations. This is a big problem because agents need to be robust to changes in their environment, such as different lighting conditions, new obstacles, or different levels of a game.\n\nTo address this issue, the researchers proposed a new method called invariance regularization, which combines reinforcement learning with supervised learning. This method encourages the agent to be invariant to variations in the observations that don't affect the action taken. The results show that this method improves the generalization of policies to environments not seen during training.\n\nThe researchers also studied the generalization of visual control policies to changes in the underlying dynamical system and presented a new training method that combines deep reinforcement learning with supervised learning. They used the VizDoom platform to generate numerous variants of a given environment and found that the tendency of RL agents to overfit is quite visible.\n\nThe researchers proposed a method called Invariance Regularization (IR), which attempts to regularize the RL model with a supervised learning loss. This method improves the generalization success and displays stable performance across different seeds.\n\nIn summary, the researchers aimed to address the overfitting problem in deep learning agents by proposing a new method that combines reinforcement learning with supervised learning. The results show that this method improves the generalization of policies to environments not seen during training.\n\nThe important entities in this document are the deep learning agents, reinforcement learning algorithms, and supervised learning methods.\n\nThe important dates in this document are not explicitly mentioned, but the researchers cite several papers from 2015 to 2019.\n\nThe events happening in this document are the training of deep learning agents, the overfitting problem, and the proposal of a new method to address this problem.\n\nThe result of these events is the development of a new method that combines reinforcement learning with supervised learning, which improves the generalization of policies to environments not seen during training."
        }
    },
    {
        "id": 392,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nUnsupervised embedding learning aims to extract good representations from data without the use of human-annotated labels. Such techniques are apparently in the limelight because of the challenges in collecting massive-scale labels required for supervised learning. This paper proposes a comprehensive approach, called Super-AND, which is based on the Anchor Neighbourhood Discovery model. Multiple losses defined in Super-AND make similar samples gather even within a low-density space and keep features invariant against augmentation. As a result, our model outperforms existing approaches in various benchmark datasets and achieves an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art. Deep learning and convolutional neural network have become an indispensable technique in computer vision (LeCun et al., 2015; Krizhevsky et al., 2012; Lawrence et al., 1997) . Remarkable developments, in particular, were led by supervised learning that requires thousands or more labeled data. However, high annotation costs have become a significant drawback in training a scalable and practical model in many domains. In contrast, unsupervised deep learning that requires no label has recently started to get attention in computer vision tasks. From clustering analysis (Caron et al., 2018; Ji et al., 2018) , and self-supervised model (Gidaris et al., 2018; Bojanowski & Joulin, 2017) to generative model (Goodfellow et al., 2014; Kingma & Welling, 2013; Radford et al., 2016) , various learning methods came out and showed possibilities and prospects. Unsupervised embedding learning aims to extract visually meaningful representations without any label information. Here \"visually meaningful\" refers to finding features that satisfy two traits: (i) positive attention and (ii) negative separation (Ye et al., 2019; Zhang et al., 2017c; Oh Song et al., 2016) . Data samples from the same ground truth class, i.e., positive samples, should be close in the embedding space (Fig. 1a) ; whereas those from different classes, i.e., negative samples, should be pushed far away in the embedding space (Fig. 1b) . However, in the setting of unsupervised learning, a model cannot have knowledge about whether given data points are positive samples or negative samples. Several new methods have been proposed to find 'visually meaningful' representations. The sample specificity method considers all data points as negative samples and separates them in the feature space (Wu et al., 2018; Bojanowski & Joulin, 2017) . Although this method achieves high performance, its decisions are known to be biased from learning only from negative separation. One approach utilizes data augmentation to consider positive samples in training (Ye et al., 2019) , which efficiently reduces any ambiguity in supervision while keeping invariant features in the embedding space. Another approach is called the Anchor Neighborhood Discovery (AND) model, which alleviates the complexity in boundaries by discovering the nearest neighbor among the data points (Huang et al., 2019) . Each of these approaches overcomes different limitations of the sample specificity method. However, no unified approach has been proposed. This paper presents a holistic method for unsupervised embedding learning, named Super-AND. Super-AND extends the AND algorithm and unifies various but dominant approaches in this domain with its unique architecture. Our proposed model not only focuses on learning distinctive features across neighborhoods, but also emphasizes edge information in embeddings and maintains the unchanging class information from the augmented data. Besides combining existing techniques, we newly introduce Unification Entropy loss (UE-loss), an adversary of sample specificity loss, which is able to gather similar data points within a low-density space. Extensive experiments are conducted on several benchmark datasets to verify the superiority of the model. The results show the synergetic advantages among modules of Super-AND. The main contributions of this paper are as follows: \u2022 We effectively unify various techniques from state-of-the-art models and introduce a new loss, UE-loss, to make similar data samples gather in the low-density space. \u2022 Super-AND outperforms all baselines in various benchmark datasets. It achieved an accuracy of 89.2% in the CIFAR-10 dataset with the ResNet18 backbone network, compared to the state-of-the-art that gained 86.3%. \u2022 The extensive experiments and the ablation study show that every component in Super-AND contributes to the performance increase, and also indicate their synergies are critical. Our model's outstanding performance is a step closer to the broader adoption of unsupervised techniques in computer vision tasks. The premise of data-less embedding learning is at its applicability to practical scenarios, where there exists only one or two examples per cluster. Codes and trained data for Super-AND are accessible via a GitHub link. Generative model. This type of model is a powerful branch in unsupervised learning. By reconstructing the underlying data distribution, a model can generate new data points as well as features from images without labels. Generative adversarial network (Goodfellow et al., 2014) has led to rapid progress in image generation problems Arjovsky et al., 2017) . While some attempts have been made in terms of unsupervised embedding learning (Radford et al., 2016) , the main objective of generative models lies at mimicking the true distribution of each class, rather than discovering distinctive categorical information the data contains. Self-supervised learning. This type of learning uses inherent structures in images as pseudo-labels and exploits labels for back-propagation. For example, a model can be trained to create embeddings by predicting the relative position of a pixel from other pixels (Doersch et al., 2015) or the degree of changes after rotating images (Gidaris et al., 2018) . Predicting future frames of a video can benefit from this technique (Walker et al., 2016) . Wu et al. (2018) proposed the sample specificity method that learns feature representation from capturing apparent discriminability among instances. All of these methods are suitable for unsupervised embedding learning, although there exists a risk of false knowledge from generated labels that weakly correlate with the underlying class information. Learning invariants from augmentation. Data augmentation is a strategy that enables a model to learn from datasets with an increased variety of instances. Popular techniques include flipping, scaling, rotation, and grey-scaling. These techniques do not deform any crucial features of data, but only change the style of images. Some studies hence use augmentation techniques and train models Clustering analysis. This type of analysis is an extensively studied area in unsupervised learning, whose main objective is to group similar objects into the same class. Many studies either leveraged deep learning for dimensionality reduction before clustering (Schroff et al., 2015; Baldi, 2012) or trained models in an end-to-end fashion (Xie et al., 2016; Yang et al., 2016) . Caron et al. (2018) proposed a concept called deep cluster, an iterative method that updates its weights by predicting cluster assignments as pseudo-labels. However, directly reasoning the global structures without any label is error-prone. The AND model, which we extend in this work, combines the advantages of sample specificity and clustering strategy to mitigate the noisy supervision via neighborhood analysis (Huang et al., 2019) . This paper presents Super-AND, a holistic technique for unsupervised embedding learning. Besides the synergetic advantage combining existing methods brings, the newly proposed UE-loss that groups nearby data points even in a low-density space while maintaining invariant features via data augmentation. The experiments with both coarse-grained and fine-grained datasets demonstrate our model's outstanding performance against the state-of-the-art models. Our efforts to advance unsupervised embedding learning directly benefit future applications that rely on various image clustering tasks. The high accuracy achieved by Super-AND makes the unsupervised learning approach an economically viable option where labels are costly to generate.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere are the answers to the questions:\n\n1. Important entities in this document are: unsupervised embedding learning, Super-AND, Anchor Neighbourhood Discovery model, CIFAR-10, Resnet18 backbone network, convolutional neural network, clustering analysis, self-supervised model, generative model, data augmentation, and various benchmark datasets.\n2. Important dates in this document are: 2012, 2014, 2015, 2016, 2017, 2018, and 2019.\n3. Events happening in this document are: the development of unsupervised deep learning, the introduction of various learning methods such as clustering analysis, self-supervised model, and generative model, the proposal of the Super-AND model, and the comparison of its performance with state-of-the-art models on various benchmark datasets.\n4. The result of these events is that the Super-AND model outperforms existing approaches in various benchmark datasets and achieves an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art.\n\nHere is a summary of the document for a high school student:\n\nUnsupervised embedding learning is a type of deep learning that helps computers understand images without needing human labels. This is important because collecting labels can be very time-consuming and expensive. The Super-AND model is a new approach that combines several existing methods to improve the performance of unsupervised embedding learning. It uses a technique called data augmentation to make the model more robust and invariant to changes in the images. The model is tested on several benchmark datasets and outperforms existing approaches, achieving an accuracy of 89.2% on the CIFAR-10 dataset. This is a significant improvement over the state-of-the-art and shows that unsupervised embedding learning can be a powerful tool for image analysis.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nUnsupervised embedding learning aims to extract good representations from data without the use of human-annotated labels. Such techniques are apparently in the limelight because of the challenges in collecting massive-scale labels required for supervised learning. This paper proposes a comprehensive approach, called Super-AND, which is based on the Anchor Neighbourhood Discovery model. Multiple losses defined in Super-AND make similar samples gather even within a low-density space and keep features invariant against augmentation. As a result, our model outperforms existing approaches in various benchmark datasets and achieves an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art. Deep learning and convolutional neural network have become an indispensable technique in computer vision (LeCun et al., 2015; Krizhevsky et al., 2012; Lawrence et al., 1997) . Remarkable developments, in particular, were led by supervised learning that requires thousands or more labeled data. However, high annotation costs have become a significant drawback in training a scalable and practical model in many domains. In contrast, unsupervised deep learning that requires no label has recently started to get attention in computer vision tasks. From clustering analysis (Caron et al., 2018; Ji et al., 2018) , and self-supervised model (Gidaris et al., 2018; Bojanowski & Joulin, 2017) to generative model (Goodfellow et al., 2014; Kingma & Welling, 2013; Radford et al., 2016) , various learning methods came out and showed possibilities and prospects. Unsupervised embedding learning aims to extract visually meaningful representations without any label information. Here \"visually meaningful\" refers to finding features that satisfy two traits: (i) positive attention and (ii) negative separation (Ye et al., 2019; Zhang et al., 2017c; Oh Song et al., 2016) . Data samples from the same ground truth class, i.e., positive samples, should be close in the embedding space (Fig. 1a) ; whereas those from different classes, i.e., negative samples, should be pushed far away in the embedding space (Fig. 1b) . However, in the setting of unsupervised learning, a model cannot have knowledge about whether given data points are positive samples or negative samples. Several new methods have been proposed to find 'visually meaningful' representations. The sample specificity method considers all data points as negative samples and separates them in the feature space (Wu et al., 2018; Bojanowski & Joulin, 2017) . Although this method achieves high performance, its decisions are known to be biased from learning only from negative separation. One approach utilizes data augmentation to consider positive samples in training (Ye et al., 2019) , which efficiently reduces any ambiguity in supervision while keeping invariant features in the embedding space. Another approach is called the Anchor Neighborhood Discovery (AND) model, which alleviates the complexity in boundaries by discovering the nearest neighbor among the data points (Huang et al., 2019) . Each of these approaches overcomes different limitations of the sample specificity method. However, no unified approach has been proposed. This paper presents a holistic method for unsupervised embedding learning, named Super-AND. Super-AND extends the AND algorithm and unifies various but dominant approaches in this domain with its unique architecture. Our proposed model not only focuses on learning distinctive features across neighborhoods, but also emphasizes edge information in embeddings and maintains the unchanging class information from the augmented data. Besides combining existing techniques, we newly introduce Unification Entropy loss (UE-loss), an adversary of sample specificity loss, which is able to gather similar data points within a low-density space. Extensive experiments are conducted on several benchmark datasets to verify the superiority of the model. The results show the synergetic advantages among modules of Super-AND. The main contributions of this paper are as follows: \u2022 We effectively unify various techniques from state-of-the-art models and introduce a new loss, UE-loss, to make similar data samples gather in the low-density space. \u2022 Super-AND outperforms all baselines in various benchmark datasets. It achieved an accuracy of 89.2% in the CIFAR-10 dataset with the ResNet18 backbone network, compared to the state-of-the-art that gained 86.3%. \u2022 The extensive experiments and the ablation study show that every component in Super-AND contributes to the performance increase, and also indicate their synergies are critical. Our model's outstanding performance is a step closer to the broader adoption of unsupervised techniques in computer vision tasks. The premise of data-less embedding learning is at its applicability to practical scenarios, where there exists only one or two examples per cluster. Codes and trained data for Super-AND are accessible via a GitHub link. Generative model. This type of model is a powerful branch in unsupervised learning. By reconstructing the underlying data distribution, a model can generate new data points as well as features from images without labels. Generative adversarial network (Goodfellow et al., 2014) has led to rapid progress in image generation problems Arjovsky et al., 2017) . While some attempts have been made in terms of unsupervised embedding learning (Radford et al., 2016) , the main objective of generative models lies at mimicking the true distribution of each class, rather than discovering distinctive categorical information the data contains. Self-supervised learning. This type of learning uses inherent structures in images as pseudo-labels and exploits labels for back-propagation. For example, a model can be trained to create embeddings by predicting the relative position of a pixel from other pixels (Doersch et al., 2015) or the degree of changes after rotating images (Gidaris et al., 2018) . Predicting future frames of a video can benefit from this technique (Walker et al., 2016) . Wu et al. (2018) proposed the sample specificity method that learns feature representation from capturing apparent discriminability among instances. All of these methods are suitable for unsupervised embedding learning, although there exists a risk of false knowledge from generated labels that weakly correlate with the underlying class information. Learning invariants from augmentation. Data augmentation is a strategy that enables a model to learn from datasets with an increased variety of instances. Popular techniques include flipping, scaling, rotation, and grey-scaling. These techniques do not deform any crucial features of data, but only change the style of images. Some studies hence use augmentation techniques and train models Clustering analysis. This type of analysis is an extensively studied area in unsupervised learning, whose main objective is to group similar objects into the same class. Many studies either leveraged deep learning for dimensionality reduction before clustering (Schroff et al., 2015; Baldi, 2012) or trained models in an end-to-end fashion (Xie et al., 2016; Yang et al., 2016) . Caron et al. (2018) proposed a concept called deep cluster, an iterative method that updates its weights by predicting cluster assignments as pseudo-labels. However, directly reasoning the global structures without any label is error-prone. The AND model, which we extend in this work, combines the advantages of sample specificity and clustering strategy to mitigate the noisy supervision via neighborhood analysis (Huang et al., 2019) . This paper presents Super-AND, a holistic technique for unsupervised embedding learning. Besides the synergetic advantage combining existing methods brings, the newly proposed UE-loss that groups nearby data points even in a low-density space while maintaining invariant features via data augmentation. The experiments with both coarse-grained and fine-grained datasets demonstrate our model's outstanding performance against the state-of-the-art models. Our efforts to advance unsupervised embedding learning directly benefit future applications that rely on various image clustering tasks. The high accuracy achieved by Super-AND makes the unsupervised learning approach an economically viable option where labels are costly to generate.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere are the answers to the questions:\n\n1. Important entities in this document are:\n\t* Super-AND: a comprehensive approach for unsupervised embedding learning\n\t* Anchor Neighbourhood Discovery (AND) model: a model used in Super-AND\n\t* Unification Entropy loss (UE-loss): a new loss function introduced in Super-AND\n\t* Resnet18 backbone network: a neural network architecture used in the experiments\n\t* CIFAR-10 dataset: a benchmark dataset used in the experiments\n2. Important dates in this document are:\n\t* 2012 (Krizhevsky et al.)\n\t* 2014 (Goodfellow et al.)\n\t* 2015 (LeCun et al., Doersch et al.)\n\t* 2016 (Radford et al., Walker et al., Xie et al., Yang et al.)\n\t* 2017 (Bojanowski & Joulin, Arjovsky et al.)\n\t* 2018 (Caron et al., Gidaris et al., Ji et al., Wu et al., Huang et al.)\n\t* 2019 (Ye et al., Zhang et al., Oh Song et al.)\n3. Events happening in this document are:\n\t* Unsupervised embedding learning aims to extract visually meaningful representations without human-annotated labels\n\t* Super-AND is proposed as a comprehensive approach for unsupervised embedding learning\n\t* The AND model is extended and combined with other techniques to form Super-AND\n\t* UE-loss is introduced to group nearby data points in a low-density space while maintaining invariant features\n\t* Experiments are conducted on various benchmark datasets to verify the superiority of Super-AND\n4. The result of these events is:\n\t* Super-AND outperforms existing approaches in various benchmark datasets, achieving an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art\n\nHere is a summary of the document for a high school student:\n\nUnsupervised embedding learning is a way to extract important features from data without needing human-annotated labels. This is useful when it's hard to get labeled data. The paper proposes a new approach called Super-AND, which combines different techniques to learn visually meaningful representations. Super-AND uses a model called AND, which looks at the neighborhood of each data point to learn features. It also introduces a new loss function called UE-loss, which helps group similar data points together. The authors tested Super-AND on several datasets and found that it outperformed existing approaches, achieving an accuracy of 89.2% on the CIFAR-10 dataset. This is a significant improvement over the state-of-the-art, and it shows that unsupervised learning can be a powerful tool for extracting meaningful features from data.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nUnsupervised embedding learning aims to extract good representations from data without the use of human-annotated labels. Such techniques are apparently in the limelight because of the challenges in collecting massive-scale labels required for supervised learning. This paper proposes a comprehensive approach, called Super-AND, which is based on the Anchor Neighbourhood Discovery model. Multiple losses defined in Super-AND make similar samples gather even within a low-density space and keep features invariant against augmentation. As a result, our model outperforms existing approaches in various benchmark datasets and achieves an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art. Deep learning and convolutional neural network have become an indispensable technique in computer vision (LeCun et al., 2015; Krizhevsky et al., 2012; Lawrence et al., 1997) . Remarkable developments, in particular, were led by supervised learning that requires thousands or more labeled data. However, high annotation costs have become a significant drawback in training a scalable and practical model in many domains. In contrast, unsupervised deep learning that requires no label has recently started to get attention in computer vision tasks. From clustering analysis (Caron et al., 2018; Ji et al., 2018) , and self-supervised model (Gidaris et al., 2018; Bojanowski & Joulin, 2017) to generative model (Goodfellow et al., 2014; Kingma & Welling, 2013; Radford et al., 2016) , various learning methods came out and showed possibilities and prospects. Unsupervised embedding learning aims to extract visually meaningful representations without any label information. Here \"visually meaningful\" refers to finding features that satisfy two traits: (i) positive attention and (ii) negative separation (Ye et al., 2019; Zhang et al., 2017c; Oh Song et al., 2016) . Data samples from the same ground truth class, i.e., positive samples, should be close in the embedding space (Fig. 1a) ; whereas those from different classes, i.e., negative samples, should be pushed far away in the embedding space (Fig. 1b) . However, in the setting of unsupervised learning, a model cannot have knowledge about whether given data points are positive samples or negative samples. Several new methods have been proposed to find 'visually meaningful' representations. The sample specificity method considers all data points as negative samples and separates them in the feature space (Wu et al., 2018; Bojanowski & Joulin, 2017) . Although this method achieves high performance, its decisions are known to be biased from learning only from negative separation. One approach utilizes data augmentation to consider positive samples in training (Ye et al., 2019) , which efficiently reduces any ambiguity in supervision while keeping invariant features in the embedding space. Another approach is called the Anchor Neighborhood Discovery (AND) model, which alleviates the complexity in boundaries by discovering the nearest neighbor among the data points (Huang et al., 2019) . Each of these approaches overcomes different limitations of the sample specificity method. However, no unified approach has been proposed. This paper presents a holistic method for unsupervised embedding learning, named Super-AND. Super-AND extends the AND algorithm and unifies various but dominant approaches in this domain with its unique architecture. Our proposed model not only focuses on learning distinctive features across neighborhoods, but also emphasizes edge information in embeddings and maintains the unchanging class information from the augmented data. Besides combining existing techniques, we newly introduce Unification Entropy loss (UE-loss), an adversary of sample specificity loss, which is able to gather similar data points within a low-density space. Extensive experiments are conducted on several benchmark datasets to verify the superiority of the model. The results show the synergetic advantages among modules of Super-AND. The main contributions of this paper are as follows: \u2022 We effectively unify various techniques from state-of-the-art models and introduce a new loss, UE-loss, to make similar data samples gather in the low-density space. \u2022 Super-AND outperforms all baselines in various benchmark datasets. It achieved an accuracy of 89.2% in the CIFAR-10 dataset with the ResNet18 backbone network, compared to the state-of-the-art that gained 86.3%. \u2022 The extensive experiments and the ablation study show that every component in Super-AND contributes to the performance increase, and also indicate their synergies are critical. Our model's outstanding performance is a step closer to the broader adoption of unsupervised techniques in computer vision tasks. The premise of data-less embedding learning is at its applicability to practical scenarios, where there exists only one or two examples per cluster. Codes and trained data for Super-AND are accessible via a GitHub link. Generative model. This type of model is a powerful branch in unsupervised learning. By reconstructing the underlying data distribution, a model can generate new data points as well as features from images without labels. Generative adversarial network (Goodfellow et al., 2014) has led to rapid progress in image generation problems Arjovsky et al., 2017) . While some attempts have been made in terms of unsupervised embedding learning (Radford et al., 2016) , the main objective of generative models lies at mimicking the true distribution of each class, rather than discovering distinctive categorical information the data contains. Self-supervised learning. This type of learning uses inherent structures in images as pseudo-labels and exploits labels for back-propagation. For example, a model can be trained to create embeddings by predicting the relative position of a pixel from other pixels (Doersch et al., 2015) or the degree of changes after rotating images (Gidaris et al., 2018) . Predicting future frames of a video can benefit from this technique (Walker et al., 2016) . Wu et al. (2018) proposed the sample specificity method that learns feature representation from capturing apparent discriminability among instances. All of these methods are suitable for unsupervised embedding learning, although there exists a risk of false knowledge from generated labels that weakly correlate with the underlying class information. Learning invariants from augmentation. Data augmentation is a strategy that enables a model to learn from datasets with an increased variety of instances. Popular techniques include flipping, scaling, rotation, and grey-scaling. These techniques do not deform any crucial features of data, but only change the style of images. Some studies hence use augmentation techniques and train models Clustering analysis. This type of analysis is an extensively studied area in unsupervised learning, whose main objective is to group similar objects into the same class. Many studies either leveraged deep learning for dimensionality reduction before clustering (Schroff et al., 2015; Baldi, 2012) or trained models in an end-to-end fashion (Xie et al., 2016; Yang et al., 2016) . Caron et al. (2018) proposed a concept called deep cluster, an iterative method that updates its weights by predicting cluster assignments as pseudo-labels. However, directly reasoning the global structures without any label is error-prone. The AND model, which we extend in this work, combines the advantages of sample specificity and clustering strategy to mitigate the noisy supervision via neighborhood analysis (Huang et al., 2019) . This paper presents Super-AND, a holistic technique for unsupervised embedding learning. Besides the synergetic advantage combining existing methods brings, the newly proposed UE-loss that groups nearby data points even in a low-density space while maintaining invariant features via data augmentation. The experiments with both coarse-grained and fine-grained datasets demonstrate our model's outstanding performance against the state-of-the-art models. Our efforts to advance unsupervised embedding learning directly benefit future applications that rely on various image clustering tasks. The high accuracy achieved by Super-AND makes the unsupervised learning approach an economically viable option where labels are costly to generate.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere are the answers to the questions:\n\n1. The important entities in this document are:\n\t* Unsupervised embedding learning\n\t* Super-AND (the proposed model)\n\t* Anchor Neighbourhood Discovery (AND) model\n\t* CIFAR-10 dataset\n\t* Resnet18 backbone network\n\t* Generative model\n\t* Self-supervised learning\n\t* Clustering analysis\n\t* Deep learning\n\t* Convolutional neural network\n2. The important dates in this document are:\n\t* 2012 (Krizhevsky et al.)\n\t* 2014 (Goodfellow et al., Radford et al.)\n\t* 2015 (LeCun et al., Doersch et al.)\n\t* 2016 (Walker et al., Xie et al., Yang et al.)\n\t* 2017 (Arjovsky et al., Bojanowski & Joulin)\n\t* 2018 (Caron et al., Gidaris et al., Wu et al., Huang et al.)\n\t* 2019 (Ye et al., Zhang et al., Oh Song et al.)\n3. The events happening in this document are:\n\t* The development of unsupervised embedding learning techniques\n\t* The proposal of the Super-AND model\n\t* The comparison of Super-AND with existing models\n\t* The demonstration of the superiority of Super-AND in various benchmark datasets\n4. The result of these events is:\n\t* The development of a new unsupervised embedding learning model, Super-AND, which outperforms existing models in various benchmark datasets, achieving an accuracy of 89.2% in the CIFAR-10 dataset with the Resnet18 backbone network.\n\nHere is a summary of the document for a high school student:\n\nUnsupervised embedding learning is a technique used in computer vision that helps machines learn from images without using labeled data. The goal is to extract meaningful features from the images, which can be used for various tasks such as image clustering and recognition. In this paper, the authors propose a new model called Super-AND, which combines the advantages of existing techniques to achieve better results. Super-AND uses a combination of losses to make similar images gather together in the feature space and keep the features invariant against image augmentation. The authors tested Super-AND on various benchmark datasets and achieved an accuracy of 89.2% in the CIFAR-10 dataset, outperforming existing models. This is a significant breakthrough in the field of unsupervised embedding learning, as it makes it possible to train models without labeled data, which can be expensive and time-consuming to obtain.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 16 sentences.\n\nScholarly document\nUnsupervised embedding learning aims to extract good representations from data without the use of human-annotated labels. Such techniques are apparently in the limelight because of the challenges in collecting massive-scale labels required for supervised learning. This paper proposes a comprehensive approach, called Super-AND, which is based on the Anchor Neighbourhood Discovery model. Multiple losses defined in Super-AND make similar samples gather even within a low-density space and keep features invariant against augmentation. As a result, our model outperforms existing approaches in various benchmark datasets and achieves an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art. Deep learning and convolutional neural network have become an indispensable technique in computer vision (LeCun et al., 2015; Krizhevsky et al., 2012; Lawrence et al., 1997) . Remarkable developments, in particular, were led by supervised learning that requires thousands or more labeled data. However, high annotation costs have become a significant drawback in training a scalable and practical model in many domains. In contrast, unsupervised deep learning that requires no label has recently started to get attention in computer vision tasks. From clustering analysis (Caron et al., 2018; Ji et al., 2018) , and self-supervised model (Gidaris et al., 2018; Bojanowski & Joulin, 2017) to generative model (Goodfellow et al., 2014; Kingma & Welling, 2013; Radford et al., 2016) , various learning methods came out and showed possibilities and prospects. Unsupervised embedding learning aims to extract visually meaningful representations without any label information. Here \"visually meaningful\" refers to finding features that satisfy two traits: (i) positive attention and (ii) negative separation (Ye et al., 2019; Zhang et al., 2017c; Oh Song et al., 2016) . Data samples from the same ground truth class, i.e., positive samples, should be close in the embedding space (Fig. 1a) ; whereas those from different classes, i.e., negative samples, should be pushed far away in the embedding space (Fig. 1b) . However, in the setting of unsupervised learning, a model cannot have knowledge about whether given data points are positive samples or negative samples. Several new methods have been proposed to find 'visually meaningful' representations. The sample specificity method considers all data points as negative samples and separates them in the feature space (Wu et al., 2018; Bojanowski & Joulin, 2017) . Although this method achieves high performance, its decisions are known to be biased from learning only from negative separation. One approach utilizes data augmentation to consider positive samples in training (Ye et al., 2019) , which efficiently reduces any ambiguity in supervision while keeping invariant features in the embedding space. Another approach is called the Anchor Neighborhood Discovery (AND) model, which alleviates the complexity in boundaries by discovering the nearest neighbor among the data points (Huang et al., 2019) . Each of these approaches overcomes different limitations of the sample specificity method. However, no unified approach has been proposed. This paper presents a holistic method for unsupervised embedding learning, named Super-AND. Super-AND extends the AND algorithm and unifies various but dominant approaches in this domain with its unique architecture. Our proposed model not only focuses on learning distinctive features across neighborhoods, but also emphasizes edge information in embeddings and maintains the unchanging class information from the augmented data. Besides combining existing techniques, we newly introduce Unification Entropy loss (UE-loss), an adversary of sample specificity loss, which is able to gather similar data points within a low-density space. Extensive experiments are conducted on several benchmark datasets to verify the superiority of the model. The results show the synergetic advantages among modules of Super-AND. The main contributions of this paper are as follows: \u2022 We effectively unify various techniques from state-of-the-art models and introduce a new loss, UE-loss, to make similar data samples gather in the low-density space. \u2022 Super-AND outperforms all baselines in various benchmark datasets. It achieved an accuracy of 89.2% in the CIFAR-10 dataset with the ResNet18 backbone network, compared to the state-of-the-art that gained 86.3%. \u2022 The extensive experiments and the ablation study show that every component in Super-AND contributes to the performance increase, and also indicate their synergies are critical. Our model's outstanding performance is a step closer to the broader adoption of unsupervised techniques in computer vision tasks. The premise of data-less embedding learning is at its applicability to practical scenarios, where there exists only one or two examples per cluster. Codes and trained data for Super-AND are accessible via a GitHub link. Generative model. This type of model is a powerful branch in unsupervised learning. By reconstructing the underlying data distribution, a model can generate new data points as well as features from images without labels. Generative adversarial network (Goodfellow et al., 2014) has led to rapid progress in image generation problems Arjovsky et al., 2017) . While some attempts have been made in terms of unsupervised embedding learning (Radford et al., 2016) , the main objective of generative models lies at mimicking the true distribution of each class, rather than discovering distinctive categorical information the data contains. Self-supervised learning. This type of learning uses inherent structures in images as pseudo-labels and exploits labels for back-propagation. For example, a model can be trained to create embeddings by predicting the relative position of a pixel from other pixels (Doersch et al., 2015) or the degree of changes after rotating images (Gidaris et al., 2018) . Predicting future frames of a video can benefit from this technique (Walker et al., 2016) . Wu et al. (2018) proposed the sample specificity method that learns feature representation from capturing apparent discriminability among instances. All of these methods are suitable for unsupervised embedding learning, although there exists a risk of false knowledge from generated labels that weakly correlate with the underlying class information. Learning invariants from augmentation. Data augmentation is a strategy that enables a model to learn from datasets with an increased variety of instances. Popular techniques include flipping, scaling, rotation, and grey-scaling. These techniques do not deform any crucial features of data, but only change the style of images. Some studies hence use augmentation techniques and train models Clustering analysis. This type of analysis is an extensively studied area in unsupervised learning, whose main objective is to group similar objects into the same class. Many studies either leveraged deep learning for dimensionality reduction before clustering (Schroff et al., 2015; Baldi, 2012) or trained models in an end-to-end fashion (Xie et al., 2016; Yang et al., 2016) . Caron et al. (2018) proposed a concept called deep cluster, an iterative method that updates its weights by predicting cluster assignments as pseudo-labels. However, directly reasoning the global structures without any label is error-prone. The AND model, which we extend in this work, combines the advantages of sample specificity and clustering strategy to mitigate the noisy supervision via neighborhood analysis (Huang et al., 2019) . This paper presents Super-AND, a holistic technique for unsupervised embedding learning. Besides the synergetic advantage combining existing methods brings, the newly proposed UE-loss that groups nearby data points even in a low-density space while maintaining invariant features via data augmentation. The experiments with both coarse-grained and fine-grained datasets demonstrate our model's outstanding performance against the state-of-the-art models. Our efforts to advance unsupervised embedding learning directly benefit future applications that rely on various image clustering tasks. The high accuracy achieved by Super-AND makes the unsupervised learning approach an economically viable option where labels are costly to generate.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere are the answers to the questions:\n\n1. The important entities in this document are:\n\t* Super-AND: a comprehensive approach for unsupervised embedding learning\n\t* Anchor Neighbourhood Discovery (AND) model\n\t* Unification Entropy loss (UE-loss)\n\t* Resnet18 backbone network\n\t* CIFAR-10 dataset\n\t* Generative model\n\t* Self-supervised learning\n\t* Clustering analysis\n2. The important dates in this document are:\n\t* 2012: Krizhevsky et al. published a paper on convolutional neural networks\n\t* 2014: Goodfellow et al. published a paper on generative adversarial networks\n\t* 2015: LeCun et al. published a paper on convolutional neural networks\n\t* 2016: Radford et al. published a paper on generative models\n\t* 2017: Bojanowski & Joulin published a paper on self-supervised learning\n\t* 2018: Various papers were published on clustering analysis, self-supervised learning, and generative models\n3. The events happening in this document are:\n\t* The development of unsupervised embedding learning techniques\n\t* The proposal of Super-AND, a comprehensive approach for unsupervised embedding learning\n\t* The comparison of Super-AND with existing approaches\n\t* The achievement of high accuracy in various benchmark datasets\n4. The result of these events is:\n\t* The development of a new approach for unsupervised embedding learning that outperforms existing methods\n\t* The potential for unsupervised embedding learning to be applied in practical scenarios where labels are costly to generate\n\nHere is a summary of the document that embeds the answers to the questions:\n\nUnsupervised embedding learning is a technique that aims to extract good representations from data without the use of human-annotated labels. This approach is gaining popularity due to the challenges in collecting massive-scale labels required for supervised learning. In this paper, we propose a comprehensive approach called Super-AND, which is based on the Anchor Neighbourhood Discovery model. Super-AND makes similar samples gather even within a low-density space and keeps features invariant against augmentation. Our model outperforms existing approaches in various benchmark datasets, achieving an accuracy of 89.2% in the CIFAR-10 dataset with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art. The development of unsupervised embedding learning techniques has been led by supervised learning, which requires thousands or more labeled data. However, high annotation costs have become a significant drawback in training a scalable and practical model in many domains. In contrast, unsupervised deep learning that requires no label has recently started to gain attention in computer vision tasks. The proposed Super-AND model combines the advantages of sample specificity and clustering strategy to mitigate the noisy supervision via neighborhood analysis. The newly proposed UE-loss that groups nearby data points even in a low-density space while maintaining invariant features via data augmentation is a key component of Super-AND. The experiments with both coarse-grained and fine-grained datasets demonstrate our model's outstanding performance against the state-of-the-art models."
        }
    },
    {
        "id": 963,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nWe present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning. We have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised representations on graph-structured data. By leveraging local mutual information maximization across the graph's patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph. This enables competitive performance across a variety of both transductive and inductive classification tasks, at times even outperforming relevant supervised architectures.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document that embeds the answers to the questions within the text:\n\nThe document introduces Deep Graph Infomax (DGI), a novel approach for learning node representations in graph-structured data without labeled supervision. The important entities in this document are the node representations, patch representations, and graph convolutional network architectures. The important dates are not mentioned, as this is a theoretical paper. The events happening in this document are the development and demonstration of the DGI approach, which relies on maximizing mutual information between patch representations and high-level summaries of graphs. The result of these events is the creation of a new approach that can learn node embeddings that are mindful of the global structural properties of the graph, achieving competitive performance across various classification tasks, including node classification benchmarks and transductive and inductive learning setups.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nWe present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning. We have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised representations on graph-structured data. By leveraging local mutual information maximization across the graph's patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph. This enables competitive performance across a variety of both transductive and inductive classification tasks, at times even outperforming relevant supervised architectures.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe Deep Graph Infomax (DGI) approach is a new way to learn node representations in graph-structured data without any labels. By using a technique called mutual information maximization, DGI creates patch representations of subgraphs centered around important nodes, which can then be used for various tasks such as node classification.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nWe present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning. We have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised representations on graph-structured data. By leveraging local mutual information maximization across the graph's patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph. This enables competitive performance across a variety of both transductive and inductive classification tasks, at times even outperforming relevant supervised architectures.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document presents a new approach called Deep Graph Infomax (DGI) that helps computers learn representations of nodes in graph-structured data without needing labeled data. By analyzing the relationships between local patches of nodes and the overall structure of the graph, DGI creates node embeddings that are effective for various classification tasks, often performing as well as or even better than supervised learning methods.\n\nNote: The important entities in this document are nodes, graphs, and patch representations. The important dates are not mentioned as this is a theoretical paper and does not have specific dates. The events happening in this document are the development and testing of the DGI approach. The result of these events is the creation of a new method for unsupervised learning on graph-structured data that achieves competitive performance on various classification tasks.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 2 sentences.\n\nScholarly document\nWe present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning. We have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised representations on graph-structured data. By leveraging local mutual information maximization across the graph's patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph. This enables competitive performance across a variety of both transductive and inductive classification tasks, at times even outperforming relevant supervised architectures.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document that answers the questions in a way that is coherent and easy to understand for a high school student:\n\nThe researchers introduced a new approach called Deep Graph Infomax (DGI) to learn node representations in graph-structured data without any supervision. The approach focuses on maximizing the mutual information between local patch representations and high-level summaries of the graph, which are both derived from established graph convolutional network architectures."
        }
    },
    {
        "id": 1107,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 13 sentences.\n\nScholarly document\nConvolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works. The rich diversity of object appearance in images arises from variations in object semantics and deformation. Semantics describe the high-level abstraction of what we perceive, and deformation defines the geometric transformation tied to specific data (Gibson, 1950) . Humans are remarkably adept at making abstractions of the world (Hudson & Manning, 2019) ; we see in raw visual signals, abstract semantics away from deformation, and form concepts. Interestingly, modern convolutional networks follow an analogous process by making abstractions through local connectivity and weight sharing (Zhang, 2019) . However, such a mechanism is an inefficient one, as the emergent representations encode semantics and deformation together, instead of as disjoint notions. Though a convolution responds accordingly to each input, how it responds is primarily programmed by its rigid kernels, as in Figure 1(a, b) . In effect, this consumes large model capacity and data modes . We argue that the awareness of deformations emerges from adaptivity -the ability to adapt at runtime (Kanazawa et al., 2016; Jia et al., 2016; Li et al., 2019) . Modeling of geometric transformations has been a constant pursuit for vision researchers over decades (Lowe et al., 1999; Lazebnik et al., 2006; Jaderberg et al., 2015; Dai et al., 2017) . A basic idea is to spatially recompose data towards a common mode such that semantic recognition suffers less from deformation. A recent work that is representative of this direction is Deformable Convolution (Dai et al., 2017; Zhu et al., 2019) . As shown in Figure 1 (c), it augments the convolutions with free-form sampling grids in the data space. It is previously justified as adapting receptive field, or what we phrase as the \"theoretical receptive field\", that defines which input pixels can contribute to the final output. However, theoretical receptive field does not measure how much impact an input pixel actually has. On the other hand, (Dai et al., 2017) reconfigure data towards common arrangement to counter the effects of geometric deformation. (d) Our Deformable Kernels (DKs) instead resample kernels and, in effect, adapt kernel spaces while leaving the data untouched. Note that (b) and (c) share kernel values but sample different data locations, while (b) and (d) share data locations but sample different kernel values. Luo et al. (2016) propose to measure the effective receptive field (ERF), i.e. the partial derivative of the output with respect to the input data, to quantify the exact contribution of each raw pixel to the convolution. Since adapting the theoretical receptive field is not the goal but a means to adapt the ERF, why not directly tune the ERF to specific data and tasks at runtime? Toward this end, we introduce Deformable Kernels (DKs), a family of novel and generic convolutional operators for deformation modeling. We aim to augment rigid kernels with the expressiveness to directly interact with the ERF of the computation during inference. Illustrated in Figure 1 (d), DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt ERF while leaving receptive field untouched. The design of DKs that is agnostic to data coordinates naturally leads to two variants -the global DK and the local DK, which behave differently in practice as we later investigate. We justify our approach with theoretical results which show that ERF is strictly determined by data sampling locations and kernel values. Used as a generic drop-in replacement of rigid kernels, DKs achieve empirical results coherent with our developed theory. Concretely, we evaluate our operator with standard base models on image classification and object detection. DKs perform favorably against prior works that adapt during runtime. With both quantitative and qualitative analysis, we further show that DKs can work orthogonally and complementarily with previous techniques. In this paper, we introduced Deformable Kernels (DKs) to adapt effective receptive fields (ERFs) of convolutional networks for object deformation. We proposed to sample kernel values from the original kernel space. This in effect samples the ERF in linear networks and also roughly generalizes to non-linear cases. We instantiated two variants of DKs and validate our designs, showing connections to previous works. Consistent improvements over them and compatibility with them were found, as illustrated in visualizations. image patch kernel patch Figure 5 : Illustration of feed-forwarding through a 3\u00d73 local Deformable Kernel from a 4\u00d74 scope. For each input patch, local DK first generates a group of kernel offsets {\u2206k} from input feature patch using the light-weight generator G (a 3\u00d73 convolution of rigid kernel). Given the original kernel weights W and the offset group {\u2206k}, DK samples a new set of kernel W using a bilinear sampler B. Finally, DK convolves the input feature map and the sampled kernels to complete the whole computation.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses a problem with convolutional neural networks, which are not good at recognizing objects that are deformed or changed shape. This is because the networks are not aware of these changes and can't adapt to them. To solve this problem, researchers have been trying to find ways to make the networks more aware of deformations. One way to do this is by changing the way the network looks at the data, so that it can recognize the same object even if it's deformed.\n\nThe document introduces a new approach called Deformable Kernels (DKs), which is a way to make the network more aware of deformations by adapting the way it looks at the data. DKs are a type of convolutional operator that can be used to handle object deformations by directly adapting the effective receptive field (ERF) of the network. The ERF is the area of the input data that the network is actually looking at, and adapting it allows the network to focus on the parts of the object that are most important for recognition.\n\nThe authors of the document tested DKs on several tasks and compared them to other approaches that also try to handle object deformations. They found that DKs performed better than the other approaches in many cases, and that they can be used to improve the performance of convolutional neural networks.\n\nThe document also discusses the theoretical background of DKs, including the idea that the ERF is strictly determined by the data sampling locations and kernel values. This means that the ERF is not just a fixed property of the network, but can be changed depending on the data and the task.\n\nOverall, the document presents a new approach to handling object deformations in convolutional neural networks, and shows that it can be effective in improving the performance of these networks.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 13 sentences.\n\nScholarly document\nConvolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works. The rich diversity of object appearance in images arises from variations in object semantics and deformation. Semantics describe the high-level abstraction of what we perceive, and deformation defines the geometric transformation tied to specific data (Gibson, 1950) . Humans are remarkably adept at making abstractions of the world (Hudson & Manning, 2019) ; we see in raw visual signals, abstract semantics away from deformation, and form concepts. Interestingly, modern convolutional networks follow an analogous process by making abstractions through local connectivity and weight sharing (Zhang, 2019) . However, such a mechanism is an inefficient one, as the emergent representations encode semantics and deformation together, instead of as disjoint notions. Though a convolution responds accordingly to each input, how it responds is primarily programmed by its rigid kernels, as in Figure 1(a, b) . In effect, this consumes large model capacity and data modes . We argue that the awareness of deformations emerges from adaptivity -the ability to adapt at runtime (Kanazawa et al., 2016; Jia et al., 2016; Li et al., 2019) . Modeling of geometric transformations has been a constant pursuit for vision researchers over decades (Lowe et al., 1999; Lazebnik et al., 2006; Jaderberg et al., 2015; Dai et al., 2017) . A basic idea is to spatially recompose data towards a common mode such that semantic recognition suffers less from deformation. A recent work that is representative of this direction is Deformable Convolution (Dai et al., 2017; Zhu et al., 2019) . As shown in Figure 1 (c), it augments the convolutions with free-form sampling grids in the data space. It is previously justified as adapting receptive field, or what we phrase as the \"theoretical receptive field\", that defines which input pixels can contribute to the final output. However, theoretical receptive field does not measure how much impact an input pixel actually has. On the other hand, (Dai et al., 2017) reconfigure data towards common arrangement to counter the effects of geometric deformation. (d) Our Deformable Kernels (DKs) instead resample kernels and, in effect, adapt kernel spaces while leaving the data untouched. Note that (b) and (c) share kernel values but sample different data locations, while (b) and (d) share data locations but sample different kernel values. Luo et al. (2016) propose to measure the effective receptive field (ERF), i.e. the partial derivative of the output with respect to the input data, to quantify the exact contribution of each raw pixel to the convolution. Since adapting the theoretical receptive field is not the goal but a means to adapt the ERF, why not directly tune the ERF to specific data and tasks at runtime? Toward this end, we introduce Deformable Kernels (DKs), a family of novel and generic convolutional operators for deformation modeling. We aim to augment rigid kernels with the expressiveness to directly interact with the ERF of the computation during inference. Illustrated in Figure 1 (d), DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt ERF while leaving receptive field untouched. The design of DKs that is agnostic to data coordinates naturally leads to two variants -the global DK and the local DK, which behave differently in practice as we later investigate. We justify our approach with theoretical results which show that ERF is strictly determined by data sampling locations and kernel values. Used as a generic drop-in replacement of rigid kernels, DKs achieve empirical results coherent with our developed theory. Concretely, we evaluate our operator with standard base models on image classification and object detection. DKs perform favorably against prior works that adapt during runtime. With both quantitative and qualitative analysis, we further show that DKs can work orthogonally and complementarily with previous techniques. In this paper, we introduced Deformable Kernels (DKs) to adapt effective receptive fields (ERFs) of convolutional networks for object deformation. We proposed to sample kernel values from the original kernel space. This in effect samples the ERF in linear networks and also roughly generalizes to non-linear cases. We instantiated two variants of DKs and validate our designs, showing connections to previous works. Consistent improvements over them and compatibility with them were found, as illustrated in visualizations. image patch kernel patch Figure 5 : Illustration of feed-forwarding through a 3\u00d73 local Deformable Kernel from a 4\u00d74 scope. For each input patch, local DK first generates a group of kernel offsets {\u2206k} from input feature patch using the light-weight generator G (a 3\u00d73 convolution of rigid kernel). Given the original kernel weights W and the offset group {\u2206k}, DK samples a new set of kernel W using a bilinear sampler B. Finally, DK convolves the input feature map and the sampled kernels to complete the whole computation.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses how convolutional neural networks (CNNs) struggle to recognize objects that have different shapes or deformations. This is because the networks are not aware of these geometric variations and use a fixed kernel to process the data. To overcome this issue, researchers have developed methods that adapt the kernel to the data and task at hand. However, these methods do not directly address the issue of how much each pixel contributes to the output, which is important for object recognition.\n\nThe authors of this paper propose a new approach called Deformable Kernels (DKs), which directly adapts the effective receptive field (ERF) of the network to the data and task. The ERF is the area of the input image that the network is sensitive to, and adapting it allows the network to better recognize objects that have different shapes or deformations.\n\nThe authors implement DKs as a drop-in replacement for traditional convolutional layers and evaluate their performance on image classification and object detection tasks. They find that DKs perform favorably against other methods that adapt during runtime and can work orthogonally and complementarily with previous techniques.\n\nThe important entities in this document are convolutional neural networks, deformations, effective receptive field, and Deformable Kernels.\n\nThe important dates mentioned in the document are not specific dates, but rather references to previous works and studies in the field of computer vision and machine learning.\n\nThe events happening in this document are the development of Deformable Kernels, the evaluation of their performance on various tasks, and the comparison with other methods.\n\nThe result of these events is the development of a new approach to adapting convolutional neural networks to object deformations, which can improve their performance on tasks such as image classification and object detection.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 13 sentences.\n\nScholarly document\nConvolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works. The rich diversity of object appearance in images arises from variations in object semantics and deformation. Semantics describe the high-level abstraction of what we perceive, and deformation defines the geometric transformation tied to specific data (Gibson, 1950) . Humans are remarkably adept at making abstractions of the world (Hudson & Manning, 2019) ; we see in raw visual signals, abstract semantics away from deformation, and form concepts. Interestingly, modern convolutional networks follow an analogous process by making abstractions through local connectivity and weight sharing (Zhang, 2019) . However, such a mechanism is an inefficient one, as the emergent representations encode semantics and deformation together, instead of as disjoint notions. Though a convolution responds accordingly to each input, how it responds is primarily programmed by its rigid kernels, as in Figure 1(a, b) . In effect, this consumes large model capacity and data modes . We argue that the awareness of deformations emerges from adaptivity -the ability to adapt at runtime (Kanazawa et al., 2016; Jia et al., 2016; Li et al., 2019) . Modeling of geometric transformations has been a constant pursuit for vision researchers over decades (Lowe et al., 1999; Lazebnik et al., 2006; Jaderberg et al., 2015; Dai et al., 2017) . A basic idea is to spatially recompose data towards a common mode such that semantic recognition suffers less from deformation. A recent work that is representative of this direction is Deformable Convolution (Dai et al., 2017; Zhu et al., 2019) . As shown in Figure 1 (c), it augments the convolutions with free-form sampling grids in the data space. It is previously justified as adapting receptive field, or what we phrase as the \"theoretical receptive field\", that defines which input pixels can contribute to the final output. However, theoretical receptive field does not measure how much impact an input pixel actually has. On the other hand, (Dai et al., 2017) reconfigure data towards common arrangement to counter the effects of geometric deformation. (d) Our Deformable Kernels (DKs) instead resample kernels and, in effect, adapt kernel spaces while leaving the data untouched. Note that (b) and (c) share kernel values but sample different data locations, while (b) and (d) share data locations but sample different kernel values. Luo et al. (2016) propose to measure the effective receptive field (ERF), i.e. the partial derivative of the output with respect to the input data, to quantify the exact contribution of each raw pixel to the convolution. Since adapting the theoretical receptive field is not the goal but a means to adapt the ERF, why not directly tune the ERF to specific data and tasks at runtime? Toward this end, we introduce Deformable Kernels (DKs), a family of novel and generic convolutional operators for deformation modeling. We aim to augment rigid kernels with the expressiveness to directly interact with the ERF of the computation during inference. Illustrated in Figure 1 (d), DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt ERF while leaving receptive field untouched. The design of DKs that is agnostic to data coordinates naturally leads to two variants -the global DK and the local DK, which behave differently in practice as we later investigate. We justify our approach with theoretical results which show that ERF is strictly determined by data sampling locations and kernel values. Used as a generic drop-in replacement of rigid kernels, DKs achieve empirical results coherent with our developed theory. Concretely, we evaluate our operator with standard base models on image classification and object detection. DKs perform favorably against prior works that adapt during runtime. With both quantitative and qualitative analysis, we further show that DKs can work orthogonally and complementarily with previous techniques. In this paper, we introduced Deformable Kernels (DKs) to adapt effective receptive fields (ERFs) of convolutional networks for object deformation. We proposed to sample kernel values from the original kernel space. This in effect samples the ERF in linear networks and also roughly generalizes to non-linear cases. We instantiated two variants of DKs and validate our designs, showing connections to previous works. Consistent improvements over them and compatibility with them were found, as illustrated in visualizations. image patch kernel patch Figure 5 : Illustration of feed-forwarding through a 3\u00d73 local Deformable Kernel from a 4\u00d74 scope. For each input patch, local DK first generates a group of kernel offsets {\u2206k} from input feature patch using the light-weight generator G (a 3\u00d73 convolution of rigid kernel). Given the original kernel weights W and the offset group {\u2206k}, DK samples a new set of kernel W using a bilinear sampler B. Finally, DK convolves the input feature map and the sampled kernels to complete the whole computation.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document, tailored for a high school student:\n\nThe document discusses a problem in computer vision, where convolutional neural networks (CNNs) struggle to recognize objects with geometric variations. This is because the networks are not aware of these variations and therefore waste model and data capacity. To overcome this issue, researchers have been working on deformation modeling, which involves reconfiguring the data to make it easier for the network to recognize. However, this approach is not effective because it doesn't directly adapt the \"effective receptive field\" (ERF), which is the actual contribution of each pixel to the output.\n\nThe authors of this document propose a new approach called Deformable Kernels (DKs), which directly adapts the ERF by resampling the kernel space. This is done by learning free-form offsets on kernel coordinates to deform the original kernel space towards specific data modalities. The authors justify their approach with theoretical results, which show that the ERF is strictly determined by data sampling locations and kernel values.\n\nThe authors evaluate their approach using standard base models on image classification and object detection tasks, and find that it performs favorably against prior works that adapt during runtime. They also show that their approach can work orthogonally and complementarily with previous techniques.\n\nThe important entities in this document are convolutional neural networks, deformation modeling, and Deformable Kernels (DKs). The important dates mentioned are not specific dates, but rather references to previous works and theories.\n\nThe events happening in this document are the development of deformation modeling, the limitations of previous approaches, and the proposal of Deformable Kernels (DKs) as a solution to the problem.\n\nThe result of these events is the development of a new approach to deformation modeling that directly adapts the effective receptive field (ERF) of convolutional neural networks, which can improve their performance on tasks such as image classification and object detection.",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 13 sentences.\n\nScholarly document\nConvolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works. The rich diversity of object appearance in images arises from variations in object semantics and deformation. Semantics describe the high-level abstraction of what we perceive, and deformation defines the geometric transformation tied to specific data (Gibson, 1950) . Humans are remarkably adept at making abstractions of the world (Hudson & Manning, 2019) ; we see in raw visual signals, abstract semantics away from deformation, and form concepts. Interestingly, modern convolutional networks follow an analogous process by making abstractions through local connectivity and weight sharing (Zhang, 2019) . However, such a mechanism is an inefficient one, as the emergent representations encode semantics and deformation together, instead of as disjoint notions. Though a convolution responds accordingly to each input, how it responds is primarily programmed by its rigid kernels, as in Figure 1(a, b) . In effect, this consumes large model capacity and data modes . We argue that the awareness of deformations emerges from adaptivity -the ability to adapt at runtime (Kanazawa et al., 2016; Jia et al., 2016; Li et al., 2019) . Modeling of geometric transformations has been a constant pursuit for vision researchers over decades (Lowe et al., 1999; Lazebnik et al., 2006; Jaderberg et al., 2015; Dai et al., 2017) . A basic idea is to spatially recompose data towards a common mode such that semantic recognition suffers less from deformation. A recent work that is representative of this direction is Deformable Convolution (Dai et al., 2017; Zhu et al., 2019) . As shown in Figure 1 (c), it augments the convolutions with free-form sampling grids in the data space. It is previously justified as adapting receptive field, or what we phrase as the \"theoretical receptive field\", that defines which input pixels can contribute to the final output. However, theoretical receptive field does not measure how much impact an input pixel actually has. On the other hand, (Dai et al., 2017) reconfigure data towards common arrangement to counter the effects of geometric deformation. (d) Our Deformable Kernels (DKs) instead resample kernels and, in effect, adapt kernel spaces while leaving the data untouched. Note that (b) and (c) share kernel values but sample different data locations, while (b) and (d) share data locations but sample different kernel values. Luo et al. (2016) propose to measure the effective receptive field (ERF), i.e. the partial derivative of the output with respect to the input data, to quantify the exact contribution of each raw pixel to the convolution. Since adapting the theoretical receptive field is not the goal but a means to adapt the ERF, why not directly tune the ERF to specific data and tasks at runtime? Toward this end, we introduce Deformable Kernels (DKs), a family of novel and generic convolutional operators for deformation modeling. We aim to augment rigid kernels with the expressiveness to directly interact with the ERF of the computation during inference. Illustrated in Figure 1 (d), DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt ERF while leaving receptive field untouched. The design of DKs that is agnostic to data coordinates naturally leads to two variants -the global DK and the local DK, which behave differently in practice as we later investigate. We justify our approach with theoretical results which show that ERF is strictly determined by data sampling locations and kernel values. Used as a generic drop-in replacement of rigid kernels, DKs achieve empirical results coherent with our developed theory. Concretely, we evaluate our operator with standard base models on image classification and object detection. DKs perform favorably against prior works that adapt during runtime. With both quantitative and qualitative analysis, we further show that DKs can work orthogonally and complementarily with previous techniques. In this paper, we introduced Deformable Kernels (DKs) to adapt effective receptive fields (ERFs) of convolutional networks for object deformation. We proposed to sample kernel values from the original kernel space. This in effect samples the ERF in linear networks and also roughly generalizes to non-linear cases. We instantiated two variants of DKs and validate our designs, showing connections to previous works. Consistent improvements over them and compatibility with them were found, as illustrated in visualizations. image patch kernel patch Figure 5 : Illustration of feed-forwarding through a 3\u00d73 local Deformable Kernel from a 4\u00d74 scope. For each input patch, local DK first generates a group of kernel offsets {\u2206k} from input feature patch using the light-weight generator G (a 3\u00d73 convolution of rigid kernel). Given the original kernel weights W and the offset group {\u2206k}, DK samples a new set of kernel W using a bilinear sampler B. Finally, DK convolves the input feature map and the sampled kernels to complete the whole computation.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the limitations of convolutional networks in recognizing objects with geometric variations. These networks are not aware of these variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, researchers have developed methods to adapt the data towards a common arrangement, but these methods do not directly address the problem. The document proposes a new approach called Deformable Kernels (DKs), which directly adapts the effective receptive field (ERF) of the convolutional network during runtime.\n\nThe ERF is the amount of contribution each pixel makes to the final output, and it is determined by the data sampling locations and kernel values. The DKs approach samples the kernel values from the original kernel space, which effectively samples the ERF in linear networks and roughly generalizes to non-linear cases. The document also proposes two variants of DKs, the global DK and the local DK, which behave differently in practice.\n\nThe document presents several experiments to validate the DKs approach, including image classification and object detection tasks. The results show that DKs perform favorably against prior works that adapt during runtime and can work orthogonally and complementarily with previous techniques.\n\nThe important entities in this document are convolutional networks, geometric variations, data, kernel values, and the effective receptive field (ERF).\n\nThe important dates mentioned in the document are not explicitly stated, but the references provided give an idea of the timeline of research in this area.\n\nThe events happening in this document are the development of convolutional networks, the recognition of the limitations of these networks, and the proposal of a new approach called Deformable Kernels (DKs) to adapt the effective receptive field (ERF) of the convolutional network during runtime.\n\nThe result of these events is the development of a new approach to improve the performance of convolutional networks in recognizing objects with geometric variations."
        }
    },
    {
        "id": 1125,
        "summaries": {
            "best_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 17 sentences.\n\nScholarly document\nAs an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, absorbing the idea of mining-based strategies is adopted to emphasize the misclassified samples and achieve promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issues. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Code will be available upon publication. The success of Convolutional Neural Networks (CNNs) on face recognition can be mainly credited to : enormous training data, network architectures, and loss functions. Recently, designing appropriate loss functions that enhance discriminative power is pivotal for training deep face CNNs. Current state-of-the-art face recognition methods mainly adopt softmax-based classification loss. Since the learned features with the original softmax is not discriminative enough for the open-set face recognition problem, several margin-based variants have been proposed to enhance features' discriminative power. For example, explicit margin, i.e., CosFace (Wang et al., 2018a) , Sphereface (Li et al., 2017) , ArcFace (Deng et al., 2019) , and implicit margin, i.e., Adacos (Zhang et al., 2019a) , supplement the original softmax function to enforce greater intra-class compactness and inter-class discrepancy, which are shown to result in more discriminate features. However, these margin-based loss functions do not explicitly emphasize each sample according to its importance. As demonstrated in Chen et al. (2019) , hard sample mining is also a critical step to further improve the final accuracy. Recently, Triplet loss (Schroff et al., 2015) and SV-Arc-Softmax (Wang et al., 2018b) integrate the motivations of both margin and mining into one framework for deep face recognition. Triplet loss adopts a semi-hard mining strategy to obtain semi-hard triplets and enlarge the margin between triplet samples. SV-Arc-Softmax (Wang et al., 2018b) clearly defines hard samples as misclassified samples and emphasizes them by increasing the weights of their negative cosine similarities with a preset constant. In a nutshell, mining-based loss functions explicitly emphasize the effects of semi-hard or hard samples. However, there are drawbacks in training strategies of both margin-and mining-based loss functions. For margin-based methods, mining strategy is ignored and thus the difficultness of each sample is not fully exploited, which may lead to convergence issues when using a large margin on small backbones, e.g., MobileFaceNet (Chen et al., 2018) . As shown in Fig. 1 , the modulation coefficient for the negative cosine similarities I(\u00b7) is fixed as a constant 1 in ArcFace for all samples during the entire training process. For mining-based methods, over-emphasizing hard samples in early training Figure 1: Different training strategies for modulating negative cosine similarities of hard samples (i.e., the mis-classified sample) in ArcFace, SV-Arc-Softmax and our CurricularFace. Left: The modulation coefficients I(t, cos \u03b8j) for negative cosine similarities of hard samples in different methods, where t is an adaptively estimated parameter and \u03b8j denotes the angle between the hard sample and the non-ground truth j-class center. Right: The corresponding hard samples' negative cosine similarities N (t, cos \u03b8j) = I(t, cos \u03b8j) cos \u03b8j + c after modulation, where c indicates a constant. On one hand, during early training stage (e.g., t is close to 0), hard sample's negative cosine similarities is usually reduced and thus leads to smaller hard sample loss than the original one. Therefore, easier samples are relatively emphasized; during later training stage (e.g., t is close to 1), the hard sample's negative cosine similarities are enhanced and thus leads to larger hard sample loss. On the other hand, in the same training stage, we modulate the hard samples' negative cosine similarities with cos \u03b8j. Specifically, the smaller the angle \u03b8j is, the larger the modulation coefficient should be. stage may hinder the model to converge. As SV-Arc-Softmax claimed, the manually defined constant t plays a key role in the model convergence property and a slight larger value (e.g., >1.4) may cause the model difficult to converge. Thus t needs to be carefully tuned. In this work, we propose a novel adaptive curriculum learning loss, termed CurricularFace, to achieve a novel training strategy for deep face recognition. Motivated by the nature of human learning that easy cases are learned first and then come the hard ones (Bengio et al., 2009) , our CurricularFace incorporates the idea of Curriculum Learning (CL) into face recognition in an adaptive manner, which differs from the traditional CL in two aspects. First, the curriculum construction is adaptive. In traditional CL, the samples are ordered by the corresponding difficultness, which are often defined by a prior and then fixed to establish the curriculum. In CurricularFace, the samples are randomly selected in each mini-batch, while the curriculum is established adaptively via mining the hard samples online, which shows the diversity in samples with different importance. Second, the importance of hard samples are adaptive. On one hand, the relative importance between easy and hard samples is dynamic and could be adjusted in different training stages. On the other hand, the importance of each hard sample in current mini-batch depends on its own difficultness. Specifically, the mis-classified samples in mini-batch are chosen as hard samples and weighted by adjusting the modulation coefficients I(t, cos\u03b8 j ) of cosine similarities between the sample and the non-ground truth class center vectors, i.e., negative cosine similarity N (t, cos\u03b8 j ). To achieve the goal of adaptive curricular learning in the entire training, we design a novel coefficient function I(\u00b7) that is determined by two factors: 1) the adaptively estimated parameter t that utilizes moving average of positive cosine similarities between samples and the corresponding ground-truth class center to unleash the burden of manually tuning; and 2) the angle \u03b8 j that defines the difficultness of hard samples to achieve adaptive assignment. To sum up, the contributions of this work are: \u2022 We propose an adaptive curriculum learning loss for face recognition, which automatically emphasizes easy samples first and hard samples later. To the best of our knowledge, it is the first work to introduce the idea of adaptive curriculum learning for face recognition. \u2022 We design a novel modulation coefficient function I(\u00b7) to achieve adaptive curriculum learning during training, which connects positive and negative cosine similarity simultaneously without the need of manually tuning any additional hyper-parameter. \u2022 We conduct extensive experiments on popular facial benchmarks, which demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Comparison with ArcFace and SV-Arc-Softmax We first discuss the difference between our CurricularFace and the two competitors, ArcFace and SV-Arc-Softmax, from the perspective of the decision boundary in Tab. 1. ArcFace introduces a margin function T (cos \u03b8 yi ) = cos(\u03b8 yi + m) from the perspective of positive cosine similarity. As shown in Fig. 4 , its decision condition changes from cos \u03b8 yi = cos \u03b8 j (i.e., blue line) to cos(\u03b8 yi + m) = cos \u03b8 j (i.e., red line) for each sample. SV-Arc-Softmax introduces additional margin from the perspective of negative cosine similarity for hard samples, and the decision boundary becomes cos(\u03b8 yi + m) = t cos \u03b8 j + t \u2212 1 (i.e., green line). Conversely, we adaptively adjust the weights of hard samples in different training stages. The decision condition becomes cos(\u03b8 yi +m) = (t+cos \u03b8 j ) cos \u03b8 j (i.e., purple line). During the training stage, the decision boundary for hard samples changes from one purple line (early stage) to another (later stage), which emphasizes easy samples first and hard samples later. Comparison with Focal loss Focal loss is a soft mining-based loss, which is formulated as: \u03b2 , where \u03b1 and \u03b2 are modulating factors that need to be tuned manually. The definition of hard samples in Focal loss is ambiguous, since it always focuses on relatively hard samples by reducing the weight of easier samples during the entire training process. In contrast, the definition of hard samples in our CurricularFace is more clear, i.e., mis-classified samples. Meanwhile, the weights of hard samples are adaptively determined in different training stages. In this paper, we propose a novel Adaptive Curriculum Learning Loss that embeds the idea of adaptive curriculum learning into deep face recognition. Our key idea is to address easy samples in the early training stage and hard ones in the later stage. Our method is easy to implement and robust to converge. Extensive experiments on popular facial benchmarks demonstrate the effectiveness of our method compared to the state-of-the-art competitors. Following the main idea of this work, future research can be expanded in various aspects, including designing a better function N (\u00b7) for negative cosine similarity that shares similar adaptive characteristic during training, and investigating the effects of noise samples that could be optimized as hard samples.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document that embeds the answers to the questions within the summary:\n\nThe article discusses the importance of designing margin-based loss functions for face recognition, which can enhance the feature margin between different classes for better discriminability. The authors propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition. The CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages, addressing easy samples in the early stage and hard ones in the later stage. This approach is motivated by the nature of human learning, where easy cases are learned first and then come the hard ones.\n\nThe authors compare their approach with existing methods, such as ArcFace and SV-Arc-Softmax, which do not explicitly emphasize the importance of each sample. They also discuss the drawbacks of these methods, including ignoring the difficultness of each sample and over-emphasizing hard samples in early training. The authors argue that their approach is more effective because it adapts to the difficulty of each sample and adjusts the importance of hard samples in different training stages.\n\nThe CurricularFace is designed to be easy to implement and robust to converge, and extensive experiments on popular facial benchmarks demonstrate its superiority over the state-of-the-art competitors. The authors also discuss the potential for future research, including designing better functions for negative cosine similarity and investigating the effects of noise samples.\n\nThe important entities in this document are the authors, the proposed method (CurricularFace), and the existing methods (ArcFace and SV-Arc-Softmax). The important dates are not mentioned in the document. The events happening in this document are the development of the proposed method, the comparison with existing methods, and the experimental results. The result of these events is the proposed method's superiority over the existing methods.\n\nNote that the summary is written in a way that is easy to understand for a high school student, while still conveying the main ideas and concepts of the original document.",
            "all_sum": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 17 sentences.\n\nScholarly document\nAs an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, absorbing the idea of mining-based strategies is adopted to emphasize the misclassified samples and achieve promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issues. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Code will be available upon publication. The success of Convolutional Neural Networks (CNNs) on face recognition can be mainly credited to : enormous training data, network architectures, and loss functions. Recently, designing appropriate loss functions that enhance discriminative power is pivotal for training deep face CNNs. Current state-of-the-art face recognition methods mainly adopt softmax-based classification loss. Since the learned features with the original softmax is not discriminative enough for the open-set face recognition problem, several margin-based variants have been proposed to enhance features' discriminative power. For example, explicit margin, i.e., CosFace (Wang et al., 2018a) , Sphereface (Li et al., 2017) , ArcFace (Deng et al., 2019) , and implicit margin, i.e., Adacos (Zhang et al., 2019a) , supplement the original softmax function to enforce greater intra-class compactness and inter-class discrepancy, which are shown to result in more discriminate features. However, these margin-based loss functions do not explicitly emphasize each sample according to its importance. As demonstrated in Chen et al. (2019) , hard sample mining is also a critical step to further improve the final accuracy. Recently, Triplet loss (Schroff et al., 2015) and SV-Arc-Softmax (Wang et al., 2018b) integrate the motivations of both margin and mining into one framework for deep face recognition. Triplet loss adopts a semi-hard mining strategy to obtain semi-hard triplets and enlarge the margin between triplet samples. SV-Arc-Softmax (Wang et al., 2018b) clearly defines hard samples as misclassified samples and emphasizes them by increasing the weights of their negative cosine similarities with a preset constant. In a nutshell, mining-based loss functions explicitly emphasize the effects of semi-hard or hard samples. However, there are drawbacks in training strategies of both margin-and mining-based loss functions. For margin-based methods, mining strategy is ignored and thus the difficultness of each sample is not fully exploited, which may lead to convergence issues when using a large margin on small backbones, e.g., MobileFaceNet (Chen et al., 2018) . As shown in Fig. 1 , the modulation coefficient for the negative cosine similarities I(\u00b7) is fixed as a constant 1 in ArcFace for all samples during the entire training process. For mining-based methods, over-emphasizing hard samples in early training Figure 1: Different training strategies for modulating negative cosine similarities of hard samples (i.e., the mis-classified sample) in ArcFace, SV-Arc-Softmax and our CurricularFace. Left: The modulation coefficients I(t, cos \u03b8j) for negative cosine similarities of hard samples in different methods, where t is an adaptively estimated parameter and \u03b8j denotes the angle between the hard sample and the non-ground truth j-class center. Right: The corresponding hard samples' negative cosine similarities N (t, cos \u03b8j) = I(t, cos \u03b8j) cos \u03b8j + c after modulation, where c indicates a constant. On one hand, during early training stage (e.g., t is close to 0), hard sample's negative cosine similarities is usually reduced and thus leads to smaller hard sample loss than the original one. Therefore, easier samples are relatively emphasized; during later training stage (e.g., t is close to 1), the hard sample's negative cosine similarities are enhanced and thus leads to larger hard sample loss. On the other hand, in the same training stage, we modulate the hard samples' negative cosine similarities with cos \u03b8j. Specifically, the smaller the angle \u03b8j is, the larger the modulation coefficient should be. stage may hinder the model to converge. As SV-Arc-Softmax claimed, the manually defined constant t plays a key role in the model convergence property and a slight larger value (e.g., >1.4) may cause the model difficult to converge. Thus t needs to be carefully tuned. In this work, we propose a novel adaptive curriculum learning loss, termed CurricularFace, to achieve a novel training strategy for deep face recognition. Motivated by the nature of human learning that easy cases are learned first and then come the hard ones (Bengio et al., 2009) , our CurricularFace incorporates the idea of Curriculum Learning (CL) into face recognition in an adaptive manner, which differs from the traditional CL in two aspects. First, the curriculum construction is adaptive. In traditional CL, the samples are ordered by the corresponding difficultness, which are often defined by a prior and then fixed to establish the curriculum. In CurricularFace, the samples are randomly selected in each mini-batch, while the curriculum is established adaptively via mining the hard samples online, which shows the diversity in samples with different importance. Second, the importance of hard samples are adaptive. On one hand, the relative importance between easy and hard samples is dynamic and could be adjusted in different training stages. On the other hand, the importance of each hard sample in current mini-batch depends on its own difficultness. Specifically, the mis-classified samples in mini-batch are chosen as hard samples and weighted by adjusting the modulation coefficients I(t, cos\u03b8 j ) of cosine similarities between the sample and the non-ground truth class center vectors, i.e., negative cosine similarity N (t, cos\u03b8 j ). To achieve the goal of adaptive curricular learning in the entire training, we design a novel coefficient function I(\u00b7) that is determined by two factors: 1) the adaptively estimated parameter t that utilizes moving average of positive cosine similarities between samples and the corresponding ground-truth class center to unleash the burden of manually tuning; and 2) the angle \u03b8 j that defines the difficultness of hard samples to achieve adaptive assignment. To sum up, the contributions of this work are: \u2022 We propose an adaptive curriculum learning loss for face recognition, which automatically emphasizes easy samples first and hard samples later. To the best of our knowledge, it is the first work to introduce the idea of adaptive curriculum learning for face recognition. \u2022 We design a novel modulation coefficient function I(\u00b7) to achieve adaptive curriculum learning during training, which connects positive and negative cosine similarity simultaneously without the need of manually tuning any additional hyper-parameter. \u2022 We conduct extensive experiments on popular facial benchmarks, which demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Comparison with ArcFace and SV-Arc-Softmax We first discuss the difference between our CurricularFace and the two competitors, ArcFace and SV-Arc-Softmax, from the perspective of the decision boundary in Tab. 1. ArcFace introduces a margin function T (cos \u03b8 yi ) = cos(\u03b8 yi + m) from the perspective of positive cosine similarity. As shown in Fig. 4 , its decision condition changes from cos \u03b8 yi = cos \u03b8 j (i.e., blue line) to cos(\u03b8 yi + m) = cos \u03b8 j (i.e., red line) for each sample. SV-Arc-Softmax introduces additional margin from the perspective of negative cosine similarity for hard samples, and the decision boundary becomes cos(\u03b8 yi + m) = t cos \u03b8 j + t \u2212 1 (i.e., green line). Conversely, we adaptively adjust the weights of hard samples in different training stages. The decision condition becomes cos(\u03b8 yi +m) = (t+cos \u03b8 j ) cos \u03b8 j (i.e., purple line). During the training stage, the decision boundary for hard samples changes from one purple line (early stage) to another (later stage), which emphasizes easy samples first and hard samples later. Comparison with Focal loss Focal loss is a soft mining-based loss, which is formulated as: \u03b2 , where \u03b1 and \u03b2 are modulating factors that need to be tuned manually. The definition of hard samples in Focal loss is ambiguous, since it always focuses on relatively hard samples by reducing the weight of easier samples during the entire training process. In contrast, the definition of hard samples in our CurricularFace is more clear, i.e., mis-classified samples. Meanwhile, the weights of hard samples are adaptively determined in different training stages. In this paper, we propose a novel Adaptive Curriculum Learning Loss that embeds the idea of adaptive curriculum learning into deep face recognition. Our key idea is to address easy samples in the early training stage and hard ones in the later stage. Our method is easy to implement and robust to converge. Extensive experiments on popular facial benchmarks demonstrate the effectiveness of our method compared to the state-of-the-art competitors. Following the main idea of this work, future research can be expanded in various aspects, including designing a better function N (\u00b7) for negative cosine similarity that shares similar adaptive characteristic during training, and investigating the effects of noise samples that could be optimized as hard samples.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nIn the field of face recognition, researchers have been trying to design better loss functions to improve the accuracy of facial recognition systems. One important idea is to use \"margin-based\" loss functions, which try to increase the difference between faces of different people. However, previous methods have some limitations. For example, they don't always emphasize the most important samples, which can lead to poor performance.\n\nTo address this issue, the authors of this paper propose a new loss function called CurricularFace. This loss function is inspired by the way humans learn, where we start with easy tasks and gradually move on to harder ones. In CurricularFace, the algorithm starts by focusing on easy samples in the early stages of training and then gradually shifts its attention to harder samples as training progresses.\n\nThe authors also introduce a new way to adjust the importance of hard samples, which are samples that are misclassified by the model. They use a special function to adjust the weight of these hard samples, so that the model pays more attention to them as training progresses.\n\nThe authors tested their CurricularFace loss function on several popular face recognition benchmarks and found that it outperformed the state-of-the-art competitors. They also compared their method with other loss functions, such as ArcFace and SV-Arc-Softmax, and found that CurricularFace is more effective.\n\nOverall, the authors' new loss function, CurricularFace, is a significant improvement over previous methods and has the potential to be used in real-world applications such as facial recognition systems.",
            "all_summ_ev": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 17 sentences.\n\nScholarly document\nAs an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, absorbing the idea of mining-based strategies is adopted to emphasize the misclassified samples and achieve promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issues. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Code will be available upon publication. The success of Convolutional Neural Networks (CNNs) on face recognition can be mainly credited to : enormous training data, network architectures, and loss functions. Recently, designing appropriate loss functions that enhance discriminative power is pivotal for training deep face CNNs. Current state-of-the-art face recognition methods mainly adopt softmax-based classification loss. Since the learned features with the original softmax is not discriminative enough for the open-set face recognition problem, several margin-based variants have been proposed to enhance features' discriminative power. For example, explicit margin, i.e., CosFace (Wang et al., 2018a) , Sphereface (Li et al., 2017) , ArcFace (Deng et al., 2019) , and implicit margin, i.e., Adacos (Zhang et al., 2019a) , supplement the original softmax function to enforce greater intra-class compactness and inter-class discrepancy, which are shown to result in more discriminate features. However, these margin-based loss functions do not explicitly emphasize each sample according to its importance. As demonstrated in Chen et al. (2019) , hard sample mining is also a critical step to further improve the final accuracy. Recently, Triplet loss (Schroff et al., 2015) and SV-Arc-Softmax (Wang et al., 2018b) integrate the motivations of both margin and mining into one framework for deep face recognition. Triplet loss adopts a semi-hard mining strategy to obtain semi-hard triplets and enlarge the margin between triplet samples. SV-Arc-Softmax (Wang et al., 2018b) clearly defines hard samples as misclassified samples and emphasizes them by increasing the weights of their negative cosine similarities with a preset constant. In a nutshell, mining-based loss functions explicitly emphasize the effects of semi-hard or hard samples. However, there are drawbacks in training strategies of both margin-and mining-based loss functions. For margin-based methods, mining strategy is ignored and thus the difficultness of each sample is not fully exploited, which may lead to convergence issues when using a large margin on small backbones, e.g., MobileFaceNet (Chen et al., 2018) . As shown in Fig. 1 , the modulation coefficient for the negative cosine similarities I(\u00b7) is fixed as a constant 1 in ArcFace for all samples during the entire training process. For mining-based methods, over-emphasizing hard samples in early training Figure 1: Different training strategies for modulating negative cosine similarities of hard samples (i.e., the mis-classified sample) in ArcFace, SV-Arc-Softmax and our CurricularFace. Left: The modulation coefficients I(t, cos \u03b8j) for negative cosine similarities of hard samples in different methods, where t is an adaptively estimated parameter and \u03b8j denotes the angle between the hard sample and the non-ground truth j-class center. Right: The corresponding hard samples' negative cosine similarities N (t, cos \u03b8j) = I(t, cos \u03b8j) cos \u03b8j + c after modulation, where c indicates a constant. On one hand, during early training stage (e.g., t is close to 0), hard sample's negative cosine similarities is usually reduced and thus leads to smaller hard sample loss than the original one. Therefore, easier samples are relatively emphasized; during later training stage (e.g., t is close to 1), the hard sample's negative cosine similarities are enhanced and thus leads to larger hard sample loss. On the other hand, in the same training stage, we modulate the hard samples' negative cosine similarities with cos \u03b8j. Specifically, the smaller the angle \u03b8j is, the larger the modulation coefficient should be. stage may hinder the model to converge. As SV-Arc-Softmax claimed, the manually defined constant t plays a key role in the model convergence property and a slight larger value (e.g., >1.4) may cause the model difficult to converge. Thus t needs to be carefully tuned. In this work, we propose a novel adaptive curriculum learning loss, termed CurricularFace, to achieve a novel training strategy for deep face recognition. Motivated by the nature of human learning that easy cases are learned first and then come the hard ones (Bengio et al., 2009) , our CurricularFace incorporates the idea of Curriculum Learning (CL) into face recognition in an adaptive manner, which differs from the traditional CL in two aspects. First, the curriculum construction is adaptive. In traditional CL, the samples are ordered by the corresponding difficultness, which are often defined by a prior and then fixed to establish the curriculum. In CurricularFace, the samples are randomly selected in each mini-batch, while the curriculum is established adaptively via mining the hard samples online, which shows the diversity in samples with different importance. Second, the importance of hard samples are adaptive. On one hand, the relative importance between easy and hard samples is dynamic and could be adjusted in different training stages. On the other hand, the importance of each hard sample in current mini-batch depends on its own difficultness. Specifically, the mis-classified samples in mini-batch are chosen as hard samples and weighted by adjusting the modulation coefficients I(t, cos\u03b8 j ) of cosine similarities between the sample and the non-ground truth class center vectors, i.e., negative cosine similarity N (t, cos\u03b8 j ). To achieve the goal of adaptive curricular learning in the entire training, we design a novel coefficient function I(\u00b7) that is determined by two factors: 1) the adaptively estimated parameter t that utilizes moving average of positive cosine similarities between samples and the corresponding ground-truth class center to unleash the burden of manually tuning; and 2) the angle \u03b8 j that defines the difficultness of hard samples to achieve adaptive assignment. To sum up, the contributions of this work are: \u2022 We propose an adaptive curriculum learning loss for face recognition, which automatically emphasizes easy samples first and hard samples later. To the best of our knowledge, it is the first work to introduce the idea of adaptive curriculum learning for face recognition. \u2022 We design a novel modulation coefficient function I(\u00b7) to achieve adaptive curriculum learning during training, which connects positive and negative cosine similarity simultaneously without the need of manually tuning any additional hyper-parameter. \u2022 We conduct extensive experiments on popular facial benchmarks, which demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Comparison with ArcFace and SV-Arc-Softmax We first discuss the difference between our CurricularFace and the two competitors, ArcFace and SV-Arc-Softmax, from the perspective of the decision boundary in Tab. 1. ArcFace introduces a margin function T (cos \u03b8 yi ) = cos(\u03b8 yi + m) from the perspective of positive cosine similarity. As shown in Fig. 4 , its decision condition changes from cos \u03b8 yi = cos \u03b8 j (i.e., blue line) to cos(\u03b8 yi + m) = cos \u03b8 j (i.e., red line) for each sample. SV-Arc-Softmax introduces additional margin from the perspective of negative cosine similarity for hard samples, and the decision boundary becomes cos(\u03b8 yi + m) = t cos \u03b8 j + t \u2212 1 (i.e., green line). Conversely, we adaptively adjust the weights of hard samples in different training stages. The decision condition becomes cos(\u03b8 yi +m) = (t+cos \u03b8 j ) cos \u03b8 j (i.e., purple line). During the training stage, the decision boundary for hard samples changes from one purple line (early stage) to another (later stage), which emphasizes easy samples first and hard samples later. Comparison with Focal loss Focal loss is a soft mining-based loss, which is formulated as: \u03b2 , where \u03b1 and \u03b2 are modulating factors that need to be tuned manually. The definition of hard samples in Focal loss is ambiguous, since it always focuses on relatively hard samples by reducing the weight of easier samples during the entire training process. In contrast, the definition of hard samples in our CurricularFace is more clear, i.e., mis-classified samples. Meanwhile, the weights of hard samples are adaptively determined in different training stages. In this paper, we propose a novel Adaptive Curriculum Learning Loss that embeds the idea of adaptive curriculum learning into deep face recognition. Our key idea is to address easy samples in the early training stage and hard ones in the later stage. Our method is easy to implement and robust to converge. Extensive experiments on popular facial benchmarks demonstrate the effectiveness of our method compared to the state-of-the-art competitors. Following the main idea of this work, future research can be expanded in various aspects, including designing a better function N (\u00b7) for negative cosine similarity that shares similar adaptive characteristic during training, and investigating the effects of noise samples that could be optimized as hard samples.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the importance of designing effective loss functions for face recognition systems. The authors propose a new loss function called CurricularFace, which adapts the importance of easy and hard samples during different training stages. This approach is inspired by the way humans learn, where easy tasks are learned first and then harder tasks are tackled. The authors use a novel modulation coefficient function to adjust the weights of hard samples in each mini-batch, making it more effective than previous methods.\n\nThe authors compare their approach to two existing methods, ArcFace and SV-Arc-Softmax, and show that their method outperforms them on popular facial benchmarks. They also discuss the limitations of these existing methods, such as not explicitly emphasizing the importance of hard samples and not adapting to the difficulty of samples during training.\n\nThe authors also mention that their method is easy to implement and robust to converge, making it a practical solution for face recognition systems. The document concludes by suggesting future research directions, such as designing better functions for negative cosine similarity and investigating the effects of noise samples.\n\nThe important entities in this document are:\n\n* Face recognition systems\n* Loss functions\n* Easy and hard samples\n* Modulation coefficient function\n* ArcFace and SV-Arc-Softmax\n* CurricularFace\n\nThe important dates in this document are:\n\n* None mentioned\n\nThe events happening in this document are:\n\n* The authors propose a new loss function called CurricularFace\n* They compare their approach to existing methods\n* They show the effectiveness of their method on popular facial benchmarks\n* They discuss the limitations of existing methods\n* They suggest future research directions\n\nThe result of these events is:\n\n* The development of a new and effective loss function for face recognition systems\n* The demonstration of the superiority of CurricularFace over existing methods\n* The suggestion of future research directions to improve the method further",
            "zero-shot": "\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nPlease answer the following questions:\n1. What are the important entities in this document? \n2. What are the important dates in this document? \n3. What events are happening in this document? \n4. What is the result of these events? \n\nSummarize the given scholarly document for a high school student in a way that embeds answers to the above questions within the summary itself.\nEnsure the summary is coherent and does not explicitly list the questions and answers, with a maximum length of 17 sentences.\n\nScholarly document\nAs an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, absorbing the idea of mining-based strategies is adopted to emphasize the misclassified samples and achieve promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issues. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Code will be available upon publication. The success of Convolutional Neural Networks (CNNs) on face recognition can be mainly credited to : enormous training data, network architectures, and loss functions. Recently, designing appropriate loss functions that enhance discriminative power is pivotal for training deep face CNNs. Current state-of-the-art face recognition methods mainly adopt softmax-based classification loss. Since the learned features with the original softmax is not discriminative enough for the open-set face recognition problem, several margin-based variants have been proposed to enhance features' discriminative power. For example, explicit margin, i.e., CosFace (Wang et al., 2018a) , Sphereface (Li et al., 2017) , ArcFace (Deng et al., 2019) , and implicit margin, i.e., Adacos (Zhang et al., 2019a) , supplement the original softmax function to enforce greater intra-class compactness and inter-class discrepancy, which are shown to result in more discriminate features. However, these margin-based loss functions do not explicitly emphasize each sample according to its importance. As demonstrated in Chen et al. (2019) , hard sample mining is also a critical step to further improve the final accuracy. Recently, Triplet loss (Schroff et al., 2015) and SV-Arc-Softmax (Wang et al., 2018b) integrate the motivations of both margin and mining into one framework for deep face recognition. Triplet loss adopts a semi-hard mining strategy to obtain semi-hard triplets and enlarge the margin between triplet samples. SV-Arc-Softmax (Wang et al., 2018b) clearly defines hard samples as misclassified samples and emphasizes them by increasing the weights of their negative cosine similarities with a preset constant. In a nutshell, mining-based loss functions explicitly emphasize the effects of semi-hard or hard samples. However, there are drawbacks in training strategies of both margin-and mining-based loss functions. For margin-based methods, mining strategy is ignored and thus the difficultness of each sample is not fully exploited, which may lead to convergence issues when using a large margin on small backbones, e.g., MobileFaceNet (Chen et al., 2018) . As shown in Fig. 1 , the modulation coefficient for the negative cosine similarities I(\u00b7) is fixed as a constant 1 in ArcFace for all samples during the entire training process. For mining-based methods, over-emphasizing hard samples in early training Figure 1: Different training strategies for modulating negative cosine similarities of hard samples (i.e., the mis-classified sample) in ArcFace, SV-Arc-Softmax and our CurricularFace. Left: The modulation coefficients I(t, cos \u03b8j) for negative cosine similarities of hard samples in different methods, where t is an adaptively estimated parameter and \u03b8j denotes the angle between the hard sample and the non-ground truth j-class center. Right: The corresponding hard samples' negative cosine similarities N (t, cos \u03b8j) = I(t, cos \u03b8j) cos \u03b8j + c after modulation, where c indicates a constant. On one hand, during early training stage (e.g., t is close to 0), hard sample's negative cosine similarities is usually reduced and thus leads to smaller hard sample loss than the original one. Therefore, easier samples are relatively emphasized; during later training stage (e.g., t is close to 1), the hard sample's negative cosine similarities are enhanced and thus leads to larger hard sample loss. On the other hand, in the same training stage, we modulate the hard samples' negative cosine similarities with cos \u03b8j. Specifically, the smaller the angle \u03b8j is, the larger the modulation coefficient should be. stage may hinder the model to converge. As SV-Arc-Softmax claimed, the manually defined constant t plays a key role in the model convergence property and a slight larger value (e.g., >1.4) may cause the model difficult to converge. Thus t needs to be carefully tuned. In this work, we propose a novel adaptive curriculum learning loss, termed CurricularFace, to achieve a novel training strategy for deep face recognition. Motivated by the nature of human learning that easy cases are learned first and then come the hard ones (Bengio et al., 2009) , our CurricularFace incorporates the idea of Curriculum Learning (CL) into face recognition in an adaptive manner, which differs from the traditional CL in two aspects. First, the curriculum construction is adaptive. In traditional CL, the samples are ordered by the corresponding difficultness, which are often defined by a prior and then fixed to establish the curriculum. In CurricularFace, the samples are randomly selected in each mini-batch, while the curriculum is established adaptively via mining the hard samples online, which shows the diversity in samples with different importance. Second, the importance of hard samples are adaptive. On one hand, the relative importance between easy and hard samples is dynamic and could be adjusted in different training stages. On the other hand, the importance of each hard sample in current mini-batch depends on its own difficultness. Specifically, the mis-classified samples in mini-batch are chosen as hard samples and weighted by adjusting the modulation coefficients I(t, cos\u03b8 j ) of cosine similarities between the sample and the non-ground truth class center vectors, i.e., negative cosine similarity N (t, cos\u03b8 j ). To achieve the goal of adaptive curricular learning in the entire training, we design a novel coefficient function I(\u00b7) that is determined by two factors: 1) the adaptively estimated parameter t that utilizes moving average of positive cosine similarities between samples and the corresponding ground-truth class center to unleash the burden of manually tuning; and 2) the angle \u03b8 j that defines the difficultness of hard samples to achieve adaptive assignment. To sum up, the contributions of this work are: \u2022 We propose an adaptive curriculum learning loss for face recognition, which automatically emphasizes easy samples first and hard samples later. To the best of our knowledge, it is the first work to introduce the idea of adaptive curriculum learning for face recognition. \u2022 We design a novel modulation coefficient function I(\u00b7) to achieve adaptive curriculum learning during training, which connects positive and negative cosine similarity simultaneously without the need of manually tuning any additional hyper-parameter. \u2022 We conduct extensive experiments on popular facial benchmarks, which demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. Comparison with ArcFace and SV-Arc-Softmax We first discuss the difference between our CurricularFace and the two competitors, ArcFace and SV-Arc-Softmax, from the perspective of the decision boundary in Tab. 1. ArcFace introduces a margin function T (cos \u03b8 yi ) = cos(\u03b8 yi + m) from the perspective of positive cosine similarity. As shown in Fig. 4 , its decision condition changes from cos \u03b8 yi = cos \u03b8 j (i.e., blue line) to cos(\u03b8 yi + m) = cos \u03b8 j (i.e., red line) for each sample. SV-Arc-Softmax introduces additional margin from the perspective of negative cosine similarity for hard samples, and the decision boundary becomes cos(\u03b8 yi + m) = t cos \u03b8 j + t \u2212 1 (i.e., green line). Conversely, we adaptively adjust the weights of hard samples in different training stages. The decision condition becomes cos(\u03b8 yi +m) = (t+cos \u03b8 j ) cos \u03b8 j (i.e., purple line). During the training stage, the decision boundary for hard samples changes from one purple line (early stage) to another (later stage), which emphasizes easy samples first and hard samples later. Comparison with Focal loss Focal loss is a soft mining-based loss, which is formulated as: \u03b2 , where \u03b1 and \u03b2 are modulating factors that need to be tuned manually. The definition of hard samples in Focal loss is ambiguous, since it always focuses on relatively hard samples by reducing the weight of easier samples during the entire training process. In contrast, the definition of hard samples in our CurricularFace is more clear, i.e., mis-classified samples. Meanwhile, the weights of hard samples are adaptively determined in different training stages. In this paper, we propose a novel Adaptive Curriculum Learning Loss that embeds the idea of adaptive curriculum learning into deep face recognition. Our key idea is to address easy samples in the early training stage and hard ones in the later stage. Our method is easy to implement and robust to converge. Extensive experiments on popular facial benchmarks demonstrate the effectiveness of our method compared to the state-of-the-art competitors. Following the main idea of this work, future research can be expanded in various aspects, including designing a better function N (\u00b7) for negative cosine similarity that shares similar adaptive characteristic during training, and investigating the effects of noise samples that could be optimized as hard samples.\n<|eot_id|>\n<|start_header_id|>assistent<|end_header_id|>\nHere is a summary of the scholarly document for a high school student:\n\nThe document discusses the importance of designing loss functions in deep face recognition, which is a type of artificial intelligence that helps computers recognize human faces. The researchers propose a new loss function called CurricularFace, which is designed to adaptively adjust the importance of easy and hard samples during training. This means that the model first focuses on easy samples in the early stages of training and then shifts its attention to harder samples later on.\n\nThe CurricularFace loss function is inspired by the way humans learn, where we first learn simple concepts and then move on to more complex ones. The researchers also compare their method to other state-of-the-art methods, such as ArcFace and SV-Arc-Softmax, and show that their method outperforms them.\n\nThe document also highlights the limitations of current methods, such as not fully exploiting the importance of hard samples or over-emphasizing them in early training stages. The researchers argue that their CurricularFace method addresses these limitations and provides a more effective way to train deep face recognition models.\n\nOverall, the document presents a new approach to deep face recognition that is inspired by human learning and adapts to the difficulty of samples during training. The results show that this approach outperforms other state-of-the-art methods, making it a promising direction for future research.\n\nImportant entities:\n\n* Deep face recognition\n* Loss functions\n* CurricularFace\n* ArcFace\n* SV-Arc-Softmax\n* Focal loss\n\nImportant dates:\n\n* Not applicable (this is a scholarly document, not a historical or event-based document)\n\nEvents:\n\n* The development of deep face recognition\n* The proposal of the CurricularFace loss function\n* The comparison of CurricularFace to other state-of-the-art methods\n\nResult:\n\n* The CurricularFace loss function outperforms other state-of-the-art methods in deep face recognition."
        }
    }
]