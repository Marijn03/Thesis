The document discusses the limitations of training artificial agents to operate in one environment, which often leads to overfitting and inability to generalize to changes in the environment. The authors investigate this issue in the context of visual navigation tasks and propose a new method called invariance regularization to improve the generalization of policies to unseen environments. The method combines reinforcement learning with supervised learning to encourage the policy to be invariant to variations in observations that do not affect the action taken. The authors show that deep reinforcement learning agents can overfit even when trained on multiple environments simultaneously, and that their proposed method improves the generalization of policies to environments not seen during training. The authors also discuss the importance of generalization in reinforcement learning, as agents may encounter changes in the environment that can degrade their performance. The document highlights the need for robust policies that can generalize to different environments and scenarios, and proposes future directions for research in this area.

The important entities in this document are the artificial agents, reinforcement learning algorithms, and the environments in which they operate. The important dates mentioned are not explicitly stated, but the document references several research papers published in the years 2015-2019. The events described in the document are the training of artificial agents in different environments, the overfitting of these agents, and the proposed method of invariance regularization to improve generalization. The result of these events is the development of more robust and generalizable policies that can operate effectively in a variety of environments.
