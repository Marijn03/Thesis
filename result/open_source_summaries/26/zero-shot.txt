The paper discusses the limitations of dropout, a technique used to improve generalization performance and prevent overfitting in deep neural networks (DNNs). The authors identify three key observations about dropout: it acts as a smoothing technique, encourages local linear models to be trained on nearby data points, and has inconsistent normalization between training and testing conditions when used with batch normalization. To address these issues, the authors propose a new method called "Jumpout," which samples the dropout rate using a monotone decreasing distribution, adapts the dropout rate to each layer and sample, and rescales the outputs to maintain consistency between training and testing phases. The results show that Jumpout significantly improves performance on various datasets while introducing negligible additional memory and computation costs. The authors also highlight the limitations of dropout, including the need to tune dropout rates, potential for over-perturbation, and incompatibility with batch normalization.
