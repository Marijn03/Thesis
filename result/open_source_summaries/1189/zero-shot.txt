Imagine you're trying to learn a new concept, like a new language or a new skill. A good teacher would tailor their teaching to your individual needs and background, right? For example, if you're interested in music, they might use music-related examples to help you understand the concept better. In the same way, when machine learning models are trying to learn from each other, it would be helpful if they could tailor their teaching to the specific needs of the student model. This is exactly what the researchers in this study did. They designed a new approach called "data augmentation agents" that generate training data specifically for the teacher and student models, taking into account the differences in their capacities and abilities. By doing so, they found that the student model can learn more effectively from the teacher model, and the results are more accurate. This approach is particularly useful when the teacher model has a higher capacity (more parameters or higher bit-widths) than the student model. The researchers tested their approach on popular neural architectures and found that it improves the effectiveness of knowledge distillation, which is a common method for transferring knowledge from one model to another.
