Convolutional neural networks (CNNs) are commonly trained on fixed image sizes, but they can be used to evaluate images of various sizes at test time. However, this can be a limitation, as CNNs are sensitive to image size changes. To address this issue, researchers have proposed various methods to make CNNs more scale-invariant, such as using data augmentation or modifying the network structure. In this study, the authors introduce a novel mixed-size training regime called "MixSize" that mixes several image sizes at training time. This regime allows CNNs to be more resilient to image size changes and generalize well even on small images. As a result, models trained using MixSize can be evaluated at smaller image sizes, reducing the computational requirements for inference. For example, a ResNet50 model trained using MixSize achieved a top-1 accuracy of 76.43% with an image size of 160, which is comparable to the accuracy of the baseline model with 2x fewer computations. The MixSize regime also allows for a trade-off between training time and final accuracy, making it a promising approach for practical applications.

The important entities in this document are convolutional neural networks (CNNs), image sizes, and the MixSize training regime. The important dates mentioned are not explicitly stated, but the references provided are from 2012 to 2019. The events described in the document are the development of the MixSize training regime and its evaluation on various image sizes. The result of these events is that the MixSize regime allows CNNs to be more resilient to image size changes and generalize well even on small images, making it a promising approach for practical applications.
