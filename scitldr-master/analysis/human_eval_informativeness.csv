,paper_id,title,auth_pr,tldr,area_field_topic,problem_motivation,mode_of_contrib,details_descrip,results_findings,value_signif
1,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,baseline_aic,"In this paper, the authors propose to use ridge regression and logistic regression as the main adaptation mechanism for few-shot learning.",1,0,1,0,0,0
3,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,tldr_gen_aic,The authors use ridge regression methods from standard machine learning methods to make deep networks learn from fewer examples.,1,0,1,0,0,0
5,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,baseline_aic,"The authors investigate the representational capacity of learned wasserstein embeddings, showing that they can embed a wide variety of complex metric structures.",1,0,0,0,1,0
7,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,tldr_gen_aic,"The authors embed data as discrete probability distributions in a wasserstein space, endowed with an optimal transport metric.",1,0,0,1,0,0
9,SygHGnRqK7,Probabilistic Federated Neural Matching,baseline_aic,"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.",1,1,0,0,0,0
11,SygHGnRqK7,Probabilistic Federated Neural Matching,tldr_gen_aic,Federated learning with neural networks .,1,0,0,0,0,0
13,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,baseline_aic,The authors develop a vae enhancement that requires no additional hyperparameters or tuning that is competitive with a variety of gan models.,1,0,1,0,0,1
15,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,tldr_gen_aic,The authors analyze and improve variational autoencoders and develop a simple enhancement that improves their generative ability to generate crisp samples.,1,0,1,0,1,0
17,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,baseline_aic,The authors propose an attack that is optimal against a variety of confidence thresholding models.,1,0,1,0,0,0
19,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,tldr_gen_aic,Maxconfidence: a tradeoff curve-based attack against confidence thresholding models .,1,0,1,1,0,0
21,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,baseline_aic,"The authors propose a model, network of gcn (n-gcn), which marries these two lines of work.",1,0,1,0,0,0
23,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,tldr_gen_aic,"The authors propose network of graph convolutional networks (n-gcn), a novel architecture for semi-supervised node classification on graphs.",1,0,1,0,0,1
25,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,baseline_aic,"In this paper, the authors propose an efficient learning strategy to mimic features stage by stage and show that the proposed approach significantly narrows down the gap between",1,0,1,1,1,0
27,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,tldr_gen_aic,"The authors propose a task independent knowledge transfer approach for deep learning, where student is trained to mimic features from teacher stage by stage.",1,0,1,1,0,0
29,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,baseline_aic,"In this work, the authors face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages their finding that entropy minimization",1,0,1,0,0,1
31,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,tldr_gen_aic,Unsupervised domain adaptation with deep learning .,1,0,0,0,0,0
33,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,baseline_aic,"The authors propose a conceptor-aided backpropagation algorithm, ""conceptor-aided backprop"" (cab), in which",1,0,1,0,0,0
35,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,tldr_gen_aic,The authors extend conceptor-aided backpropagation to deep feedforward networks and propose a method to overcome catastrophic interference.,1,1,1,0,0,0
37,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,baseline_aic,The authors propose a max-marginal ranking regularization term to avoid seq2seq models from producing the generic and uninformative responses.,1,1,1,0,0,0
39,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,tldr_gen_aic,On neural response generation from the perspective of human-to-human conversational corpora .,1,0,0,1,0,0
41,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,baseline_aic,"Our time-agnostic reframing of the prediction problem targets the minima of bottlenecks, where prediction is intuitively easiest.",0,0,1,1,0,0
43,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,tldr_gen_aic,"Time-agnostic visual prediction does not require predictions at specific time intervals, and it can work for both forward/future and intermediate frame prediction.",1,0,0,0,1,0
45,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,baseline_aic,The authors propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model.,1,0,1,1,0,0
47,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,tldr_gen_aic,The authors propose an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of,1,0,1,1,0,0
49,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,baseline_aic,"Empirical risk minimization (erm), with proper loss function and regularization, is the common practice of supervised classification.",1,0,0,0,0,0
51,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,tldr_gen_aic,"The authors prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of u data, but",1,0,1,0,1,0
53,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,baseline_aic,Somatic cancer mutation detection at ultra-low variant allele frequencies (vafs) .,1,0,0,0,0,0
55,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,tldr_gen_aic,The authors developed a novel architecture that enables the detection of cancer-associated mutations at ultra-low variant allele frequencies (vafs) in cell-,1,0,1,0,0,0
57,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,baseline_aic,The authors propose a suite of triage methods and compare them on problem spaces of varying complexity.,1,0,1,0,0,0
59,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,tldr_gen_aic,"The authors present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance",1,0,1,1,0,0
61,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,baseline_aic,The authors study the initialisation requirements of infinite-width networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights,1,0,0,1,1,0
63,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,tldr_gen_aic,The authors propose a principled approach to weight initialisation that allows the construction of infinite-width networks with more than two hidden layers.,1,0,1,1,0,0
65,Syl3_2JCZ,A Self-Organizing Memory Network,baseline_aic,The authors identify biologically plausible synaptic plasticity rules that can solve this fine-tuning problem without making strong assumptions like 'tight frame' representations.,1,1,1,0,0,1
67,Syl3_2JCZ,A Self-Organizing Memory Network,tldr_gen_aic,The authors derive biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing in working memory to be retained for longer than the time,1,1,1,0,0,0
69,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,baseline_aic,"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.",1,0,0,0,0,0
71,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,tldr_gen_aic,The authors propose new loss components that improve the quality of kmeans clustering in terms of mutual information scores and outperforms previous methods.,1,0,1,0,1,1
73,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,baseline_aic,The authors propose a new structure named pre-dropout to solve the feature-reuse obstruction when applying standard dropout method on densenet.,1,1,1,0,0,0
75,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,tldr_gen_aic,"The authors design a specialized dropout method for densenet from three aspects, dropout location, drop out granularity, and dropout probability.",1,0,1,1,0,0
77,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,baseline_aic,The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.,1,1,0,0,0,0
79,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,tldr_gen_aic,"The authors propose a simple and efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model",1,1,1,0,0,1
81,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,baseline_aic,The authors propose the maximal divergence sequential auto-encoder for binary code vulnerability detection.,1,1,1,0,0,0
83,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,tldr_gen_aic,The authors propose a novel method for binary code vulnerability detection using deep learning techniques to extract crucial information from the original binaries.,1,1,1,0,0,1
85,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,baseline_aic,In this paper the authors show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.,1,0,0,0,1,0
87,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,tldr_gen_aic,The authors present a principled architecture for posterior attention for sequence to sequence learning.,1,0,1,0,0,1
89,By3VrbbAb,Realtime query completion via deep language models,baseline_aic,The authors propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix.,1,0,1,0,0,0
91,By3VrbbAb,Realtime query completion via deep language models,tldr_gen_aic,The authors present a real-time search query completion approach based upon character-level deep language models.,1,0,1,0,0,0
93,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,baseline_aic,The authors present proxylessnas that can directly learn the architectures for large-scale target tasks and target hardware platforms.,1,0,1,0,0,0
95,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,tldr_gen_aic,The authors address the high memory consumption issue of differentiable nas and reduce the computational cost (gpu hours and gpu memory) to the same level of regular,1,1,0,1,0,0
97,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,baseline_aic,"In this paper, the authors are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of",1,0,1,1,0,0
99,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,tldr_gen_aic,The authors propose a variational dirichlet framework with entropy-based uncertainty measure to detect out-of-distribution examples.,1,0,1,1,0,0
101,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,baseline_aic,The authors present a new technique for learning visual-semantic embeddings for cross-modal retrieval.,1,0,1,0,0,0
103,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,tldr_gen_aic,"Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, the authors introduce a simple change to common loss functions",1,1,1,0,0,0
105,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,baseline_aic,"In this paper, the authors address such optimal assumption by learning only from the most suitable demonstrations in a given set.",1,0,1,1,0,0
107,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,tldr_gen_aic,"The authors propose a generic framework with the demonstration suitability assessor leveraging meta-learning, where the authors train adaptive parameters via meta-imitation-learning",1,0,1,1,0,0
109,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,baseline_aic,The authors provide a theoretical justification to self-normalization by viewing nce as a low-rank matrix approximation.,1,0,1,1,0,0
111,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,tldr_gen_aic,The authors provide a theoretical justification to the self-normalization properties of language models trained using noise contrastive estimation and provide empirical comparison to the alternative,1,0,1,1,0,0
113,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,baseline_aic,"The authors show that the generalization error of linear graph embedding methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension",1,0,0,0,1,0
115,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,tldr_gen_aic,"The authors show that the generalization performance of linear graph embedding methods is not due to the dimensionality constraint as commonly believed, but rather the small",1,0,0,0,1,0
117,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,baseline_aic,The maximum mean discrepancy between two probability measures p and q is a metric that is zero if and only if all moments of the two measures are equal,1,0,0,0,1,0
119,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,tldr_gen_aic,The authors construct an unbiased estimator for the maximum mean discrepancy between two probability measures p and q and use it to train a generative neural network.,1,0,1,1,0,0
121,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,baseline_aic,"The authors define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected",1,0,0,1,0,0
123,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,tldr_gen_aic,The authors define the active learning problem for deep cnns as core-set selection problem and provide a rigorous bound between an average loss over any given subset,1,0,0,1,0,0
125,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,baseline_aic,This paper attempts to provide an alternative understanding of deep learning generalization from the perspective of maximum entropy.,1,0,1,0,0,0
127,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,tldr_gen_aic,Dnn is essentially a recursive solution to approximate the feature conditions and thus maximally fulfill maximum entropy principle.,0,0,0,0,0,0
129,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,baseline_aic,"The authors proposed waat, a 3d authoring tool allowing user to quickly create 3d models of the assembly line in a boiler factory.",1,0,1,1,0,0
131,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,tldr_gen_aic,Waat: a 3d authoring tool for the training of assembly line operators .,1,0,1,0,0,0
133,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,baseline_aic,The authors study the dynamics of gans from an optimization point of view.,1,0,0,0,0,0
135,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,tldr_gen_aic,The authors prove that the first-order iterative method in training generative adversarial networks converges to a stationary solution with a sublinear rate.,1,0,1,0,1,0
137,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,baseline_aic,In this paper the authors propose a new defensive mechanism under the generative adversarial network~(gan) framework.,1,0,1,0,0,0
139,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,tldr_gen_aic,The authors propose a new defensive method based on generative adversarial network to defend against black box attacks.,1,1,1,0,0,0
141,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,baseline_aic,The authors propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.,0,1,0,1,0,0
143,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,tldr_gen_aic,The authors significantly reduce convergence damage caused by gradient dropping through data-parallelism training by combining local and global gradients.,1,0,0,1,1,1
145,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,baseline_aic,The authors introduce an off-policy rl algorithm (metamimic) to narrow this gap.,1,1,1,0,0,0
147,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,tldr_gen_aic,The authors introduce a meta-learning approach to learn high-fidelity one-shot imitation policies by off-policy rl.,1,1,1,0,0,0
149,Hk91SGWR-,Investigating Human Priors for Playing Video Games,baseline_aic,This paper investigates the role of human priors for solving video games.,1,0,0,0,0,0
151,Hk91SGWR-,Investigating Human Priors for Playing Video Games,tldr_gen_aic,The authors perform a series of ablation studies to quantify the importance of various priors for solving video games.,1,1,0,0,0,0
153,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,baseline_aic,A modified version of the fully connected layer the authors call a block diagonal inner product layer.,0,0,0,0,0,0
155,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,tldr_gen_aic,"The authors have shown that block diagonal inner product layers can reduce network size, training time and final execution time without significant harm to the network performance.",0,0,1,0,1,1
157,Bys4ob-Rb,Certified Defenses against Adversarial Examples,baseline_aic,The authors propose a method based on a semidefinite relaxation that outputs a certificate that no attack can force the error to exceed a certain value.,0,0,1,1,0,0
159,Bys4ob-Rb,Certified Defenses against Adversarial Examples,tldr_gen_aic,The authors study neural networks with one hidden layer with semidefinite relaxation and provide an adaptive regularizer that encourages robustness against all attacks.,0,1,0,1,0,0
161,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",baseline_aic,The authors prove that sgd minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.,0,0,1,0,1,0
163,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",tldr_gen_aic,"The authors prove that stochastic gradient descent implicitly performs variational inference, but not in the classical sense.",0,0,1,0,1,0
165,BkisuzWRW,Zero-Shot Visual Imitation,baseline_aic,The authors develop a zero-shot imitator that learns a goal-conditioned skill policy with a novel forward consistency loss.,1,0,1,0,0,0
167,BkisuzWRW,Zero-Shot Visual Imitation,tldr_gen_aic,The authors pursue an alternative paradigm for imitation learning in which an agent first explores the world without any expert supervision and then distills its experience into a goal,1,0,0,1,0,0
169,SJd0EAy0b,Generalized Graph Embedding Models,baseline_aic,"In this paper, the authors conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms.",1,0,0,1,0,0
171,SJd0EAy0b,Generalized Graph Embedding Models,tldr_gen_aic,The authors propose a novel and efficient multishot framework for embedding learning in generalized knowledge graphs.,1,0,1,0,0,1
173,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,baseline_aic,"The authors develop implicit causal models, a class of causal models that leverages neural architectures with an implicit density.",0,0,1,1,0,0
175,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,tldr_gen_aic,"The authors develop implicit causal models that generalize previous methods to capture important nonlinearities, such as gene-gene and gene-population interaction.",0,1,1,0,0,1
177,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,baseline_aic,Autodidactic iteration is able to solve the rubik's cube and the 15-puzzle without relying on human data.,0,1,0,0,1,1
179,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,tldr_gen_aic,"The authors develop autodidactic iteration, a new form of approximate policy iteration that solves the rubik's cube without human data",0,1,1,0,0,0
181,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,baseline_aic,Deep learning software demands reliability and performance.,0,0,0,0,0,0
183,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,tldr_gen_aic,"The authors present dlvm, a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and",0,0,1,1,0,0
185,Hkbd5xZRb,Spherical CNNs,baseline_aic,The authors propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.,1,0,0,1,0,0
187,Hkbd5xZRb,Spherical CNNs,tldr_gen_aic,Convolutional neural networks for spherical mnist images .,1,0,0,0,0,0
189,r1GaAjRcF7,Differentiable Greedy Networks,baseline_aic,The authors develop a subset selection algorithm that is differentiable and discrete and can model complex dependencies between elements in a straightforward and comprehensible way.,0,0,1,0,1,0
191,r1GaAjRcF7,Differentiable Greedy Networks,tldr_gen_aic,Differentiable greedy network for sentence selection .,0,0,0,0,0,0
193,ryxSrhC9KX,Revealing interpretable object representations from human behavior,baseline_aic,"The authors estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.",0,0,0,1,0,0
195,ryxSrhC9KX,Revealing interpretable object representations from human behavior,tldr_gen_aic,Low-dimensional embeddings for mental object concepts can be used to model human behavior and predict human typicality judgments.,0,0,0,0,1,0
197,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,baseline_aic,This paper proposes an approach for text generation using generative adversarial networks with skip-thought vectors.,1,0,1,0,0,0
199,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,tldr_gen_aic,This work presents an approach to text generation using skip-thought sentence embeddings in conjunction with gans based on gradient penalty functions and,1,0,1,1,0,0
201,BJemQ209FQ,Learning to Navigate the Web,baseline_aic,The authors present two methods for reinforcement learning in large state and action spaces with sparse rewards for the web navigation.,1,1,1,0,0,0
203,BJemQ209FQ,Learning to Navigate the Web,tldr_gen_aic,"The authors train dqn, deep reinforcement learning agent, with q-value function approximated with a novel qweb neural network architecture on these smaller",0,0,0,0,1,0
0,rkeZRGbRW,Variance Regularizing Adversarial Learning,auth_gold,"The authors introduce meta-adversarial learning, a new technique to regularize gans, and propose a training method by explicitly controlling the discriminator's output distribution.",1,0,1,1,0,1
1,rkeZRGbRW,Variance Regularizing Adversarial Learning,pr_gold,The paper proposes variance regularizing adversarial learning for training gans to ensure that the gradient for the generator does not vanish,1,1,1,1,0,0
2,B1e4wo09K7,Invariant-equivariant representation learning for multi-class data,auth_gold,This paper presents a novel latent-variable generative modelling technique that enables the representation of global information into one latent variable and local information into another latent variable.,1,0,1,1,0,1
3,B1e4wo09K7,Invariant-equivariant representation learning for multi-class data,pr_gold,The paper presents a vae that uses labels to separate the learned representation into an invariant and a covariant part.,1,0,1,1,0,0
4,rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,auth_gold,"The authors show how using skip connections can make speech enhancement models more interpretable, as it makes them use similar mechanisms that have been explored in the dsp literature.",1,1,1,0,1,0
5,rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,pr_gold,"The authors propose incorporating residual, highway and masking blocks inside a fully convolutional pipeline in order to understand how iterative inference of the output and the masking is performed in a speech enhancement task",1,1,1,1,0,0
6,rJl0r3R9KX,Regularized Learning for  Domain Adaptation under Label Shifts,auth_gold,A practical and provably guaranteed approach for training efficiently classifiers in the presence of label shifts between source and target data sets,1,1,1,0,0,1
7,rJl0r3R9KX,Regularized Learning for  Domain Adaptation under Label Shifts,pr_gold,The authors propose a new algorithm for improving the stability of class importance weighting estimation procedure with a two-step procedure.,1,1,1,1,0,1
8,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,auth_gold,A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.,1,0,1,1,0,1
9,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,pr_gold,This paper proposes two regularization terms based on a compound hinge loss over the kl divergence between two softmax-normalized input arguments to encourage learning disentangled representations,1,1,1,1,0,0
10,r1q7n9gAb,The Implicit Bias of Gradient Descent on Separable Data,auth_gold,The normalized solution of gradient descent on logistic regression (or a similarly decaying loss) slowly converges to the l2 max margin solution on separable data.,1,0,0,0,1,0
11,r1q7n9gAb,The Implicit Bias of Gradient Descent on Separable Data,pr_gold,The paper offers a formal proof that gradient descent on the logistic loss converges very slowly to the hard svm solution in the case where the data are linearly separable.,1,0,1,0,1,0
12,r1xwKoR9Y7,GamePad: A Learning Environment for Theorem Proving,auth_gold,The authors introduce a system called gamepad to explore the application of machine learning methods to theorem proving in the coq proof assistant.,1,0,1,0,0,0
13,r1xwKoR9Y7,GamePad: A Learning Environment for Theorem Proving,pr_gold,"This paper describes a system for applying machine learning to interactive theorem proving, focuses on tasks of tactic prediction and position evaluation, and shows that a neural model outperforms an svm on both tasks.",1,0,1,1,1,0
14,4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,auth_gold,A mixed reality driving simulator using stereo cameras and passthrough vr evaluated in a user study with 24 participants.,1,0,1,1,0,0
15,4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,pr_gold,Proposes a complicated system for driving simulation.,1,0,1,0,0,0
16,SygHGnRqK7,Probabilistic Federated Neural Matching,auth_gold,The authors propose a bayesian nonparametric model for federated learning with neural networks.,1,0,1,0,0,0
17,SygHGnRqK7,Probabilistic Federated Neural Matching,pr_gold,Uses beta process to do federated neural matching.,1,0,0,1,0,0
18,Sklsm20ctX,Competitive experience replay,auth_gold,A novel method to learn with sparse reward using adversarial reward re-labeling,1,0,1,1,0,1
19,Sklsm20ctX,Competitive experience replay,pr_gold,Proposes to use a competitive multi-agent setting for encouraging exploration and shows that cer + her > her ~ cer,1,1,1,0,1,0
20,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,auth_gold,The authors prove that nce is self-normalized and demonstrate it on datasets,1,0,1,0,1,0
21,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,pr_gold,Presents a proof of the self normalization of nce as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.,1,0,1,0,1,0
22,B1G9doA9F7,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,auth_gold,A new cyclic adversarial learning augmented with auxiliary task model which improves domain adaptation performance in low resource supervised and unsupervised situations,1,0,1,1,1,1
23,B1G9doA9F7,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,pr_gold,Proposes an extension of cycle-consistent adversatial adaptation methods in order to tackle domain adaptation where limited supervised target data is available.,1,1,1,1,0,0
24,HkinqfbAb,Automatic Parameter Tying in Neural Networks,auth_gold,A k-means prior combined with l1 regularization yields state-of-the-art compression results.,1,0,0,1,0,1
25,HkinqfbAb,Automatic Parameter Tying in Neural Networks,pr_gold,This paper explores soft parameter tying and compression of dnns/cnns,1,0,0,0,0,0
26,rkzcvoA9YX,Few-Shot Learning by Exploiting Object Relation,auth_gold,Few-shot learning by exploiting the object-level relation to learn the image-level relation (similarity),1,0,1,0,0,0
27,rkzcvoA9YX,Few-Shot Learning by Exploiting Object Relation,pr_gold,This paper deals with the problem of few-shot learning by proposing an embedding-based approach that learns to compare object-level features between support and query set examples,1,1,1,1,0,0
28,H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,auth_gold,A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.,1,0,1,0,1,0
29,H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,pr_gold,The paper proposes an algorithm to restrict the staleness in asynchronous sgd and provides theoretical analysis,1,1,1,0,1,0
30,rJQDjk-0b,Unbiased Online Recurrent Optimization,auth_gold,"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.",1,0,1,0,0,1
31,rJQDjk-0b,Unbiased Online Recurrent Optimization,pr_gold,The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the imitation of truncated backpropagation through time,1,1,1,0,0,1
32,rye4g3AqFm,Deep learning generalizes because the parameter-function map is biased towards simple functions,auth_gold,The parameter-function map of deep networks is hugely biased; this can explain why they generalize. the authors use pac-bayes and gaussian processes to obtain nonvacuous bounds.,1,1,1,1,1,0
33,rye4g3AqFm,Deep learning generalizes because the parameter-function map is biased towards simple functions,pr_gold,"The paper studies the generalization capabilities of deep neural networks, with the help of the pac-bayesian learning theory and empirically backed intuitions.",1,0,0,1,0,0
34,B16yEqkCZ,Avoiding Catastrophic States with Intrinsic Fear,auth_gold,Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.,1,1,1,1,0,0
35,B16yEqkCZ,Avoiding Catastrophic States with Intrinsic Fear,pr_gold,An rl algorithm that combines the dqn algorithm with a fear model trained in parallel to predict catastropohic states.,1,1,1,1,0,0
36,HyHmGyZCZ,Comparison of Paragram and GloVe Results for Similarity Benchmarks,auth_gold,Paper provides a description of a procedure to enhance word vector space model with an evaluation of paragram and glove models for similarity benchmarks.,1,0,1,1,0,0
37,HyHmGyZCZ,Comparison of Paragram and GloVe Results for Similarity Benchmarks,pr_gold,This paper suggests a new algorithm that adjusts glove word vectors and then uses a non-euclidean similarity function between them.,1,0,1,1,0,1
38,SySpa-Z0Z,From Information Bottleneck To Activation Norm Penalty,auth_gold,The authors derive a norm penalty on the output of the neural network from the information bottleneck perspective,1,0,1,1,0,0
39,SySpa-Z0Z,From Information Bottleneck To Activation Norm Penalty,pr_gold,"Puts forward activation norm penalty, an l_2 type regularization on the activations, deriving it from the information bottleneck principle",1,0,1,1,0,0
40,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,auth_gold,The authors solve the rubik's cube with pure reinforcement learning,1,0,1,0,0,0
41,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,pr_gold,Solution to solving rubik cube using reinforcement learning (rl) with monte-carlo tree search (mcts) through autodidactic iteration.,1,0,1,1,0,0
42,ryeHw1vjiQ,Variational Autoencoders with implicit priors for short-duration text-independent speaker verification,auth_gold,The authors evaluate the effectiveness of having auxiliary discriminative tasks performed on top of statistics of the posterior distribution learned by variational autoencoders to enforce speaker dependency.,1,0,1,1,0,0
43,ryeHw1vjiQ,Variational Autoencoders with implicit priors for short-duration text-independent speaker verification,pr_gold,Propose an autoencoder model to learn a representation for speaker verification using short-duration analysis windows.,1,0,1,0,0,0
44,rJhR_pxCZ,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,auth_gold,The authors combine differentiable decision trees with supervised variational autoencoders to enhance interpretability of classification.,1,1,1,0,0,0
45,rJhR_pxCZ,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,pr_gold,"This paper proposes a hybrid model of a variational autoencoder composed with a differentiable decision tree, and an accompanying training scheme, with experiments demonstrating tree classification performance, neg. log likelihood performance, and latent space interpretability.",1,0,1,1,0,0
46,ryf7ioRqFX,h-detach: Modifying the LSTM Gradient Towards Better Optimization,auth_gold,A simple algorithm to improve optimization and handling of long term dependencies in lstm,1,1,1,0,0,0
47,ryf7ioRqFX,h-detach: Modifying the LSTM Gradient Towards Better Optimization,pr_gold,The paper introduces a simple stochastic algorithm called h-detach that is specific to lstm optimization and targeted towards addressing this problem.,1,0,1,0,0,0
48,S1fNJhRqFX,Exploration using Distributional RL and UCB,auth_gold,Exploration using distributional rl and truncagted variance.,1,0,0,1,0,0
49,S1fNJhRqFX,Exploration using Distributional RL and UCB,pr_gold,Presents an rl method to manage exploration-explotation trade-offs via ucb techniques.,1,1,0,1,0,0
50,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,auth_gold,"The authors propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.",1,0,1,1,1,1
51,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,pr_gold,"The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep nn), and treating each task as having its own final layer which could be a ridge regression or a logistic regression.",1,0,1,1,0,0
52,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,auth_gold,The authors show that wasserstein spaces are good targets for embedding data with complex semantic structure.,1,0,0,0,1,0
53,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,pr_gold,"Learns embeddings in a discrete space of probability distributions, using a minimized, regularised version of wasserstein distances.",1,0,0,1,0,0
54,SygHGnRqK7,Probabilistic Federated Neural Matching,auth_gold,The authors propose a bayesian nonparametric model for federated learning with neural networks.,1,0,1,0,0,0
55,SygHGnRqK7,Probabilistic Federated Neural Matching,pr_gold,Uses beta process to do federated neural matching.,1,0,0,1,0,0
56,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,auth_gold,The authors closely analyze the vae objective function and draw novel conclusions that lead to simple enhancements.,1,0,0,0,0,1
57,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,pr_gold,Proposes a two-stage vae method to generate high-quality samples and avoid blurriness.,1,1,1,0,0,0
58,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,auth_gold,The authors present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding,1,0,1,1,0,0
59,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,pr_gold,"This paper introduces a family of attack on confidence thresholding algortihms, focusing mainly on evaluation methodologies.",1,0,1,0,0,0
60,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,auth_gold,"The authors make a network of graph convolution networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.",1,0,1,1,1,1
61,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,pr_gold,Proposes a new network of gcns with two approaches: a fully connected layer on top of stacked features and attention mechanism that uses scalar weight per gcn.,1,0,1,1,0,1
62,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,auth_gold,This paper proposes to transfer knowledge from deep model to shallow one by mimicking features stage by stage.,1,0,1,1,0,0
63,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,pr_gold,Explains a stage by stage knowledge transer method by using different structures of resnets,1,0,1,1,0,0
64,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,auth_gold,A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization,1,0,1,0,1,0
65,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,pr_gold,"Improves the correlation alignment approach to domain adaptation by replacing the euclidean distance with the geodesic log-euclidean distance between two covariance matices, and automatically selecting the balancing cost by the entropy on the target domain.",1,0,1,1,1,0
66,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,auth_gold,"The authors propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.",1,0,1,1,0,0
67,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,pr_gold,"This paper applies the notion of conceptors, a form a regulariser, to prevent forgetting in continual learning in the training of neural networks on sequential tasks.",1,1,1,1,0,0
68,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,auth_gold,Analyze the reason for neural response generative models preferring universal replies; propose a method to avoid it.,1,1,1,0,0,0
69,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,pr_gold,Investigates the problem of universal replies plaguing the seq2seq neural generation models,1,1,0,0,0,0
70,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,auth_gold,"In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent ""bottleneck state"" predictions, which are useful for planning.",1,0,1,0,1,1
71,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,pr_gold,"A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction.",1,0,1,1,0,0
72,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,auth_gold,Compact perception of dynamical process,1,0,0,0,0,0
73,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,pr_gold,Studies the problem of compactly representing the model of a complex dynamic system while preserving information by using an information bottleneck method.,1,0,0,1,0,0
74,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,auth_gold,"Three class priors are all you need to train deep models from only u data, while any two should not be enough.",1,0,0,0,1,0
75,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,pr_gold,Proposes an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors and discusses theoretical properties of the estimators.,1,0,1,0,0,0
76,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,auth_gold," current somatic mutation methods do not work with liquid biopsies (ie low coverage sequencing), the authors apply a cnn architecture to a unique representation of a read and its ailgnment, the authors show significant improvement over previous methods in the low frequency setting.",1,1,1,1,1,0
77,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,pr_gold,Proposes a cnn based solution called kittyhawk for somatic mutation calling at ultra low allele frequencies.,1,0,1,0,0,0
78,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,auth_gold,The authors seek to understand learned representations in compressed networks via an experimental regime the authors call deep net triage,1,0,1,1,0,0
79,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,pr_gold,Compares various initialization and training methods of transferring knowledge from vgg network to a smaller student network by replacing blocks of layers with single layers.,1,0,1,1,0,0
80,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,auth_gold,"The authors propose a method for the construction of arbitrarily deep infinite-width networks, based on which the authors derive a novel weight initialisation scheme for finite-width networks and demonstrate its competitive performance.",1,0,1,1,1,1
81,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,pr_gold,Proposes a weight initialization approach to enable infinitely deep and infinite-width networks with experimental results on small datasets.,1,0,1,1,0,0
82,Syl3_2JCZ,A Self-Organizing Memory Network,auth_gold,The authors derived biologically plausible synaptic plasticity learning rules for a recurrent neural network to store stimulus representations.,1,1,1,0,0,0
83,Syl3_2JCZ,A Self-Organizing Memory Network,pr_gold,A neural network model consisting of recurrently connected neurons and one or more redouts which aims to retain some output over time.,1,0,1,1,0,0
84,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,auth_gold,A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.,1,1,1,0,0,0
85,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,pr_gold,This paper proposes two regularization terms based on a compound hinge loss over the kl divergence between two softmax-normalized input arguments to encourage learning disentangled representations,1,1,1,1,0,0
86,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,auth_gold,"Realizing the drawbacks when applying original dropout on densenet, the authors craft the design of dropout method from three aspects, the idea of which could also be applied on other cnn models.",1,1,1,0,0,1
87,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,pr_gold,Application of different binary dropout structures and schedules with the specific aim to regularise the densenet architecture.,1,1,0,1,0,0
88,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,auth_gold,Detecting out-of-distribution samples by using low-order feature statistics without requiring any change in underlying dnn.,1,1,1,1,0,0
89,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,pr_gold,Presents an algorithm to detect out-of-distribution samples by using the running estimate of mean and variance within batchnorm layers to construct feature representations later fed into a linear classifier.,1,1,1,1,0,0
90,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,auth_gold,The authors propose a novel method named maximal divergence sequential auto-encoder that leverages variational autoencoder representation for binary code vulnerability detection.,1,1,1,1,0,0
91,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,pr_gold,"This paper proposes a variational autoencoder-based architecture for code embeddings for binary software vulnerability detection, with learned embeddings more effective at distinguishing between vulnerable and non-vulnerable binary code than baselines.",1,1,1,1,1,0
92,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,auth_gold,Computing attention based on posterior distribution leads to more meaningful attention and better performance,1,0,0,0,1,1
93,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,pr_gold,"This paper proposes a sequence to sequence model where attention is treated as a latent variable, and derives novel inference procedures for this model, obtaining improvements in machine translation and morphological inflection generation tasks.",1,0,1,1,1,0
94,By3VrbbAb,Realtime query completion via deep language models,auth_gold,Realtime search query completion using character-level lstm language models,1,0,0,1,0,0
95,By3VrbbAb,Realtime query completion via deep language models,pr_gold,"This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a cpu.",1,0,1,0,0,1
96,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,auth_gold,Proxy-less neural architecture search for directly learning architectures on large-scale target task (imagenet) while reducing the cost to the same level of normal training.,1,0,1,0,1,0
97,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,pr_gold,"This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on ""proxy"" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size.",1,0,0,1,0,0
98,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,auth_gold,A new framework based variational inference for out-of-distribution detection,1,0,1,0,0,0
99,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,pr_gold,Describes a probabilistic approach to quantifying uncertainty in dnn classification tasks that outperforms other sota methods in the task of out-of-distribution detection.,1,0,1,0,1,0
100,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,auth_gold,A new loss based on relatively hard negatives that achieves state-of-the-art performance in image-caption retrieval.,1,0,1,0,1,1
101,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,pr_gold,Learning joint embedding of sentences and images using triplet loss that is applied to hardest negatives instead of averaging over all triplets,1,0,1,1,0,0
102,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,auth_gold,The authors propose a framework to learn a good policy through imitation learning from a noisy demonstration set via meta-training a demonstration suitability assessor.,1,0,1,1,0,0
103,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,pr_gold,"Contributes a maml based algorithm to imitation learning which automatically determines if provided demonstrations are ""suitable"".",1,0,1,1,0,0
104,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,auth_gold,The authors prove that nce is self-normalized and demonstrate it on datasets,1,0,1,0,1,0
105,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,pr_gold,Presents a proof of the self normalization of nce as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.,1,0,1,0,1,0
106,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,auth_gold,The authors argue that the generalization of linear graph embedding is not due to the dimensionality constraint but rather the small norm of embedding vectors.,1,0,0,0,1,0
107,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,pr_gold,The authors show that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors rather than dimensionality constraints,1,0,0,0,1,0
108,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,auth_gold,"The authors propose an estimator for the maximum mean discrepancy, appropriate when a target distribution is only accessible via a biased sample selection procedure, and show that it can be used in a generative network to correct for this bias.",1,0,1,1,1,0
109,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,pr_gold,Proposes an importance-weighted estimator of the mmd to estimate the mmd between distributions based on samples biased according to a known or estimated unknown scheme.,1,0,1,1,0,0
110,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,auth_gold,The authors approach to the problem of active learning as a core-set selection problem and show that this approach is especially useful in the batch active learning setting which is crucial when training cnns.,1,0,0,1,0,0
111,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,pr_gold,The authors provide an algorithm-agnostic active learning algorithm for multi-class classification,1,0,1,0,0,0
112,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,auth_gold,The authors prove that dnn is a recursively approximated solution to the maximum entropy principle.,0,0,1,0,0,0
113,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,pr_gold,Presents a derivation which links a dnn to recursive application of maximum entropy model fitting.,0,0,1,0,0,0
114,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,auth_gold,This paper describe a 3d authoring tool for providing ar in assembly lines of industry 4.0,1,0,1,0,0,0
115,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,pr_gold,The paper addresses how ar authoring tools support training of assembly line systems and proposes an approach,1,0,0,0,0,0
116,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,auth_gold,"The authors show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training gans would in fact converge to a stationary solution with a sublinear rate.",0,1,0,0,1,0
117,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,pr_gold,This paper uses gans and multi-task learning to provide a convergence guarantee for primal-dual algorithms on certain min-max problems.,1,0,1,0,0,0
118,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,auth_gold,Jointly train an adversarial noise generating network with a classification network to provide better robustness to adversarial attacks.,1,1,0,1,0,0
119,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,pr_gold,"A gan solution for deep models of classification, faced to white and black box attacks, that produces robust models. ",1,1,1,0,0,0
120,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,auth_gold,The authors improve gradient dropping (a technique of only exchanging large gradients on distributed training) by incorporating local gradients while doing a parameter update to reduce quality loss and further improve the training time.,1,1,0,1,0,0
121,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,pr_gold,This paper proposes a 3 modes for combining local and global gradients to better use more computing nodes,0,1,0,0,0,0
122,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,auth_gold,"The authors present metamimic, an algorithm that takes as input a demonstration dataset and outputs (i) a one-shot high-fidelity imitation policy (ii) an unconditional task policy.",0,0,1,1,0,0
123,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,pr_gold,"The paper looks at the problem of one-shot imitation with high accuracy of imitation, extending ddpgfd to use only state trajectories.",1,0,0,1,0,0
124,Hk91SGWR-,Investigating Human Priors for Playing Video Games,auth_gold,The authors investigate the various kinds of prior knowledge that help human learning and find that general priors about objects play the most critical role in guiding human gameplay.,1,0,0,0,0,0
125,Hk91SGWR-,Investigating Human Priors for Playing Video Games,pr_gold,"The authors study by experiment, what aspects of human priors are the important for reinforcement learning in video games.",1,0,0,0,0,0
126,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,auth_gold,The authors look at neural networks with block diagonal inner product layers for efficiency.,0,1,0,0,0,0
127,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,pr_gold,"This paper proposes making the inner layers in a neural network be block diagonal, and discusses that block diagonal matrices are more efficient than pruning and block diagonal layers lead to more efficient networks.",0,1,1,0,1,0
128,Bys4ob-Rb,Certified Defenses against Adversarial Examples,auth_gold,"The authors demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.",1,1,1,0,0,1
129,Bys4ob-Rb,Certified Defenses against Adversarial Examples,pr_gold,Proposes a new defense against security attacks on neural networks with the atack model that outputs a security certificate on the algorithm.,1,0,1,1,0,0
130,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",auth_gold,"Sgd implicitly performs variational inference; gradient noise is highly non-isotropic, so sgd does not even converge to critical points of the original loss",0,0,0,1,1,0
131,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",pr_gold,This paper provides a variational analysis of sgd as a non-equilibrium process.,1,0,1,0,0,0
132,BkisuzWRW,Zero-Shot Visual Imitation,auth_gold,Agents can learn to imitate solely visual demonstrations (without actions) at test time after learning from their own experience without any form of supervision at training time.,0,1,0,0,0,0
133,BkisuzWRW,Zero-Shot Visual Imitation,pr_gold,This paper proposes and approach for zero-shot visual learning by learning parametric skill functions.,1,0,1,0,0,0
134,SJd0EAy0b,Generalized Graph Embedding Models,auth_gold,Generalized graph embedding models,1,0,0,0,0,0
135,SJd0EAy0b,Generalized Graph Embedding Models,pr_gold,"A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches.

",1,0,1,1,1,0
136,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,auth_gold,Implicit models applied to causality and genetics,0,0,0,0,0,0
137,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,pr_gold,The authors propose to use the implicit model to tackle genome-wide association problem.,0,1,1,0,0,0
138,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,auth_gold,The authors solve the rubik's cube with pure reinforcement learning,0,1,0,0,0,0
139,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,pr_gold,Solution to solving rubik cube using reinforcement learning (rl) with monte-carlo tree search (mcts) through autodidactic iteration.,0,1,0,1,0,0
140,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,auth_gold,The authors introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks.,0,1,1,0,0,1
141,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,pr_gold,Proposal to move from ad-hoc code generation in deep learning engines to compiler and languages best practices.,0,1,0,0,0,0
142,Hkbd5xZRb,Spherical CNNs,auth_gold,"The authors introduce spherical cnns, a convolutional network for spherical signals, and apply it to 3d model recognition and molecular energy regression.",1,0,1,0,0,0
143,Hkbd5xZRb,Spherical CNNs,pr_gold,The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts,1,0,1,1,0,1
144,r1GaAjRcF7,Differentiable Greedy Networks,auth_gold,The authors propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.,0,0,1,1,0,0
145,r1GaAjRcF7,Differentiable Greedy Networks,pr_gold,Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'differentiable greedy network' (dgn).,0,0,1,1,0,0
146,ryxSrhC9KX,Revealing interpretable object representations from human behavior,auth_gold,Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks,0,0,0,0,0,0
147,ryxSrhC9KX,Revealing interpretable object representations from human behavior,pr_gold,This paper describes a large-scale experiment on human object/sematic representations and a model of such representations.,1,0,1,0,0,0
148,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,auth_gold,Generating text using sentence embeddings from skip-thought vectors with the help of generative adversarial networks.,1,0,0,0,0,0
149,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,pr_gold,Describes application of generative adversarial networks for modeling textual data with the help of ski-thought vectors and experiments with different flavors of gans for two different datasets.,1,0,0,0,0,0
150,BJemQ209FQ,Learning to Navigate the Web,auth_gold,"The authors train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.",1,1,0,1,0,0
151,BJemQ209FQ,Learning to Navigate the Web,pr_gold,"Develops a curriculum learning method for training an rl agent to navigate a web, based on the idea of decomposing an instruction in to multiple sub-instructions.",1,1,1,1,0,0
0,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,auth_gold,"The authors propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.",1,0,1,1,1,1
1,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,pr_gold,"The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep nn), and treating each task as having its own final layer which could be a ridge regression or a logistic regression.",1,0,1,1,0,0
2,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,baseline_aic,"In this paper, the authors propose to use ridge regression and logistic regression as the main adaptation mechanism for few-shot learning.",1,0,1,0,0,0
3,HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,tldr_gen_aic,The authors use ridge regression methods from standard machine learning methods to make deep networks learn from fewer examples.,1,0,1,0,0,0
4,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,auth_gold,The authors show that wasserstein spaces are good targets for embedding data with complex semantic structure.,1,0,0,0,1,0
5,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,pr_gold,"Learns embeddings in a discrete space of probability distributions, using a minimized, regularised version of wasserstein distances.",1,0,0,1,0,0
6,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,baseline_aic,"The authors investigate the representational capacity of learned wasserstein embeddings, showing that they can embed a wide variety of complex metric structures.",1,0,0,0,1,0
7,rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,tldr_gen_aic,"The authors embed data as discrete probability distributions in a wasserstein space, endowed with an optimal transport metric.",1,0,0,1,0,0
8,SygHGnRqK7,Probabilistic Federated Neural Matching,auth_gold,The authors propose a bayesian nonparametric model for federated learning with neural networks.,1,0,1,0,0,0
9,SygHGnRqK7,Probabilistic Federated Neural Matching,pr_gold,Uses beta process to do federated neural matching.,1,0,0,1,0,0
10,SygHGnRqK7,Probabilistic Federated Neural Matching,baseline_aic,"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.",1,1,0,0,0,0
11,SygHGnRqK7,Probabilistic Federated Neural Matching,tldr_gen_aic,Federated learning with neural networks .,1,0,0,0,0,0
12,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,auth_gold,The authors closely analyze the vae objective function and draw novel conclusions that lead to simple enhancements.,1,0,0,0,0,1
13,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,pr_gold,Proposes a two-stage vae method to generate high-quality samples and avoid blurriness.,1,1,1,0,0,0
14,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,baseline_aic,The authors develop a vae enhancement that requires no additional hyperparameters or tuning that is competitive with a variety of gan models.,1,0,1,0,0,1
15,B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,tldr_gen_aic,The authors analyze and improve variational autoencoders and develop a simple enhancement that improves their generative ability to generate crisp samples.,1,0,1,0,1,0
16,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,auth_gold,The authors present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding,1,0,1,1,0,0
17,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,pr_gold,"This paper introduces a family of attack on confidence thresholding algortihms, focusing mainly on evaluation methodologies.",1,0,1,0,0,0
18,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,baseline_aic,The authors propose an attack that is optimal against a variety of confidence thresholding models.,1,0,1,0,0,0
19,H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,tldr_gen_aic,Maxconfidence: a tradeoff curve-based attack against confidence thresholding models .,1,0,1,1,0,0
20,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,auth_gold,"The authors make a network of graph convolution networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.",1,0,1,1,1,1
21,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,pr_gold,Proposes a new network of gcns with two approaches: a fully connected layer on top of stacked features and attention mechanism that uses scalar weight per gcn.,1,0,1,1,0,1
22,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,baseline_aic,"The authors propose a model, network of gcn (n-gcn), which marries these two lines of work.",1,0,1,0,0,0
23,SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,tldr_gen_aic,"The authors propose network of graph convolutional networks (n-gcn), a novel architecture for semi-supervised node classification on graphs.",1,0,1,0,0,1
24,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,auth_gold,This paper proposes to transfer knowledge from deep model to shallow one by mimicking features stage by stage.,1,0,1,1,0,0
25,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,pr_gold,Explains a stage by stage knowledge transer method by using different structures of resnets,1,0,1,1,0,0
26,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,baseline_aic,"In this paper, the authors propose an efficient learning strategy to mimic features stage by stage and show that the proposed approach significantly narrows down the gap between",1,0,1,1,1,0
27,rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,tldr_gen_aic,"The authors propose a task independent knowledge transfer approach for deep learning, where student is trained to mimic features from teacher stage by stage.",1,0,1,1,0,0
28,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,auth_gold,A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization,1,0,1,0,1,0
29,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,pr_gold,"Improves the correlation alignment approach to domain adaptation by replacing the euclidean distance with the geodesic log-euclidean distance between two covariance matices, and automatically selecting the balancing cost by the entropy on the target domain.",1,0,1,1,1,0
30,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,baseline_aic,"In this work, the authors face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages their finding that entropy minimization",1,0,1,0,0,1
31,rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,tldr_gen_aic,Unsupervised domain adaptation with deep learning .,1,0,0,0,0,0
32,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,auth_gold,"The authors propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.",1,0,1,1,0,0
33,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,pr_gold,"This paper applies the notion of conceptors, a form a regulariser, to prevent forgetting in continual learning in the training of neural networks on sequential tasks.",1,1,1,1,0,0
34,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,baseline_aic,"The authors propose a conceptor-aided backpropagation algorithm, ""conceptor-aided backprop"" (cab), in which",1,0,1,0,0,0
35,B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,tldr_gen_aic,The authors extend conceptor-aided backpropagation to deep feedforward networks and propose a method to overcome catastrophic interference.,1,1,1,0,0,0
36,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,auth_gold,Analyze the reason for neural response generative models preferring universal replies; propose a method to avoid it.,1,1,1,0,0,0
37,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,pr_gold,Investigates the problem of universal replies plaguing the seq2seq neural generation models,1,1,0,0,0,0
38,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,baseline_aic,The authors propose a max-marginal ranking regularization term to avoid seq2seq models from producing the generic and uninformative responses.,1,1,1,0,0,0
39,H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,tldr_gen_aic,On neural response generation from the perspective of human-to-human conversational corpora .,1,0,0,1,0,0
40,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,auth_gold,"In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent ""bottleneck state"" predictions, which are useful for planning.",1,0,1,0,1,1
41,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,pr_gold,"A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction.",1,0,1,1,0,0
42,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,baseline_aic,"Our time-agnostic reframing of the prediction problem targets the minima of bottlenecks, where prediction is intuitively easiest.",0,0,1,1,0,0
43,SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,tldr_gen_aic,"Time-agnostic visual prediction does not require predictions at specific time intervals, and it can work for both forward/future and intermediate frame prediction.",1,0,0,0,1,0
44,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,auth_gold,Compact perception of dynamical process,1,0,0,0,0,0
45,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,pr_gold,Studies the problem of compactly representing the model of a complex dynamic system while preserving information by using an information bottleneck method.,1,0,0,1,0,0
46,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,baseline_aic,The authors propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model.,1,0,1,1,0,0
47,rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,tldr_gen_aic,The authors propose an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of,1,0,1,1,0,0
48,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,auth_gold,"Three class priors are all you need to train deep models from only u data, while any two should not be enough.",1,0,0,0,1,0
49,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,pr_gold,Proposes an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors and discusses theoretical properties of the estimators.,1,0,1,0,0,0
50,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,baseline_aic,"Empirical risk minimization (erm), with proper loss function and regularization, is the common practice of supervised classification.",1,0,0,0,0,0
51,B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,tldr_gen_aic,"The authors prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of u data, but",1,0,1,0,1,0
52,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,auth_gold," current somatic mutation methods do not work with liquid biopsies (ie low coverage sequencing), the authors apply a cnn architecture to a unique representation of a read and its ailgnment, the authors show significant improvement over previous methods in the low frequency setting.",1,1,1,1,1,0
53,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,pr_gold,Proposes a cnn based solution called kittyhawk for somatic mutation calling at ultra low allele frequencies.,1,0,1,0,0,0
54,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,baseline_aic,Somatic cancer mutation detection at ultra-low variant allele frequencies (vafs) .,1,0,0,0,0,0
55,H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,tldr_gen_aic,The authors developed a novel architecture that enables the detection of cancer-associated mutations at ultra-low variant allele frequencies (vafs) in cell-,1,0,1,0,0,0
56,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,auth_gold,The authors seek to understand learned representations in compressed networks via an experimental regime the authors call deep net triage,1,0,1,1,0,0
57,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,pr_gold,Compares various initialization and training methods of transferring knowledge from vgg network to a smaller student network by replacing blocks of layers with single layers.,1,0,1,1,0,0
58,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,baseline_aic,The authors propose a suite of triage methods and compare them on problem spaces of varying complexity.,1,0,1,0,0,0
59,HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,tldr_gen_aic,"The authors present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance",1,0,1,1,0,0
60,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,auth_gold,"The authors propose a method for the construction of arbitrarily deep infinite-width networks, based on which the authors derive a novel weight initialisation scheme for finite-width networks and demonstrate its competitive performance.",1,0,1,1,1,1
61,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,pr_gold,Proposes a weight initialization approach to enable infinitely deep and infinite-width networks with experimental results on small datasets.,1,0,1,1,0,0
62,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,baseline_aic,The authors study the initialisation requirements of infinite-width networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights,1,0,0,1,1,0
63,SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,tldr_gen_aic,The authors propose a principled approach to weight initialisation that allows the construction of infinite-width networks with more than two hidden layers.,1,0,1,1,0,0
64,Syl3_2JCZ,A Self-Organizing Memory Network,auth_gold,The authors derived biologically plausible synaptic plasticity learning rules for a recurrent neural network to store stimulus representations.,1,1,1,0,0,0
65,Syl3_2JCZ,A Self-Organizing Memory Network,pr_gold,A neural network model consisting of recurrently connected neurons and one or more redouts which aims to retain some output over time.,1,0,1,1,0,0
66,Syl3_2JCZ,A Self-Organizing Memory Network,baseline_aic,The authors identify biologically plausible synaptic plasticity rules that can solve this fine-tuning problem without making strong assumptions like 'tight frame' representations.,1,1,1,0,0,1
67,Syl3_2JCZ,A Self-Organizing Memory Network,tldr_gen_aic,The authors derive biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing in working memory to be retained for longer than the time,1,1,1,0,0,0
68,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,auth_gold,A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.,1,1,1,0,0,0
69,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,pr_gold,This paper proposes two regularization terms based on a compound hinge loss over the kl divergence between two softmax-normalized input arguments to encourage learning disentangled representations,1,1,1,1,0,0
70,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,baseline_aic,"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.",1,0,0,0,0,0
71,S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,tldr_gen_aic,The authors propose new loss components that improve the quality of kmeans clustering in terms of mutual information scores and outperforms previous methods.,1,0,1,0,1,1
72,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,auth_gold,"Realizing the drawbacks when applying original dropout on densenet, the authors craft the design of dropout method from three aspects, the idea of which could also be applied on other cnn models.",1,1,1,0,0,1
73,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,pr_gold,Application of different binary dropout structures and schedules with the specific aim to regularise the densenet architecture.,1,1,0,1,0,0
74,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,baseline_aic,The authors propose a new structure named pre-dropout to solve the feature-reuse obstruction when applying standard dropout method on densenet.,1,1,1,0,0,0
75,r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,tldr_gen_aic,"The authors design a specialized dropout method for densenet from three aspects, dropout location, drop out granularity, and dropout probability.",1,0,1,1,0,0
76,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,auth_gold,Detecting out-of-distribution samples by using low-order feature statistics without requiring any change in underlying dnn.,1,1,1,1,0,0
77,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,pr_gold,Presents an algorithm to detect out-of-distribution samples by using the running estimate of mean and variance within batchnorm layers to construct feature representations later fed into a linear classifier.,1,1,1,1,0,0
78,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,baseline_aic,The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.,1,1,0,0,0,0
79,rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,tldr_gen_aic,"The authors propose a simple and efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model",1,1,1,0,0,1
80,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,auth_gold,The authors propose a novel method named maximal divergence sequential auto-encoder that leverages variational autoencoder representation for binary code vulnerability detection.,1,1,1,1,0,0
81,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,pr_gold,"This paper proposes a variational autoencoder-based architecture for code embeddings for binary software vulnerability detection, with learned embeddings more effective at distinguishing between vulnerable and non-vulnerable binary code than baselines.",1,1,1,1,1,0
82,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,baseline_aic,The authors propose the maximal divergence sequential auto-encoder for binary code vulnerability detection.,1,1,1,0,0,0
83,ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,tldr_gen_aic,The authors propose a novel method for binary code vulnerability detection using deep learning techniques to extract crucial information from the original binaries.,1,1,1,0,0,1
84,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,auth_gold,Computing attention based on posterior distribution leads to more meaningful attention and better performance,1,0,0,0,1,1
85,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,pr_gold,"This paper proposes a sequence to sequence model where attention is treated as a latent variable, and derives novel inference procedures for this model, obtaining improvements in machine translation and morphological inflection generation tasks.",1,0,1,1,1,0
86,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,baseline_aic,In this paper the authors show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.,1,0,0,0,1,0
87,BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,tldr_gen_aic,The authors present a principled architecture for posterior attention for sequence to sequence learning.,1,0,1,0,0,1
88,By3VrbbAb,Realtime query completion via deep language models,auth_gold,Realtime search query completion using character-level lstm language models,1,0,0,1,0,0
89,By3VrbbAb,Realtime query completion via deep language models,pr_gold,"This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a cpu.",1,0,1,0,0,1
90,By3VrbbAb,Realtime query completion via deep language models,baseline_aic,The authors propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix.,1,0,1,0,0,0
91,By3VrbbAb,Realtime query completion via deep language models,tldr_gen_aic,The authors present a real-time search query completion approach based upon character-level deep language models.,1,0,1,0,0,0
92,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,auth_gold,Proxy-less neural architecture search for directly learning architectures on large-scale target task (imagenet) while reducing the cost to the same level of normal training.,1,0,1,0,1,0
93,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,pr_gold,"This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on ""proxy"" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size.",1,0,0,1,0,0
94,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,baseline_aic,The authors present proxylessnas that can directly learn the architectures for large-scale target tasks and target hardware platforms.,1,0,1,0,0,0
95,HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,tldr_gen_aic,The authors address the high memory consumption issue of differentiable nas and reduce the computational cost (gpu hours and gpu memory) to the same level of regular,1,1,0,1,0,0
96,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,auth_gold,A new framework based variational inference for out-of-distribution detection,1,0,1,0,0,0
97,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,pr_gold,Describes a probabilistic approach to quantifying uncertainty in dnn classification tasks that outperforms other sota methods in the task of out-of-distribution detection.,1,0,1,0,1,0
98,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,baseline_aic,"In this paper, the authors are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of",1,0,1,1,0,0
99,ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,tldr_gen_aic,The authors propose a variational dirichlet framework with entropy-based uncertainty measure to detect out-of-distribution examples.,1,0,1,1,0,0
100,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,auth_gold,A new loss based on relatively hard negatives that achieves state-of-the-art performance in image-caption retrieval.,1,0,1,0,1,1
101,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,pr_gold,Learning joint embedding of sentences and images using triplet loss that is applied to hardest negatives instead of averaging over all triplets,1,0,1,1,0,0
102,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,baseline_aic,The authors present a new technique for learning visual-semantic embeddings for cross-modal retrieval.,1,0,1,0,0,0
103,BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,tldr_gen_aic,"Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, the authors introduce a simple change to common loss functions",1,1,1,0,0,0
104,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,auth_gold,The authors propose a framework to learn a good policy through imitation learning from a noisy demonstration set via meta-training a demonstration suitability assessor.,1,0,1,1,0,0
105,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,pr_gold,"Contributes a maml based algorithm to imitation learning which automatically determines if provided demonstrations are ""suitable"".",1,0,1,1,0,0
106,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,baseline_aic,"In this paper, the authors address such optimal assumption by learning only from the most suitable demonstrations in a given set.",1,0,1,1,0,0
107,rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,tldr_gen_aic,"The authors propose a generic framework with the demonstration suitability assessor leveraging meta-learning, where the authors train adaptive parameters via meta-imitation-learning",1,0,1,1,0,0
108,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,auth_gold,The authors prove that nce is self-normalized and demonstrate it on datasets,1,0,1,0,1,0
109,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,pr_gold,Presents a proof of the self normalization of nce as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.,1,0,1,0,1,0
110,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,baseline_aic,The authors provide a theoretical justification to self-normalization by viewing nce as a low-rank matrix approximation.,1,0,1,1,0,0
111,H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,tldr_gen_aic,The authors provide a theoretical justification to the self-normalization properties of language models trained using noise contrastive estimation and provide empirical comparison to the alternative,1,0,1,1,0,0
112,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,auth_gold,The authors argue that the generalization of linear graph embedding is not due to the dimensionality constraint but rather the small norm of embedding vectors.,1,0,0,0,1,0
113,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,pr_gold,The authors show that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors rather than dimensionality constraints,1,0,0,0,1,0
114,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,baseline_aic,"The authors show that the generalization error of linear graph embedding methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension",1,0,0,0,1,0
115,B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,tldr_gen_aic,"The authors show that the generalization performance of linear graph embedding methods is not due to the dimensionality constraint as commonly believed, but rather the small",1,0,0,0,1,0
116,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,auth_gold,"The authors propose an estimator for the maximum mean discrepancy, appropriate when a target distribution is only accessible via a biased sample selection procedure, and show that it can be used in a generative network to correct for this bias.",1,0,1,1,1,0
117,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,pr_gold,Proposes an importance-weighted estimator of the mmd to estimate the mmd between distributions based on samples biased according to a known or estimated unknown scheme.,1,0,1,1,0,0
118,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,baseline_aic,The maximum mean discrepancy between two probability measures p and q is a metric that is zero if and only if all moments of the two measures are equal,1,0,0,0,1,0
119,SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,tldr_gen_aic,The authors construct an unbiased estimator for the maximum mean discrepancy between two probability measures p and q and use it to train a generative neural network.,1,0,1,1,0,0
120,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,auth_gold,The authors approach to the problem of active learning as a core-set selection problem and show that this approach is especially useful in the batch active learning setting which is crucial when training cnns.,1,0,0,1,0,0
121,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,pr_gold,The authors provide an algorithm-agnostic active learning algorithm for multi-class classification,1,0,1,0,0,0
122,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,baseline_aic,"The authors define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected",1,0,0,1,0,0
123,H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,tldr_gen_aic,The authors define the active learning problem for deep cnns as core-set selection problem and provide a rigorous bound between an average loss over any given subset,1,0,0,1,0,0
124,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,auth_gold,The authors prove that dnn is a recursively approximated solution to the maximum entropy principle.,0,0,1,0,0,0
125,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,pr_gold,Presents a derivation which links a dnn to recursive application of maximum entropy model fitting.,0,0,1,0,0,0
126,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,baseline_aic,This paper attempts to provide an alternative understanding of deep learning generalization from the perspective of maximum entropy.,1,0,1,0,0,0
127,r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,tldr_gen_aic,Dnn is essentially a recursive solution to approximate the feature conditions and thus maximally fulfill maximum entropy principle.,0,0,0,0,0,0
128,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,auth_gold,This paper describe a 3d authoring tool for providing ar in assembly lines of industry 4.0,1,0,1,0,0,0
129,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,pr_gold,The paper addresses how ar authoring tools support training of assembly line systems and proposes an approach,1,0,0,0,0,0
130,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,baseline_aic,"The authors proposed waat, a 3d authoring tool allowing user to quickly create 3d models of the assembly line in a boiler factory.",1,0,1,1,0,0
131,1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,tldr_gen_aic,Waat: a 3d authoring tool for the training of assembly line operators .,1,0,1,0,0,0
132,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,auth_gold,"The authors show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training gans would in fact converge to a stationary solution with a sublinear rate.",0,1,0,0,1,0
133,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,pr_gold,This paper uses gans and multi-task learning to provide a convergence guarantee for primal-dual algorithms on certain min-max problems.,1,0,1,0,0,0
134,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,baseline_aic,The authors study the dynamics of gans from an optimization point of view.,1,0,0,0,0,0
135,rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,tldr_gen_aic,The authors prove that the first-order iterative method in training generative adversarial networks converges to a stationary solution with a sublinear rate.,1,0,1,0,1,0
136,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,auth_gold,Jointly train an adversarial noise generating network with a classification network to provide better robustness to adversarial attacks.,1,1,0,1,0,0
137,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,pr_gold,"A gan solution for deep models of classification, faced to white and black box attacks, that produces robust models. ",1,1,1,0,0,0
138,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,baseline_aic,In this paper the authors propose a new defensive mechanism under the generative adversarial network~(gan) framework.,1,0,1,0,0,0
139,S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,tldr_gen_aic,The authors propose a new defensive method based on generative adversarial network to defend against black box attacks.,1,1,1,0,0,0
140,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,auth_gold,The authors improve gradient dropping (a technique of only exchanging large gradients on distributed training) by incorporating local gradients while doing a parameter update to reduce quality loss and further improve the training time.,1,1,0,1,0,0
141,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,pr_gold,This paper proposes a 3 modes for combining local and global gradients to better use more computing nodes,0,1,0,0,0,0
142,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,baseline_aic,The authors propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.,0,1,0,1,0,0
143,BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,tldr_gen_aic,The authors significantly reduce convergence damage caused by gradient dropping through data-parallelism training by combining local and global gradients.,1,0,0,1,1,1
144,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,auth_gold,"The authors present metamimic, an algorithm that takes as input a demonstration dataset and outputs (i) a one-shot high-fidelity imitation policy (ii) an unconditional task policy.",0,0,1,1,0,0
145,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,pr_gold,"The paper looks at the problem of one-shot imitation with high accuracy of imitation, extending ddpgfd to use only state trajectories.",1,0,0,1,0,0
146,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,baseline_aic,The authors introduce an off-policy rl algorithm (metamimic) to narrow this gap.,1,1,1,0,0,0
147,HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,tldr_gen_aic,The authors introduce a meta-learning approach to learn high-fidelity one-shot imitation policies by off-policy rl.,1,1,1,0,0,0
148,Hk91SGWR-,Investigating Human Priors for Playing Video Games,auth_gold,The authors investigate the various kinds of prior knowledge that help human learning and find that general priors about objects play the most critical role in guiding human gameplay.,1,0,0,0,0,0
149,Hk91SGWR-,Investigating Human Priors for Playing Video Games,pr_gold,"The authors study by experiment, what aspects of human priors are the important for reinforcement learning in video games.",1,0,0,0,0,0
150,Hk91SGWR-,Investigating Human Priors for Playing Video Games,baseline_aic,This paper investigates the role of human priors for solving video games.,1,0,0,0,0,0
151,Hk91SGWR-,Investigating Human Priors for Playing Video Games,tldr_gen_aic,The authors perform a series of ablation studies to quantify the importance of various priors for solving video games.,1,1,0,0,0,0
152,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,auth_gold,The authors look at neural networks with block diagonal inner product layers for efficiency.,0,1,0,0,0,0
153,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,pr_gold,"This paper proposes making the inner layers in a neural network be block diagonal, and discusses that block diagonal matrices are more efficient than pruning and block diagonal layers lead to more efficient networks.",0,1,1,0,1,0
154,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,baseline_aic,A modified version of the fully connected layer the authors call a block diagonal inner product layer.,0,0,0,0,0,0
155,HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,tldr_gen_aic,"The authors have shown that block diagonal inner product layers can reduce network size, training time and final execution time without significant harm to the network performance.",0,0,1,0,1,1
156,Bys4ob-Rb,Certified Defenses against Adversarial Examples,auth_gold,"The authors demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.",1,1,1,0,0,1
157,Bys4ob-Rb,Certified Defenses against Adversarial Examples,pr_gold,Proposes a new defense against security attacks on neural networks with the atack model that outputs a security certificate on the algorithm.,1,0,1,1,0,0
158,Bys4ob-Rb,Certified Defenses against Adversarial Examples,baseline_aic,The authors propose a method based on a semidefinite relaxation that outputs a certificate that no attack can force the error to exceed a certain value.,0,0,1,1,0,0
159,Bys4ob-Rb,Certified Defenses against Adversarial Examples,tldr_gen_aic,The authors study neural networks with one hidden layer with semidefinite relaxation and provide an adaptive regularizer that encourages robustness against all attacks.,0,1,0,1,0,0
160,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",auth_gold,"Sgd implicitly performs variational inference; gradient noise is highly non-isotropic, so sgd does not even converge to critical points of the original loss",0,0,0,1,1,0
161,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",pr_gold,This paper provides a variational analysis of sgd as a non-equilibrium process.,1,0,1,0,0,0
162,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",baseline_aic,The authors prove that sgd minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.,0,0,1,0,1,0
163,HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",tldr_gen_aic,"The authors prove that stochastic gradient descent implicitly performs variational inference, but not in the classical sense.",0,0,1,0,1,0
164,BkisuzWRW,Zero-Shot Visual Imitation,auth_gold,Agents can learn to imitate solely visual demonstrations (without actions) at test time after learning from their own experience without any form of supervision at training time.,0,1,0,0,0,0
165,BkisuzWRW,Zero-Shot Visual Imitation,pr_gold,This paper proposes and approach for zero-shot visual learning by learning parametric skill functions.,1,0,1,0,0,0
166,BkisuzWRW,Zero-Shot Visual Imitation,baseline_aic,The authors develop a zero-shot imitator that learns a goal-conditioned skill policy with a novel forward consistency loss.,1,0,1,0,0,0
167,BkisuzWRW,Zero-Shot Visual Imitation,tldr_gen_aic,The authors pursue an alternative paradigm for imitation learning in which an agent first explores the world without any expert supervision and then distills its experience into a goal,1,0,0,1,0,0
168,SJd0EAy0b,Generalized Graph Embedding Models,auth_gold,Generalized graph embedding models,1,0,0,0,0,0
169,SJd0EAy0b,Generalized Graph Embedding Models,pr_gold,"A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches.

",1,0,1,1,1,0
170,SJd0EAy0b,Generalized Graph Embedding Models,baseline_aic,"In this paper, the authors conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms.",1,0,0,1,0,0
171,SJd0EAy0b,Generalized Graph Embedding Models,tldr_gen_aic,The authors propose a novel and efficient multishot framework for embedding learning in generalized knowledge graphs.,1,0,1,0,0,1
172,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,auth_gold,Implicit models applied to causality and genetics,0,0,0,0,0,0
173,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,pr_gold,The authors propose to use the implicit model to tackle genome-wide association problem.,0,1,1,0,0,0
174,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,baseline_aic,"The authors develop implicit causal models, a class of causal models that leverages neural architectures with an implicit density.",0,0,1,1,0,0
175,SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,tldr_gen_aic,"The authors develop implicit causal models that generalize previous methods to capture important nonlinearities, such as gene-gene and gene-population interaction.",0,1,1,0,0,1
176,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,auth_gold,The authors solve the rubik's cube with pure reinforcement learning,0,1,0,0,0,0
177,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,pr_gold,Solution to solving rubik cube using reinforcement learning (rl) with monte-carlo tree search (mcts) through autodidactic iteration.,0,1,0,1,0,0
178,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,baseline_aic,Autodidactic iteration is able to solve the rubik's cube and the 15-puzzle without relying on human data.,0,1,0,0,1,1
179,Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,tldr_gen_aic,"The authors develop autodidactic iteration, a new form of approximate policy iteration that solves the rubik's cube without human data",0,1,1,0,0,0
180,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,auth_gold,The authors introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks.,0,1,1,0,0,1
181,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,pr_gold,Proposal to move from ad-hoc code generation in deep learning engines to compiler and languages best practices.,0,1,0,0,0,0
182,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,baseline_aic,Deep learning software demands reliability and performance.,0,0,0,0,0,0
183,ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,tldr_gen_aic,"The authors present dlvm, a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and",0,0,1,1,0,0
184,Hkbd5xZRb,Spherical CNNs,auth_gold,"The authors introduce spherical cnns, a convolutional network for spherical signals, and apply it to 3d model recognition and molecular energy regression.",1,0,1,0,0,0
185,Hkbd5xZRb,Spherical CNNs,pr_gold,The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts,1,0,1,1,0,1
186,Hkbd5xZRb,Spherical CNNs,baseline_aic,The authors propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.,1,0,0,1,0,0
187,Hkbd5xZRb,Spherical CNNs,tldr_gen_aic,Convolutional neural networks for spherical mnist images .,1,0,0,0,0,0
188,r1GaAjRcF7,Differentiable Greedy Networks,auth_gold,The authors propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.,0,0,1,1,0,0
189,r1GaAjRcF7,Differentiable Greedy Networks,pr_gold,Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'differentiable greedy network' (dgn).,0,0,1,1,0,0
190,r1GaAjRcF7,Differentiable Greedy Networks,baseline_aic,The authors develop a subset selection algorithm that is differentiable and discrete and can model complex dependencies between elements in a straightforward and comprehensible way.,0,0,1,0,1,0
191,r1GaAjRcF7,Differentiable Greedy Networks,tldr_gen_aic,Differentiable greedy network for sentence selection .,0,0,0,0,0,0
192,ryxSrhC9KX,Revealing interpretable object representations from human behavior,auth_gold,Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks,0,0,0,0,0,0
193,ryxSrhC9KX,Revealing interpretable object representations from human behavior,pr_gold,This paper describes a large-scale experiment on human object/sematic representations and a model of such representations.,1,0,1,0,0,0
194,ryxSrhC9KX,Revealing interpretable object representations from human behavior,baseline_aic,"The authors estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.",0,0,0,1,0,0
195,ryxSrhC9KX,Revealing interpretable object representations from human behavior,tldr_gen_aic,Low-dimensional embeddings for mental object concepts can be used to model human behavior and predict human typicality judgments.,0,0,0,0,1,0
196,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,auth_gold,Generating text using sentence embeddings from skip-thought vectors with the help of generative adversarial networks.,1,0,0,0,0,0
197,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,pr_gold,Describes application of generative adversarial networks for modeling textual data with the help of ski-thought vectors and experiments with different flavors of gans for two different datasets.,1,0,0,0,0,0
198,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,baseline_aic,This paper proposes an approach for text generation using generative adversarial networks with skip-thought vectors.,1,0,1,0,0,0
199,SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,tldr_gen_aic,This work presents an approach to text generation using skip-thought sentence embeddings in conjunction with gans based on gradient penalty functions and,1,0,1,1,0,0
200,BJemQ209FQ,Learning to Navigate the Web,auth_gold,"The authors train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.",1,1,0,1,0,0
201,BJemQ209FQ,Learning to Navigate the Web,pr_gold,"Develops a curriculum learning method for training an rl agent to navigate a web, based on the idea of decomposing an instruction in to multiple sub-instructions.",1,1,1,1,0,0
202,BJemQ209FQ,Learning to Navigate the Web,baseline_aic,The authors present two methods for reinforcement learning in large state and action spaces with sparse rewards for the web navigation.,1,1,1,0,0,0
203,BJemQ209FQ,Learning to Navigate the Web,tldr_gen_aic,"The authors train dqn, deep reinforcement learning agent, with q-value function approximated with a novel qweb neural network architecture on these smaller",0,0,0,0,1,0
204,SJGvns0qK7,Bayesian Policy Optimization for Model Uncertainty,auth_gold,The authors formulate model uncertainty in reinforcement learning as a continuous bayes-adaptive markov decision process and present a method for practical and scalable bayesian policy optimization.,1,1,0,1,0,0
205,SJGvns0qK7,Bayesian Policy Optimization for Model Uncertainty,pr_gold,"Using a bayesian approach, there is a better trade-off between exploration and exploitation in rl",0,0,0,0,1,0
206,SJGvns0qK7,Bayesian Policy Optimization for Model Uncertainty,baseline_aic,The authors formulate the problem of model uncertainty as a continuous bayes-adaptive markov decision process (bamdp).,0,1,0,1,0,0
207,SJGvns0qK7,Bayesian Policy Optimization for Model Uncertainty,tldr_gen_aic,The authors present a method to address model uncertainty in continuous bayes-adaptive markov decision processes using bayesian filter and augmented state-bel,0,1,1,1,0,0
208,B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,auth_gold,"Conditional recurrent gans for real-valued medical sequences generation, showing novel evaluation approaches and an empirical privacy analysis.",1,0,1,1,0,1
209,B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,pr_gold,Proposes to use synthetic data generated by gans as a replacement for personally identifiable data in training ml models for privacy-sensitive applications,1,0,1,1,0,0
210,B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,baseline_aic,Generative adversarial networks (gans) have shown remarkable success as a framework for training models to produce realistic-looking data.,1,0,0,0,0,1
211,B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,tldr_gen_aic,"The authors present a generative adversarial network architecture to generate real-valued medical time series from an icu, and provide visual and quantitatively verified",1,0,1,1,0,0
212,HJxeWnCcF7,Learning Mixed-Curvature Representations in Product Spaces,auth_gold,Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.,1,0,1,0,1,0
213,HJxeWnCcF7,Learning Mixed-Curvature Representations in Product Spaces,pr_gold,"Proposes a dimensionality reduction method that embeds data into a product manifold of spherical, euclidean, and hyperbolic manifolds. the algorithm is based on matching the geodesic distances on the product manifold to graph distances.",1,0,1,1,0,0
214,HJxeWnCcF7,Learning Mixed-Curvature Representations in Product Spaces,baseline_aic,The authors introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature of the product manifold.,1,0,1,0,0,0
215,HJxeWnCcF7,Learning Mixed-Curvature Representations in Product Spaces,tldr_gen_aic,"The authors learn embeddings in a riemannian product manifold combining hyperbolic, spherical, and euclidean components and equip it with",1,0,0,1,0,0
216,ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,auth_gold,Single shot neural architecture search via direct sparse optimization,1,0,1,1,0,0
217,ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,pr_gold,Presents an architecture search method where connections are removed with sparse regularization.,1,0,1,1,0,0
218,ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,baseline_aic,This paper proposes a novel model pruning view to nas problem.,1,0,1,0,0,1
219,ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,tldr_gen_aic,This paper proposes a direct sparse optimization nas (dso-nas) method for neural architecture search.,1,0,1,1,0,0
220,Hki-ZlbA-,Ground-Truth Adversarial Examples,auth_gold,The authors use formal verification to assess the effectiveness of techniques for finding adversarial examples or for defending against adversarial examples.,1,1,0,1,0,0
221,Hki-ZlbA-,Ground-Truth Adversarial Examples,pr_gold,This paper proposes a method for computing adversarial examples with minimum distance to the original inputs.,1,0,1,0,0,0
222,Hki-ZlbA-,Ground-Truth Adversarial Examples,baseline_aic,In this paper the authors propose a method for using formal verification to assess the effectiveness of techniques for producing adversarial examples or defending against them.,1,1,1,1,0,0
223,Hki-ZlbA-,Ground-Truth Adversarial Examples,tldr_gen_aic,The authors use formal verification techniques to assess the effectiveness of neural network attack and defense techniques against adversarial examples.,1,1,1,1,0,0
224,HJewuJWCZ,Learning to Teach,auth_gold,"The authors propose and verify the effectiveness of learning to teach, a new framework to automatically guide machine learning process.",1,0,1,0,0,0
225,HJewuJWCZ,Learning to Teach,pr_gold,"This paper focuses on ""machine teaching"" and proposes leveraging reinforcement learning by defining the reward as how fast the learner learns and using policy gradient to update the teacher parameters",1,0,1,1,0,0
226,HJewuJWCZ,Learning to Teach,baseline_aic,"In this paper, the authors argue that a formal study on the role of 'teaching' in artificial intelligence is sorely needed.",1,0,0,0,1,0
227,HJewuJWCZ,Learning to Teach,tldr_gen_aic,"The authors argue that the role of teaching in artificial intelligence should be considered more closely, and that an optimization framework should be used to obtain good teaching strategies",1,0,0,0,1,0
228,HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,auth_gold,The authors applied deep learning techniques to hyperspectral image segmentation and iterative feature sampling.,1,0,0,0,0,0
229,HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,pr_gold,Proposes a greedy scheme to select a subset of highly correlated spectral features in a classification task.,0,0,1,0,0,0
230,HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,baseline_aic,The authors use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background.,1,0,0,1,0,0
231,HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,tldr_gen_aic,The authors use deep neural network methods to perform rapid classification of cyanobacterial cells sampled by hyperspectral imaging.,1,0,0,1,0,0
232,HknbyQbC-,Generating Adversarial Examples with Adversarial Networks,auth_gold,The authors propose to generate adversarial example based on generative adversarial networks in a semi-whitebox and black-box settings.,1,0,0,1,0,0
233,HknbyQbC-,Generating Adversarial Examples with Adversarial Networks,pr_gold,"Describes advgan, a conditional gan plus adversarial loss, and evaluates advgan on semi-white box and black box setting, reporting state-of-art results.",1,0,0,1,1,1
234,HknbyQbC-,Generating Adversarial Examples with Adversarial Networks,baseline_aic,"The authors propose advgan to generate adversarial examples with generative adversarial networks, which can learn and approximate the distribution of original instances.",1,0,1,1,0,0
235,HknbyQbC-,Generating Adversarial Examples with Adversarial Networks,tldr_gen_aic,The authors apply generative adversarial networks to generate adversarial examples with generative models in both semi-whitebox and black-box settings.,1,0,1,1,0,0
236,H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,auth_gold,A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.,1,0,1,0,0,0
237,H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,pr_gold,The paper proposes an algorithm to restrict the staleness in asynchronous sgd and provides theoretical analysis,1,1,1,0,0,0
238,H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,baseline_aic,"Asynchronous distributed gradient descent algorithms for training of deep neural networks are usually considered as inefficient, mainly because of the gradient delay problem.",0,1,0,0,0,0
239,H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,tldr_gen_aic,The authors propose a novel asynchronous distributed algorithm for training of deep neural networks that reduces or even completely eliminates worker idle time due to communication overhead.,1,0,1,1,1,0
240,ryetZ20ctX,Defensive Quantization: When Efficiency Meets Robustness,auth_gold,The authors designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.,1,0,1,1,0,1
241,ryetZ20ctX,Defensive Quantization: When Efficiency Meets Robustness,pr_gold,Proposes a regularization scheme to protect quantized neural networks from adversarial attacks using a lipschitz constant filitering of the inner layers' inpout-output.,1,0,1,1,0,0
242,ryetZ20ctX,Defensive Quantization: When Efficiency Meets Robustness,baseline_aic,This paper aims to raise people's awareness about the security of the quantized models.,1,1,0,0,0,0
243,ryetZ20ctX,Defensive Quantization: When Efficiency Meets Robustness,tldr_gen_aic,"The authors propose defensive quantization (dq), a novel method to defend neural networks against adversarial attacks.",1,1,1,0,0,1
244,rJl8FoRcY7,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,auth_gold,Deriving a general formulation of a multi-modal vae from the joint marginal log-likelihood.,1,0,0,0,0,0
245,rJl8FoRcY7,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,pr_gold,Proposes a multi-modal vae with a variational bound derived from the chain rule.,1,0,0,0,0,0
246,rJl8FoRcY7,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,baseline_aic,The application of multi-modal generative models by means of a variational auto encoder (vae) is an upcoming research topic.,1,0,0,0,0,0
247,rJl8FoRcY7,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,tldr_gen_aic,The authors propose a novel multi-modal variational auto encoder derived from the full joint marginal log-likelihood that is able to learn the,1,0,1,1,0,1
248,BJ8c3f-0b,Auto-Encoding Sequential Monte Carlo,auth_gold,"The authors build on auto-encoding sequential monte carlo, gain new theoretical insights and develop an improved training procedure based on those insights.",1,0,1,0,0,1
249,BJ8c3f-0b,Auto-Encoding Sequential Monte Carlo,pr_gold,The paper proposes a version of iwae-style training that uses smc instead of classical importance sampling.,1,0,1,1,0,0
250,BJ8c3f-0b,Auto-Encoding Sequential Monte Carlo,baseline_aic,The authors build on auto-encoding sequential monte carlo (aesmc): a method for model and proposal learning based on maximizing the lower bound to,1,0,1,1,0,0
251,BJ8c3f-0b,Auto-Encoding Sequential Monte Carlo,tldr_gen_aic,"The authors develop a fast, easy-to-implement and scalable method for simultaneous model learning and proposal adaptation in deep generative models.",1,0,1,0,0,1
252,rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",auth_gold,A forensic metric to determine if a given image is a copy (with possible manipulation) of another image from a given dataset.,1,1,1,0,0,0
253,rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",pr_gold,"Introduces the siamese network to identify duplicate and copied/modified images, which can be used to improve surveillance of the published and in-peer-review literature.",1,1,1,1,0,0
254,rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",baseline_aic,"This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manip",1,0,0,1,0,0
255,rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",tldr_gen_aic,The authors apply a 3-branch siamese convolutional neural network to the problem of detecting image duplication and re-use in the scientific,1,0,1,1,0,0
256,rJ33wwxRb,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,auth_gold,The authors show that sgd learns two-layer over-parameterized neural networks with leaky relu activations that provably generalize on linearly separable data.,0,0,0,1,0,0
257,rJ33wwxRb,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,pr_gold,The paper studies overparameterised models being able to learn well-generalising solutions by using a 1-hidden layer network with fixed output layer.,0,1,0,1,0,0
258,rJ33wwxRb,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,baseline_aic,"The authors study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable",0,1,0,0,0,0
259,rJ33wwxRb,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,tldr_gen_aic,The authors provide the first theoretical guarantees for the generalization of over-parameterized neural networks in the setting where the data is linearly separable,0,1,1,0,0,0
260,BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,auth_gold,The authors propose a joint model to incorporate visual knowledge in sentence representations,1,0,1,1,0,0
261,BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,pr_gold,The paper proposes a method to use videos paired with captions to improve sentence embeddings,1,0,1,1,0,0
262,BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,baseline_aic,"The authors use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information.",0,0,1,1,0,0
263,BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,tldr_gen_aic,The authors propose a new way to leverage visual knowledge for sentence representations.,0,1,0,0,0,0
