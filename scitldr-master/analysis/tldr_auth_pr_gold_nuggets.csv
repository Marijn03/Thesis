paper_id,title,auth_pr,tldr,area_field_topic,problem_motivation,mode_of_contrib,details_descrip,results_findings,value_signif
rkeZRGbRW,Variance Regularizing Adversarial Learning,auth_gold,"The authors introduce meta-adversarial learning, a new technique to regularize gans, and propose a training method by explicitly controlling the discriminator's output distribution.",1,0,1,1,0,1
rkeZRGbRW,Variance Regularizing Adversarial Learning,pr_gold,The paper proposes variance regularizing adversarial learning for training gans to ensure that the gradient for the generator does not vanish,1,1,1,1,0,0
B1e4wo09K7,Invariant-equivariant representation learning for multi-class data,auth_gold,This paper presents a novel latent-variable generative modelling technique that enables the representation of global information into one latent variable and local information into another latent variable.,1,0,1,1,0,1
B1e4wo09K7,Invariant-equivariant representation learning for multi-class data,pr_gold,The paper presents a vae that uses labels to separate the learned representation into an invariant and a covariant part.,1,0,1,1,0,0
rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,auth_gold,"The authors show how using skip connections can make speech enhancement models more interpretable, as it makes them use similar mechanisms that have been explored in the dsp literature.",1,1,1,0,1,0
rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,pr_gold,"The authors propose incorporating residual, highway and masking blocks inside a fully convolutional pipeline in order to understand how iterative inference of the output and the masking is performed in a speech enhancement task",1,1,1,1,0,0
rJl0r3R9KX,Regularized Learning for  Domain Adaptation under Label Shifts,auth_gold,A practical and provably guaranteed approach for training efficiently classifiers in the presence of label shifts between source and target data sets,1,1,1,0,0,1
rJl0r3R9KX,Regularized Learning for  Domain Adaptation under Label Shifts,pr_gold,The authors propose a new algorithm for improving the stability of class importance weighting estimation procedure with a two-step procedure.,1,1,1,1,0,1
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,auth_gold,A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.,1,0,1,1,0,1
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,pr_gold,This paper proposes two regularization terms based on a compound hinge loss over the kl divergence between two softmax-normalized input arguments to encourage learning disentangled representations,1,1,1,1,0,0
r1q7n9gAb,The Implicit Bias of Gradient Descent on Separable Data,auth_gold,The normalized solution of gradient descent on logistic regression (or a similarly decaying loss) slowly converges to the l2 max margin solution on separable data.,1,0,0,0,1,0
r1q7n9gAb,The Implicit Bias of Gradient Descent on Separable Data,pr_gold,The paper offers a formal proof that gradient descent on the logistic loss converges very slowly to the hard svm solution in the case where the data are linearly separable.,1,0,1,0,1,0
r1xwKoR9Y7,GamePad: A Learning Environment for Theorem Proving,auth_gold,The authors introduce a system called gamepad to explore the application of machine learning methods to theorem proving in the coq proof assistant.,1,0,1,0,0,0
r1xwKoR9Y7,GamePad: A Learning Environment for Theorem Proving,pr_gold,"This paper describes a system for applying machine learning to interactive theorem proving, focuses on tasks of tactic prediction and position evaluation, and shows that a neural model outperforms an svm on both tasks.",1,0,1,1,1,0
4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,auth_gold,A mixed reality driving simulator using stereo cameras and passthrough vr evaluated in a user study with 24 participants.,1,0,1,1,0,0
4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,pr_gold,Proposes a complicated system for driving simulation.,1,0,1,0,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,auth_gold,The authors propose a bayesian nonparametric model for federated learning with neural networks.,1,0,1,0,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,pr_gold,Uses beta process to do federated neural matching.,1,0,0,1,0,0
Sklsm20ctX,Competitive experience replay,auth_gold,A novel method to learn with sparse reward using adversarial reward re-labeling,1,0,1,1,0,1
Sklsm20ctX,Competitive experience replay,pr_gold,Proposes to use a competitive multi-agent setting for encouraging exploration and shows that cer + her > her ~ cer,1,1,1,0,1,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,auth_gold,The authors prove that nce is self-normalized and demonstrate it on datasets,1,0,1,0,1,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,pr_gold,Presents a proof of the self normalization of nce as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.,1,0,1,0,1,0
B1G9doA9F7,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,auth_gold,A new cyclic adversarial learning augmented with auxiliary task model which improves domain adaptation performance in low resource supervised and unsupervised situations,1,0,1,1,1,1
B1G9doA9F7,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,pr_gold,Proposes an extension of cycle-consistent adversatial adaptation methods in order to tackle domain adaptation where limited supervised target data is available.,1,1,1,1,0,0
HkinqfbAb,Automatic Parameter Tying in Neural Networks,auth_gold,A k-means prior combined with l1 regularization yields state-of-the-art compression results.,1,0,0,1,0,1
HkinqfbAb,Automatic Parameter Tying in Neural Networks,pr_gold,This paper explores soft parameter tying and compression of dnns/cnns,1,0,0,0,0,0
rkzcvoA9YX,Few-Shot Learning by Exploiting Object Relation,auth_gold,Few-shot learning by exploiting the object-level relation to learn the image-level relation (similarity),1,0,1,0,0,0
rkzcvoA9YX,Few-Shot Learning by Exploiting Object Relation,pr_gold,This paper deals with the problem of few-shot learning by proposing an embedding-based approach that learns to compare object-level features between support and query set examples,1,1,1,1,0,0
H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,auth_gold,A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.,1,0,1,0,1,0
H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,pr_gold,The paper proposes an algorithm to restrict the staleness in asynchronous sgd and provides theoretical analysis,1,1,1,0,1,0
rJQDjk-0b,Unbiased Online Recurrent Optimization,auth_gold,"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.",1,0,1,0,0,1
rJQDjk-0b,Unbiased Online Recurrent Optimization,pr_gold,The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the imitation of truncated backpropagation through time,1,1,1,0,0,1
rye4g3AqFm,Deep learning generalizes because the parameter-function map is biased towards simple functions,auth_gold,The parameter-function map of deep networks is hugely biased; this can explain why they generalize. the authors use pac-bayes and gaussian processes to obtain nonvacuous bounds.,1,1,1,1,1,0
rye4g3AqFm,Deep learning generalizes because the parameter-function map is biased towards simple functions,pr_gold,"The paper studies the generalization capabilities of deep neural networks, with the help of the pac-bayesian learning theory and empirically backed intuitions.",1,0,0,1,0,0
B16yEqkCZ,Avoiding Catastrophic States with Intrinsic Fear,auth_gold,Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.,1,1,1,1,0,0
B16yEqkCZ,Avoiding Catastrophic States with Intrinsic Fear,pr_gold,An rl algorithm that combines the dqn algorithm with a fear model trained in parallel to predict catastropohic states.,1,1,1,1,0,0
HyHmGyZCZ,Comparison of Paragram and GloVe Results for Similarity Benchmarks,auth_gold,Paper provides a description of a procedure to enhance word vector space model with an evaluation of paragram and glove models for similarity benchmarks.,1,0,1,1,0,0
HyHmGyZCZ,Comparison of Paragram and GloVe Results for Similarity Benchmarks,pr_gold,This paper suggests a new algorithm that adjusts glove word vectors and then uses a non-euclidean similarity function between them.,1,0,1,1,0,1
SySpa-Z0Z,From Information Bottleneck To Activation Norm Penalty,auth_gold,The authors derive a norm penalty on the output of the neural network from the information bottleneck perspective,1,0,1,1,0,0
SySpa-Z0Z,From Information Bottleneck To Activation Norm Penalty,pr_gold,"Puts forward activation norm penalty, an l_2 type regularization on the activations, deriving it from the information bottleneck principle",1,0,1,1,0,0
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,auth_gold,The authors solve the rubik's cube with pure reinforcement learning,1,0,1,0,0,0
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,pr_gold,Solution to solving rubik cube using reinforcement learning (rl) with monte-carlo tree search (mcts) through autodidactic iteration.,1,0,1,1,0,0
ryeHw1vjiQ,Variational Autoencoders with implicit priors for short-duration text-independent speaker verification,auth_gold,The authors evaluate the effectiveness of having auxiliary discriminative tasks performed on top of statistics of the posterior distribution learned by variational autoencoders to enforce speaker dependency.,1,0,1,1,0,0
ryeHw1vjiQ,Variational Autoencoders with implicit priors for short-duration text-independent speaker verification,pr_gold,Propose an autoencoder model to learn a representation for speaker verification using short-duration analysis windows.,1,0,1,0,0,0
rJhR_pxCZ,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,auth_gold,The authors combine differentiable decision trees with supervised variational autoencoders to enhance interpretability of classification.,1,1,1,0,0,0
rJhR_pxCZ,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,pr_gold,"This paper proposes a hybrid model of a variational autoencoder composed with a differentiable decision tree, and an accompanying training scheme, with experiments demonstrating tree classification performance, neg. log likelihood performance, and latent space interpretability.",1,0,1,1,0,0
ryf7ioRqFX,h-detach: Modifying the LSTM Gradient Towards Better Optimization,auth_gold,A simple algorithm to improve optimization and handling of long term dependencies in lstm,1,1,1,0,0,0
ryf7ioRqFX,h-detach: Modifying the LSTM Gradient Towards Better Optimization,pr_gold,The paper introduces a simple stochastic algorithm called h-detach that is specific to lstm optimization and targeted towards addressing this problem.,1,0,1,0,0,0
S1fNJhRqFX,Exploration using Distributional RL and UCB,auth_gold,Exploration using distributional rl and truncagted variance.,1,0,0,1,0,0
S1fNJhRqFX,Exploration using Distributional RL and UCB,pr_gold,Presents an rl method to manage exploration-explotation trade-offs via ucb techniques.,1,1,0,1,0,0
HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,auth_gold,"The authors propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.",1,0,1,1,1,1
HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,pr_gold,"The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep nn), and treating each task as having its own final layer which could be a ridge regression or a logistic regression.",1,0,1,1,0,0
rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,auth_gold,The authors show that wasserstein spaces are good targets for embedding data with complex semantic structure.,1,0,0,0,1,0
rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,pr_gold,"Learns embeddings in a discrete space of probability distributions, using a minimized, regularised version of wasserstein distances.",1,0,0,1,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,auth_gold,The authors propose a bayesian nonparametric model for federated learning with neural networks.,1,0,1,0,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,pr_gold,Uses beta process to do federated neural matching.,1,0,0,1,0,0
B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,auth_gold,The authors closely analyze the vae objective function and draw novel conclusions that lead to simple enhancements.,1,0,0,0,0,1
B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,pr_gold,Proposes a two-stage vae method to generate high-quality samples and avoid blurriness.,1,1,1,0,0,0
H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,auth_gold,The authors present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding,1,0,1,1,0,0
H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,pr_gold,"This paper introduces a family of attack on confidence thresholding algortihms, focusing mainly on evaluation methodologies.",1,0,1,0,0,0
SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,auth_gold,"The authors make a network of graph convolution networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.",1,0,1,1,1,1
SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,pr_gold,Proposes a new network of gcns with two approaches: a fully connected layer on top of stacked features and attention mechanism that uses scalar weight per gcn.,1,0,1,1,0,1
rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,auth_gold,This paper proposes to transfer knowledge from deep model to shallow one by mimicking features stage by stage.,1,0,1,1,0,0
rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,pr_gold,Explains a stage by stage knowledge transer method by using different structures of resnets,1,0,1,1,0,0
rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,auth_gold,A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization,1,0,1,0,1,0
rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,pr_gold,"Improves the correlation alignment approach to domain adaptation by replacing the euclidean distance with the geodesic log-euclidean distance between two covariance matices, and automatically selecting the balancing cost by the entropy on the target domain.",1,0,1,1,1,0
B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,auth_gold,"The authors propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.",1,0,1,1,0,0
B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,pr_gold,"This paper applies the notion of conceptors, a form a regulariser, to prevent forgetting in continual learning in the training of neural networks on sequential tasks.",1,1,1,1,0,0
H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,auth_gold,Analyze the reason for neural response generative models preferring universal replies; propose a method to avoid it.,1,1,1,0,0,0
H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,pr_gold,Investigates the problem of universal replies plaguing the seq2seq neural generation models,1,1,0,0,0,0
SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,auth_gold,"In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent ""bottleneck state"" predictions, which are useful for planning.",1,0,1,0,1,1
SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,pr_gold,"A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction.",1,0,1,1,0,0
rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,auth_gold,Compact perception of dynamical process,1,0,0,0,0,0
rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,pr_gold,Studies the problem of compactly representing the model of a complex dynamic system while preserving information by using an information bottleneck method.,1,0,0,1,0,0
B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,auth_gold,"Three class priors are all you need to train deep models from only u data, while any two should not be enough.",1,0,0,0,1,0
B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,pr_gold,Proposes an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors and discusses theoretical properties of the estimators.,1,0,1,0,0,0
H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,auth_gold," current somatic mutation methods do not work with liquid biopsies (ie low coverage sequencing), the authors apply a cnn architecture to a unique representation of a read and its ailgnment, the authors show significant improvement over previous methods in the low frequency setting.",1,1,1,1,1,0
H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,pr_gold,Proposes a cnn based solution called kittyhawk for somatic mutation calling at ultra low allele frequencies.,1,0,1,0,0,0
HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,auth_gold,The authors seek to understand learned representations in compressed networks via an experimental regime the authors call deep net triage,1,0,1,1,0,0
HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,pr_gold,Compares various initialization and training methods of transferring knowledge from vgg network to a smaller student network by replacing blocks of layers with single layers.,1,0,1,1,0,0
SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,auth_gold,"The authors propose a method for the construction of arbitrarily deep infinite-width networks, based on which the authors derive a novel weight initialisation scheme for finite-width networks and demonstrate its competitive performance.",1,0,1,1,1,1
SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,pr_gold,Proposes a weight initialization approach to enable infinitely deep and infinite-width networks with experimental results on small datasets.,1,0,1,1,0,0
Syl3_2JCZ,A Self-Organizing Memory Network,auth_gold,The authors derived biologically plausible synaptic plasticity learning rules for a recurrent neural network to store stimulus representations.,1,1,1,0,0,0
Syl3_2JCZ,A Self-Organizing Memory Network,pr_gold,A neural network model consisting of recurrently connected neurons and one or more redouts which aims to retain some output over time.,1,0,1,1,0,0
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,auth_gold,A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.,1,1,1,0,0,0
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,pr_gold,This paper proposes two regularization terms based on a compound hinge loss over the kl divergence between two softmax-normalized input arguments to encourage learning disentangled representations,1,1,1,1,0,0
r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,auth_gold,"Realizing the drawbacks when applying original dropout on densenet, the authors craft the design of dropout method from three aspects, the idea of which could also be applied on other cnn models.",1,1,1,0,0,1
r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,pr_gold,Application of different binary dropout structures and schedules with the specific aim to regularise the densenet architecture.,1,1,0,1,0,0
rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,auth_gold,Detecting out-of-distribution samples by using low-order feature statistics without requiring any change in underlying dnn.,1,1,1,1,0,0
rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,pr_gold,Presents an algorithm to detect out-of-distribution samples by using the running estimate of mean and variance within batchnorm layers to construct feature representations later fed into a linear classifier.,1,1,1,1,0,0
ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,auth_gold,The authors propose a novel method named maximal divergence sequential auto-encoder that leverages variational autoencoder representation for binary code vulnerability detection.,1,1,1,1,0,0
ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,pr_gold,"This paper proposes a variational autoencoder-based architecture for code embeddings for binary software vulnerability detection, with learned embeddings more effective at distinguishing between vulnerable and non-vulnerable binary code than baselines.",1,1,1,1,1,0
BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,auth_gold,Computing attention based on posterior distribution leads to more meaningful attention and better performance,1,0,0,0,1,1
BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,pr_gold,"This paper proposes a sequence to sequence model where attention is treated as a latent variable, and derives novel inference procedures for this model, obtaining improvements in machine translation and morphological inflection generation tasks.",1,0,1,1,1,0
By3VrbbAb,Realtime query completion via deep language models,auth_gold,Realtime search query completion using character-level lstm language models,1,0,0,1,0,0
By3VrbbAb,Realtime query completion via deep language models,pr_gold,"This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a cpu.",1,0,1,0,0,1
HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,auth_gold,Proxy-less neural architecture search for directly learning architectures on large-scale target task (imagenet) while reducing the cost to the same level of normal training.,1,0,1,0,1,0
HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,pr_gold,"This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on ""proxy"" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size.",1,0,0,1,0,0
ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,auth_gold,A new framework based variational inference for out-of-distribution detection,1,0,1,0,0,0
ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,pr_gold,Describes a probabilistic approach to quantifying uncertainty in dnn classification tasks that outperforms other sota methods in the task of out-of-distribution detection.,1,0,1,0,1,0
BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,auth_gold,A new loss based on relatively hard negatives that achieves state-of-the-art performance in image-caption retrieval.,1,0,1,0,1,1
BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,pr_gold,Learning joint embedding of sentences and images using triplet loss that is applied to hardest negatives instead of averaging over all triplets,1,0,1,1,0,0
rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,auth_gold,The authors propose a framework to learn a good policy through imitation learning from a noisy demonstration set via meta-training a demonstration suitability assessor.,1,0,1,1,0,0
rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,pr_gold,"Contributes a maml based algorithm to imitation learning which automatically determines if provided demonstrations are ""suitable"".",1,0,1,1,0,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,auth_gold,The authors prove that nce is self-normalized and demonstrate it on datasets,1,0,1,0,1,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,pr_gold,Presents a proof of the self normalization of nce as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.,1,0,1,0,1,0
B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,auth_gold,The authors argue that the generalization of linear graph embedding is not due to the dimensionality constraint but rather the small norm of embedding vectors.,1,0,0,0,1,0
B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,pr_gold,The authors show that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors rather than dimensionality constraints,1,0,0,0,1,0
SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,auth_gold,"The authors propose an estimator for the maximum mean discrepancy, appropriate when a target distribution is only accessible via a biased sample selection procedure, and show that it can be used in a generative network to correct for this bias.",1,0,1,1,1,0
SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,pr_gold,Proposes an importance-weighted estimator of the mmd to estimate the mmd between distributions based on samples biased according to a known or estimated unknown scheme.,1,0,1,1,0,0
H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,auth_gold,The authors approach to the problem of active learning as a core-set selection problem and show that this approach is especially useful in the batch active learning setting which is crucial when training cnns.,1,0,0,1,0,0
H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,pr_gold,The authors provide an algorithm-agnostic active learning algorithm for multi-class classification,1,0,1,0,0,0
r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,auth_gold,The authors prove that dnn is a recursively approximated solution to the maximum entropy principle.,0,0,1,0,0,0
r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,pr_gold,Presents a derivation which links a dnn to recursive application of maximum entropy model fitting.,0,0,1,0,0,0
1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,auth_gold,This paper describe a 3d authoring tool for providing ar in assembly lines of industry 4.0,1,0,1,0,0,0
1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,pr_gold,The paper addresses how ar authoring tools support training of assembly line systems and proposes an approach,1,0,0,0,0,0
rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,auth_gold,"The authors show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training gans would in fact converge to a stationary solution with a sublinear rate.",0,1,0,0,1,0
rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,pr_gold,This paper uses gans and multi-task learning to provide a convergence guarantee for primal-dual algorithms on certain min-max problems.,1,0,1,0,0,0
S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,auth_gold,Jointly train an adversarial noise generating network with a classification network to provide better robustness to adversarial attacks.,1,1,0,1,0,0
S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,pr_gold,"A gan solution for deep models of classification, faced to white and black box attacks, that produces robust models. ",1,1,1,0,0,0
BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,auth_gold,The authors improve gradient dropping (a technique of only exchanging large gradients on distributed training) by incorporating local gradients while doing a parameter update to reduce quality loss and further improve the training time.,1,1,0,1,0,0
BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,pr_gold,This paper proposes a 3 modes for combining local and global gradients to better use more computing nodes,0,1,0,0,0,0
HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,auth_gold,"The authors present metamimic, an algorithm that takes as input a demonstration dataset and outputs (i) a one-shot high-fidelity imitation policy (ii) an unconditional task policy.",0,0,1,1,0,0
HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,pr_gold,"The paper looks at the problem of one-shot imitation with high accuracy of imitation, extending ddpgfd to use only state trajectories.",1,0,0,1,0,0
Hk91SGWR-,Investigating Human Priors for Playing Video Games,auth_gold,The authors investigate the various kinds of prior knowledge that help human learning and find that general priors about objects play the most critical role in guiding human gameplay.,1,0,0,0,0,0
Hk91SGWR-,Investigating Human Priors for Playing Video Games,pr_gold,"The authors study by experiment, what aspects of human priors are the important for reinforcement learning in video games.",1,0,0,0,0,0
HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,auth_gold,The authors look at neural networks with block diagonal inner product layers for efficiency.,0,1,0,0,0,0
HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,pr_gold,"This paper proposes making the inner layers in a neural network be block diagonal, and discusses that block diagonal matrices are more efficient than pruning and block diagonal layers lead to more efficient networks.",0,1,1,0,1,0
Bys4ob-Rb,Certified Defenses against Adversarial Examples,auth_gold,"The authors demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.",1,1,1,0,0,1
Bys4ob-Rb,Certified Defenses against Adversarial Examples,pr_gold,Proposes a new defense against security attacks on neural networks with the atack model that outputs a security certificate on the algorithm.,1,0,1,1,0,0
HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",auth_gold,"Sgd implicitly performs variational inference; gradient noise is highly non-isotropic, so sgd does not even converge to critical points of the original loss",0,0,0,1,1,0
HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",pr_gold,This paper provides a variational analysis of sgd as a non-equilibrium process.,1,0,1,0,0,0
BkisuzWRW,Zero-Shot Visual Imitation,auth_gold,Agents can learn to imitate solely visual demonstrations (without actions) at test time after learning from their own experience without any form of supervision at training time.,0,1,0,0,0,0
BkisuzWRW,Zero-Shot Visual Imitation,pr_gold,This paper proposes and approach for zero-shot visual learning by learning parametric skill functions.,1,0,1,0,0,0
SJd0EAy0b,Generalized Graph Embedding Models,auth_gold,Generalized graph embedding models,1,0,0,0,0,0
SJd0EAy0b,Generalized Graph Embedding Models,pr_gold,"A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches.

",1,0,1,1,1,0
SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,auth_gold,Implicit models applied to causality and genetics,0,0,0,0,0,0
SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,pr_gold,The authors propose to use the implicit model to tackle genome-wide association problem.,0,1,1,0,0,0
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,auth_gold,The authors solve the rubik's cube with pure reinforcement learning,0,1,0,0,0,0
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,pr_gold,Solution to solving rubik cube using reinforcement learning (rl) with monte-carlo tree search (mcts) through autodidactic iteration.,0,1,0,1,0,0
ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,auth_gold,The authors introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks.,0,1,1,0,0,1
ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,pr_gold,Proposal to move from ad-hoc code generation in deep learning engines to compiler and languages best practices.,0,1,0,0,0,0
Hkbd5xZRb,Spherical CNNs,auth_gold,"The authors introduce spherical cnns, a convolutional network for spherical signals, and apply it to 3d model recognition and molecular energy regression.",1,0,1,0,0,0
Hkbd5xZRb,Spherical CNNs,pr_gold,The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts,1,0,1,1,0,1
r1GaAjRcF7,Differentiable Greedy Networks,auth_gold,The authors propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.,0,0,1,1,0,0
r1GaAjRcF7,Differentiable Greedy Networks,pr_gold,Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'differentiable greedy network' (dgn).,0,0,1,1,0,0
ryxSrhC9KX,Revealing interpretable object representations from human behavior,auth_gold,Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks,0,0,0,0,0,0
ryxSrhC9KX,Revealing interpretable object representations from human behavior,pr_gold,This paper describes a large-scale experiment on human object/sematic representations and a model of such representations.,1,0,1,0,0,0
SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,auth_gold,Generating text using sentence embeddings from skip-thought vectors with the help of generative adversarial networks.,1,0,0,0,0,0
SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,pr_gold,Describes application of generative adversarial networks for modeling textual data with the help of ski-thought vectors and experiments with different flavors of gans for two different datasets.,1,0,0,0,0,0
BJemQ209FQ,Learning to Navigate the Web,auth_gold,"The authors train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.",1,1,0,1,0,0
BJemQ209FQ,Learning to Navigate the Web,pr_gold,"Develops a curriculum learning method for training an rl agent to navigate a web, based on the idea of decomposing an instruction in to multiple sub-instructions.",1,1,1,1,0,0
