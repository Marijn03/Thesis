paper_id,title,auth_pr,tldr,area_field_topic,problem_motivation,mode_of_contrib,details_descrip,results_findings,value_signif
HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,baseline_ao,"In this paper, the authors propose to use ridge regression and logistic regression as the main adaptation mechanism for few-shot learning.",1,0,1,0,0,0
HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,baseline_aic,"In this paper, the authors propose to use ridge regression and logistic regression as the main adaptation mechanism for few-shot learning.",1,0,1,0,0,0
HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,tldr_gen_ao,The authors use ridge regression and logistic regression methods for few-shot learning and achieve state-of-the-art performance on three benchmarks.,1,0,1,0,1,1
HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,tldr_gen_aic,The authors use ridge regression methods from standard machine learning methods to make deep networks learn from fewer examples.,1,0,1,0,0,0
rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,baseline_ao,"Wasserstein spaces are much larger and more flexible than euclidean spaces, in that they can successfully embed a wider variety of metric structures.",1,0,0,0,1,0
rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,baseline_aic,"The authors investigate the representational capacity of learned wasserstein embeddings, showing that they can embed a wide variety of complex metric structures.",1,0,0,0,1,0
rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,tldr_gen_ao,The authors propose a wasserstein embedding that captures the semantic information in the wassermeier distance between embedded distributions.,1,0,0,0,1,0
rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,tldr_gen_aic,"The authors embed data as discrete probability distributions in a wasserstein space, endowed with an optimal transport metric.",1,0,0,1,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,baseline_ao,"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.",1,1,0,0,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,baseline_aic,"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.",1,1,0,0,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,tldr_gen_ao,The authors develop a bayesian nonparametric framework for federated learning with neural networks.,1,0,1,0,0,0
SygHGnRqK7,Probabilistic Federated Neural Matching,tldr_gen_aic,Federated learning with neural networks .,1,0,0,0,0,0
B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,baseline_ao,Variational autoencoders with gaussian encoder/decoder assumptions .,1,0,0,0,0,0
B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,baseline_aic,The authors develop a vae enhancement that requires no additional hyperparameters or tuning that is competitive with a variety of gan models.,1,0,1,0,0,1
B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,tldr_gen_ao,"The authors rigorously analyze vae objective, differentiating situations where this belief is and is not actually true, and then leverage the corresponding insights to develop",1,0,0,1,0,0
B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,tldr_gen_aic,The authors analyze and improve variational autoencoders and develop a simple enhancement that improves their generative ability to generate crisp samples.,1,0,1,0,1,0
H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,baseline_ao,The authors show that simple defenses are able to perform well on mnist but not on cifar.,1,0,0,0,1,0
H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,baseline_aic,The authors propose an attack that is optimal against a variety of confidence thresholding models.,1,0,1,0,0,0
H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,tldr_gen_ao,Maxconfidence attacks against linear models that use confidence thresholding .,1,0,0,0,0,0
H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,tldr_gen_aic,Maxconfidence: a tradeoff curve-based attack against confidence thresholding models .,1,0,1,1,0,0
SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,baseline_ao,"In this paper the authors propose a model, network of gcns (n-gcn), which marries these two lines of work.",1,0,1,0,0,0
SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,baseline_aic,"The authors propose a model, network of gcn (n-gcn), which marries these two lines of work.",1,0,1,0,0,0
SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,tldr_gen_ao,Network of graph convolutional networks (n-gcn) is a semi-supervised learning method for graph embeddings.,1,0,0,0,0,0
SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,tldr_gen_aic,"The authors propose network of graph convolutional networks (n-gcn), a novel architecture for semi-supervised node classification on graphs.",1,0,1,0,0,1
rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,baseline_ao,"In this work, the authors argue that feature is the most important knowledge from teacher.",0,0,0,0,1,0
rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,baseline_aic,"In this paper, the authors propose an efficient learning strategy to mimic features stage by stage and show that the proposed approach significantly narrows down the gap between",1,0,1,1,1,0
rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,tldr_gen_ao,Feature is the most important knowledge from teacher and it is sufficient for student to just learn good features regardless of the target task.,0,0,0,0,1,0
rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,tldr_gen_aic,"The authors propose a task independent knowledge transfer approach for deep learning, where student is trained to mimic features from teacher stage by stage.",1,0,1,1,0,0
rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,baseline_ao,The authors show that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains.,1,0,0,0,1,0
rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,baseline_aic,"In this work, the authors face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages their finding that entropy minimization",1,0,1,0,0,1
rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,tldr_gen_ao,"In this work, the authors face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages their finding that entropy minimization",1,0,1,0,0,1
rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,tldr_gen_aic,Unsupervised domain adaptation with deep learning .,1,0,0,0,0,0
B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,baseline_ao,"Conceptor-aided backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.",1,1,1,1,0,0
B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,baseline_aic,"The authors propose a conceptor-aided backpropagation algorithm, ""conceptor-aided backprop"" (cab), in which",1,0,1,0,0,0
B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,tldr_gen_ao,Conceptor-aided backpropagation algorithm for coping with catastrophic interference.,1,1,1,0,0,0
B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,tldr_gen_aic,The authors extend conceptor-aided backpropagation to deep feedforward networks and propose a method to overcome catastrophic interference.,1,1,1,0,0,0
H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,baseline_ao,The authors propose a max-marginal ranking regularization term to avoid seq2seq models from producing generic and uninformative responses.,1,1,1,0,0,0
H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,baseline_aic,The authors propose a max-marginal ranking regularization term to avoid seq2seq models from producing the generic and uninformative responses.,1,1,1,0,0,0
H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,tldr_gen_ao,The authors decompose the goal of neural response generation (nrg) into the optimizations of word selection and ordering and propose a max-marginal ranking,1,0,1,1,0,0
H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,tldr_gen_aic,On neural response generation from the perspective of human-to-human conversational corpora .,1,0,0,1,0,0
SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,baseline_ao,The authors decouple visual prediction from a rigid notion of time.,1,0,0,1,0,0
SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,baseline_aic,"Our time-agnostic reframing of the prediction problem targets the minima of bottlenecks, where prediction is intuitively easiest.",0,0,1,1,0,0
SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,tldr_gen_ao,"The authors propose a time-agnostic frame prediction method for future and intermediate frame prediction, which exploits the fact that most phenomena pass through predictable bottlen",1,0,1,1,0,0
SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,tldr_gen_aic,"Time-agnostic visual prediction does not require predictions at specific time intervals, and it can work for both forward/future and intermediate frame prediction.",1,0,0,0,1,0
rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,baseline_ao,The authors propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model.,1,0,1,1,0,0
rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,baseline_aic,The authors propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model.,1,0,1,1,0,0
rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,tldr_gen_ao,The authors propose an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of,1,0,1,1,0,0
rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,tldr_gen_aic,The authors propose an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of,1,0,1,1,0,0
B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,baseline_ao,"Empirical risk minimization (erm), with proper loss function and regularization, is the common practice of supervised classification.",1,0,0,0,0,0
B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,baseline_aic,"Empirical risk minimization (erm), with proper loss function and regularization, is the common practice of supervised classification.",1,0,0,0,0,0
B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,tldr_gen_ao,"The authors prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of u data, but",1,0,1,0,1,0
B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,tldr_gen_aic,"The authors prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of u data, but",1,0,1,0,1,0
H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,baseline_ao,Somatic cancer mutation detection at ultra-low variant allele frequencies .,1,0,0,0,0,0
H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,baseline_aic,Somatic cancer mutation detection at ultra-low variant allele frequencies (vafs) .,1,0,0,0,0,0
H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,tldr_gen_ao,The authors define a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at vafs in a manner independent of the depth,1,0,1,1,0,0
H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,tldr_gen_aic,The authors developed a novel architecture that enables the detection of cancer-associated mutations at ultra-low variant allele frequencies (vafs) in cell-,1,0,1,0,0,0
HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,baseline_ao,The authors propose a suite of deep net triage methods and compare them on problem spaces of varying complexity.,1,0,1,0,0,0
HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,baseline_aic,The authors propose a suite of triage methods and compare them on problem spaces of varying complexity.,1,0,1,0,0,0
HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,tldr_gen_ao,"The authors propose a suite of triage methods for assessing criticality of layers in deep networks, which can indicate the relative importance of different layers.",1,0,1,0,0,0
HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,tldr_gen_aic,"The authors present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance",1,0,1,1,0,0
SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,baseline_ao,"The authors derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task",1,0,1,1,0,1
SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,baseline_aic,The authors study the initialisation requirements of infinite-width networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights,1,0,0,1,1,0
SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,tldr_gen_ao,The authors study the initialisation requirements of infinite-width neural networks and propose a principled approach to weight initialisation that correctly accounts for the functional nature of,1,0,1,1,0,1
SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,tldr_gen_aic,The authors propose a principled approach to weight initialisation that allows the construction of infinite-width networks with more than two hidden layers.,1,0,1,1,0,0
Syl3_2JCZ,A Self-Organizing Memory Network,baseline_ao,Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away.,1,0,0,0,0,0
Syl3_2JCZ,A Self-Organizing Memory Network,baseline_aic,The authors identify biologically plausible synaptic plasticity rules that can solve this fine-tuning problem without making strong assumptions like 'tight frame' representations.,1,1,1,0,0,1
Syl3_2JCZ,A Self-Organizing Memory Network,tldr_gen_ao,"The authors identify mechanisms through which biological networks can self-organize to learn memory function, and the authors derived biologically plausible synaptic plasticity rules that dynamically modify",1,1,1,0,0,0
Syl3_2JCZ,A Self-Organizing Memory Network,tldr_gen_aic,The authors derive biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing in working memory to be retained for longer than the time,1,1,1,0,0,0
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,baseline_ao,"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.",1,0,0,0,0,0
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,baseline_aic,"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.",1,0,0,0,0,0
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,tldr_gen_ao,"The authors introduce two novel loss components that substantially improve the quality of produced clusters, and are simple to apply to arbitrary models and cost functions, and do",1,0,1,0,1,1
S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,tldr_gen_aic,The authors propose new loss components that improve the quality of kmeans clustering in terms of mutual information scores and outperforms previous methods.,1,0,1,0,1,1
r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,baseline_ao,A specialized dropout method for densenet .,1,0,1,0,0,0
r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,baseline_aic,The authors propose a new structure named pre-dropout to solve the feature-reuse obstruction when applying standard dropout method on densenet.,1,1,1,0,0,0
r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,tldr_gen_ao,The authors propose a specialized dropout method for densenets with nonlinear connections.,1,0,1,1,0,0
r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,tldr_gen_aic,"The authors design a specialized dropout method for densenet from three aspects, dropout location, drop out granularity, and dropout probability.",1,0,1,1,0,0
rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,baseline_ao,The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.,1,1,0,0,0,0
rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,baseline_aic,The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.,1,1,0,0,0,0
rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,tldr_gen_ao,A simple and efficient plug-and-play method for detecting out-of-distribution samples .,1,1,1,0,0,1
rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,tldr_gen_aic,"The authors propose a simple and efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model",1,1,1,0,0,1
ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,baseline_ao,The authors propose the maximal divergence sequential auto-encoder for binary code vulnerability detection .,1,1,1,0,0,0
ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,baseline_aic,The authors propose the maximal divergence sequential auto-encoder for binary code vulnerability detection.,1,1,1,0,0,0
ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,tldr_gen_ao,Maximal divergence sequential auto-encoder for binary code vulnerability detection .,1,1,1,0,0,0
ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,tldr_gen_aic,The authors propose a novel method for binary code vulnerability detection using deep learning techniques to extract crucial information from the original binaries.,1,1,1,0,0,1
BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,baseline_ao,In this paper the authors show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.,1,0,0,0,1,0
BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,baseline_aic,In this paper the authors show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.,1,0,0,0,1,0
BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,tldr_gen_ao,The authors propose a principled factorization of the full joint distribution of the attention and output variables and propose a posterior attention distribution conditioned on the output.,1,0,1,1,0,1
BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,tldr_gen_aic,The authors present a principled architecture for posterior attention for sequence to sequence learning.,1,0,1,0,0,1
By3VrbbAb,Realtime query completion via deep language models,baseline_ao,The authors propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix . .,1,0,1,0,0,0
By3VrbbAb,Realtime query completion via deep language models,baseline_aic,The authors propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix.,1,0,1,0,0,0
By3VrbbAb,Realtime query completion via deep language models,tldr_gen_ao,The authors propose a method for integrating error correction into language model completion via edit-distance potential and a variant of beam search that can exploit these potential functions,1,0,1,1,0,0
By3VrbbAb,Realtime query completion via deep language models,tldr_gen_aic,The authors present a real-time search query completion approach based upon character-level deep language models.,1,0,1,0,0,0
HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,baseline_ao,"In this paper, the authors present proxylessnas that can directly learn the architectures for large-scale target tasks and target hardware platforms.",1,0,1,0,0,0
HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,baseline_aic,The authors present proxylessnas that can directly learn the architectures for large-scale target tasks and target hardware platforms.,1,0,1,0,0,0
HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,tldr_gen_ao,The authors address the high memory consumption issue of differentiable nas and reduce the computational cost (gpu hours and gpu memory) to the same level of regular,1,1,0,1,0,0
HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,tldr_gen_aic,The authors address the high memory consumption issue of differentiable nas and reduce the computational cost (gpu hours and gpu memory) to the same level of regular,1,1,0,1,0,0
ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,baseline_ao,The authors propose a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task.,1,0,1,1,0,0
ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,baseline_aic,"In this paper, the authors are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of",1,0,1,1,0,0
ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,tldr_gen_ao,The authors propose a variational dirichlet framework with entropy-based uncertainty measure for detecting out-of-distribution examples.,1,0,1,1,0,0
ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,tldr_gen_aic,The authors propose a variational dirichlet framework with entropy-based uncertainty measure to detect out-of-distribution examples.,1,0,1,1,0,0
BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,baseline_ao,The authors present a new technique for learning visual-semantic embeddings for cross-modal retrieval.,1,0,1,0,0,0
BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,baseline_aic,The authors present a new technique for learning visual-semantic embeddings for cross-modal retrieval.,1,0,1,0,0,0
BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,tldr_gen_ao,"The authors introduce a simple change to common loss functions used to learn multi-modal embeddings, combined with fine-tuning and the use",1,0,1,0,0,0
BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,tldr_gen_aic,"Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, the authors introduce a simple change to common loss functions",1,1,1,0,0,0
rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,baseline_ao,A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning .,1,1,0,0,0,0
rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,baseline_aic,"In this paper, the authors address such optimal assumption by learning only from the most suitable demonstrations in a given set.",1,0,1,1,0,0
rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,tldr_gen_ao,The authors address the problem of noisy demonstration set in imitation learning by learning only from the most suitable demonstrations in a given set.,1,1,1,1,0,0
rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,tldr_gen_aic,"The authors propose a generic framework with the demonstration suitability assessor leveraging meta-learning, where the authors train adaptive parameters via meta-imitation-learning",1,0,1,1,0,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,baseline_ao,Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function.,1,0,0,0,1,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,baseline_aic,The authors provide a theoretical justification to self-normalization by viewing nce as a low-rank matrix approximation.,1,0,1,1,0,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,tldr_gen_ao,"The authors provide theoretical justification to self-normalization properties of language models trained using noise contrastive estimation, and the authors provide empirical justification to this property",1,0,1,1,0,0
H1-IBSgMz,A Matrix Approximation View of NCE that Justifies Self-Normalization,tldr_gen_aic,The authors provide a theoretical justification to the self-normalization properties of language models trained using noise contrastive estimation and provide empirical comparison to the alternative,1,0,1,1,0,0
B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,baseline_ao,Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications.,1,0,0,0,0,0
B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,baseline_aic,"The authors show that the generalization error of linear graph embedding methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension",1,0,0,0,1,0
B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,tldr_gen_ao,"The generalization error of linear graph embedding methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension.",1,0,0,0,1,0
B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,tldr_gen_aic,"The authors show that the generalization performance of linear graph embedding methods is not due to the dimensionality constraint as commonly believed, but rather the small",1,0,0,0,1,0
SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,baseline_ao,The maximum mean discrepancy (mmd) between two probability measures p . and q is a metric that is zero if and only if all moments of,1,0,0,0,1,0
SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,baseline_aic,The maximum mean discrepancy between two probability measures p and q is a metric that is zero if and only if all moments of the two measures are equal,1,0,0,0,1,0
SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,tldr_gen_ao,"The authors construct an unbiased estimator for the maximum mean discrepancy between p and q when the authors only have access to p via some biased sample selection mechanism,",1,0,1,1,0,0
SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,tldr_gen_aic,The authors construct an unbiased estimator for the maximum mean discrepancy between two probability measures p and q and use it to train a generative neural network.,1,0,1,1,0,0
H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,baseline_ao,An active learning algorithm for cnns .,1,0,0,0,0,0
H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,baseline_aic,"The authors define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected",1,0,0,1,0,0
H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,tldr_gen_ao,"The authors define active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive",1,0,0,0,0,0
H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,tldr_gen_aic,The authors define the active learning problem for deep cnns as core-set selection problem and provide a rigorous bound between an average loss over any given subset,1,0,0,1,0,0
r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,baseline_ao,Deep learning achieves remarkable generalization capability with overwhelming number of model parameters.,0,0,0,0,0,0
r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,baseline_aic,This paper attempts to provide an alternative understanding of deep learning generalization from the perspective of maximum entropy.,1,0,1,0,0,0
r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,tldr_gen_ao,Dnn is a recursive solution towards maximum entropy principle. the authors first derive two feature conditions that softmax regression strictly apply maximum entropy.,0,0,1,1,0,0
r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,tldr_gen_aic,Dnn is essentially a recursive solution to approximate the feature conditions and thus maximally fulfill maximum entropy principle.,0,0,0,0,0,0
1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,baseline_ao,The use of ar in an industrial context could help for the training of new operators.,1,0,0,0,0,0
1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,baseline_aic,"The authors proposed waat, a 3d authoring tool allowing user to quickly create 3d models of the assembly line in a boiler factory.",1,0,1,1,0,0
1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,tldr_gen_ao,"Waat is a 3d authoring tool for the assembly line and its ar annotations, which can be used for training and verification.",1,0,1,1,0,0
1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,tldr_gen_aic,Waat: a 3d authoring tool for the training of assembly line operators .,1,0,1,0,0,0
rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,baseline_ao,"The authors show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training gans would in fact converge",0,0,0,1,1,0
rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,baseline_aic,The authors study the dynamics of gans from an optimization point of view.,1,0,0,0,0,0
rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,tldr_gen_ao,Generative adversarial networks converge to a stationary solution with a sublinear rate with proper stepsize choice.,0,0,0,1,1,0
rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,tldr_gen_aic,The authors prove that the first-order iterative method in training generative adversarial networks converges to a stationary solution with a sublinear rate.,1,0,1,0,1,0
S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,baseline_ao,In this paper the authors propose a new defensive mechanism under the generative adversarial network~(gan) framework.,1,0,1,0,0,0
S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,baseline_aic,In this paper the authors propose a new defensive mechanism under the generative adversarial network~(gan) framework.,1,0,1,0,0,0
S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,tldr_gen_ao,The authors propose a generative adversarial network based on a minimax game to defend against adversarial attacks.,1,1,1,0,0,0
S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,tldr_gen_aic,The authors propose a new defensive method based on generative adversarial network to defend against black box attacks.,1,1,1,0,0,0
BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,baseline_ao,The authors propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.,0,1,0,1,0,0
BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,baseline_aic,The authors propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.,0,1,0,1,0,0
BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,tldr_gen_ao,Gradient dropping with local gradient update approaches convergence faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping.,1,0,0,0,1,0
BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,tldr_gen_aic,The authors significantly reduce convergence damage caused by gradient dropping through data-parallelism training by combining local and global gradients.,1,0,0,1,1,1
HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,baseline_ao,"This paper introduces, to the best of their knowledge, the largest existing neural networks for deep rl.",1,0,1,0,0,1
HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,baseline_aic,The authors introduce an off-policy rl algorithm (metamimic) to narrow this gap.,1,1,1,0,0,0
HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,tldr_gen_ao,"The authors introduce metamimic, an off-policy rl algorithm for high-fidelity one-shot imitation of diverse novel skills.",1,1,1,0,0,0
HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,tldr_gen_aic,The authors introduce a meta-learning approach to learn high-fidelity one-shot imitation policies by off-policy rl.,1,1,1,0,0,0
Hk91SGWR-,Investigating Human Priors for Playing Video Games,baseline_ao,This paper investigates the role of human priors for solving video games.,1,0,0,0,0,0
Hk91SGWR-,Investigating Human Priors for Playing Video Games,baseline_aic,This paper investigates the role of human priors for solving video games.,1,0,0,0,0,0
Hk91SGWR-,Investigating Human Priors for Playing Video Games,tldr_gen_ao,The authors study the role of priors in video games and find that removing priors from a video game leads to a dramatic drop in the speed with,1,0,0,0,1,0
Hk91SGWR-,Investigating Human Priors for Playing Video Games,tldr_gen_aic,The authors perform a series of ablation studies to quantify the importance of various priors for solving video games.,1,1,0,0,0,0
HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,baseline_ao,Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries.,0,0,0,1,0,0
HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,baseline_aic,A modified version of the fully connected layer the authors call a block diagonal inner product layer.,0,0,0,0,0,0
HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,tldr_gen_ao,The authors propose a block diagonal inner product layer that condenses network storage and speeds up the run time without significant adverse effect on testing accuracy.,0,0,1,0,1,0
HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,tldr_gen_aic,"The authors have shown that block diagonal inner product layers can reduce network size, training time and final execution time without significant harm to the network performance.",0,0,1,0,1,1
Bys4ob-Rb,Certified Defenses against Adversarial Examples,baseline_ao,An adaptive regularizer that encourages robustness against all attacks.,0,1,0,0,0,0
Bys4ob-Rb,Certified Defenses against Adversarial Examples,baseline_aic,The authors propose a method based on a semidefinite relaxation that outputs a certificate that no attack can force the error to exceed a certain value.,0,0,1,1,0,0
Bys4ob-Rb,Certified Defenses against Adversarial Examples,tldr_gen_ao,The authors propose an adaptive regularizer for neural networks with one hidden layer that can defend against small adversarial perturbations.,0,1,1,1,0,0
Bys4ob-Rb,Certified Defenses against Adversarial Examples,tldr_gen_aic,The authors study neural networks with one hidden layer with semidefinite relaxation and provide an adaptive regularizer that encourages robustness against all attacks.,0,1,0,1,0,0
HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",baseline_ao,The authors prove that sgd minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.,0,0,1,0,1,0
HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",baseline_aic,The authors prove that sgd minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.,0,0,1,0,1,0
HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",tldr_gen_ao,"Stochastic gradient descent does perform variational inference, but for a different loss than the one used to compute the gradients.",0,0,0,0,1,0
HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",tldr_gen_aic,"The authors prove that stochastic gradient descent implicitly performs variational inference, but not in the classical sense.",0,0,1,0,1,0
BkisuzWRW,Zero-Shot Visual Imitation,baseline_ao,The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate.,1,0,0,0,0,0
BkisuzWRW,Zero-Shot Visual Imitation,baseline_aic,The authors develop a zero-shot imitator that learns a goal-conditioned skill policy with a novel forward consistency loss.,1,0,1,0,0,0
BkisuzWRW,Zero-Shot Visual Imitation,tldr_gen_ao,The authors propose a zero-shot method for imitation learning in which an agent first explores the world without any expert supervision and then distills its experience into,1,0,1,1,0,0
BkisuzWRW,Zero-Shot Visual Imitation,tldr_gen_aic,The authors pursue an alternative paradigm for imitation learning in which an agent first explores the world without any expert supervision and then distills its experience into a goal,1,0,0,1,0,0
SJd0EAy0b,Generalized Graph Embedding Models,baseline_ao,A multi-shot unsupervised learning framework for graph embedding learning .,1,0,0,0,0,0
SJd0EAy0b,Generalized Graph Embedding Models,baseline_aic,"In this paper, the authors conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms.",1,0,0,1,0,0
SJd0EAy0b,Generalized Graph Embedding Models,tldr_gen_ao,"The authors conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend",1,0,0,1,0,0
SJd0EAy0b,Generalized Graph Embedding Models,tldr_gen_aic,The authors propose a novel and efficient multishot framework for embedding learning in generalized knowledge graphs.,1,0,1,0,0,1
SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,baseline_ao,"In this work, the authors describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density.",0,0,1,1,0,0
SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,baseline_aic,"The authors develop implicit causal models, a class of causal models that leverages neural architectures with an implicit density.",0,0,1,1,0,0
SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,tldr_gen_ao,"The authors propose implicit causal models that capture causal relationships and adjust for confounders, and achieve state of the art accuracy for identifying causal factors.",0,0,1,1,1,1
SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,tldr_gen_aic,"The authors develop implicit causal models that generalize previous methods to capture important nonlinearities, such as gene-gene and gene-population interaction.",0,1,1,0,0,1
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,baseline_ao,"Recently, approximate policy iteration algorithms have achieved super-human proficiency in two-player zero-sum games such as go, chess,",1,1,0,0,0,0
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,baseline_aic,Autodidactic iteration is able to solve the rubik's cube and the 15-puzzle without relying on human data.,0,1,0,0,1,1
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,tldr_gen_ao,Autodidactic iteration: an api algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward,0,1,1,1,0,0
Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,tldr_gen_aic,"The authors develop autodidactic iteration, a new form of approximate policy iteration that solves the rubik's cube without human data",0,1,1,0,0,0
ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,baseline_ao,Deep learning software demands reliability and performance.,0,0,0,0,0,0
ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,baseline_aic,Deep learning software demands reliability and performance.,0,0,0,0,0,0
ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,tldr_gen_ao,"The authors present dlvm, a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and",0,0,1,1,0,0
ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,tldr_gen_aic,"The authors present dlvm, a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and",0,0,1,1,0,0
Hkbd5xZRb,Spherical CNNs,baseline_ao,In this paper the authors introduce the building blocks for constructing spherical cnns.,1,0,1,0,0,0
Hkbd5xZRb,Spherical CNNs,baseline_aic,The authors propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.,1,0,0,1,0,0
Hkbd5xZRb,Spherical CNNs,tldr_gen_ao,The authors propose a generalized cross-correlation that is both expressive and rotation-equivariant for spherical convolutional neural networks.,1,0,1,1,0,0
Hkbd5xZRb,Spherical CNNs,tldr_gen_aic,Convolutional neural networks for spherical mnist images .,1,0,0,0,0,0
r1GaAjRcF7,Differentiable Greedy Networks,baseline_ao,The authors propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.,0,0,1,1,1,0
r1GaAjRcF7,Differentiable Greedy Networks,baseline_aic,The authors develop a subset selection algorithm that is differentiable and discrete and can model complex dependencies between elements in a straightforward and comprehensible way.,0,0,1,0,1,0
r1GaAjRcF7,Differentiable Greedy Networks,tldr_gen_ao,"Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization. in this paper, the authors propose",1,1,0,0,0,0
r1GaAjRcF7,Differentiable Greedy Networks,tldr_gen_aic,Differentiable greedy network for sentence selection .,0,0,0,0,0,0
ryxSrhC9KX,Revealing interpretable object representations from human behavior,baseline_ao,"To study how mental object representations are related to behavior, the authors estimated sparse, non-negative representations of objects using human behavioral judgments.",0,1,0,1,0,0
ryxSrhC9KX,Revealing interpretable object representations from human behavior,baseline_aic,"The authors estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.",0,0,0,1,0,0
ryxSrhC9KX,Revealing interpretable object representations from human behavior,tldr_gen_ao,"Sparse, non-negative representations of objects can be used to explain human behavioral judgments.",1,0,0,0,1,0
ryxSrhC9KX,Revealing interpretable object representations from human behavior,tldr_gen_aic,Low-dimensional embeddings for mental object concepts can be used to model human behavior and predict human typicality judgments.,0,0,0,0,1,0
SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,baseline_ao,This work presents an approach to text generation using skip-thought sentence embeddings in conjunction with gans.,1,0,1,0,0,0
SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,baseline_aic,This paper proposes an approach for text generation using generative adversarial networks with skip-thought vectors.,1,0,1,0,0,0
SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,tldr_gen_ao,Using skip-thought sentence embeddings with gans for text generation based on gradient penalty functions and f-measures.,1,0,0,1,0,0
SkGMOi05FQ,Generating Text through Adversarial Training using Skip-Thought Vectors,tldr_gen_aic,This work presents an approach to text generation using skip-thought sentence embeddings in conjunction with gans based on gradient penalty functions and,1,0,1,1,0,0
BJemQ209FQ,Learning to Navigate the Web,baseline_ao,"The authors train dqn, deep reinforcement learning agent with q-value function approximated with a novel qweb neural network architecture on synthetic instructions.",0,0,0,1,0,0
BJemQ209FQ,Learning to Navigate the Web,baseline_aic,The authors present two methods for reinforcement learning in large state and action spaces with sparse rewards for the web navigation.,1,1,1,0,0,0
BJemQ209FQ,Learning to Navigate the Web,tldr_gen_ao,"Learning in environments with large state and action spaces, and sparse rewards, can hinder a reinforcement learning agent's learning through trial-and-error",1,0,0,0,1,0
BJemQ209FQ,Learning to Navigate the Web,tldr_gen_aic,"The authors train dqn, deep reinforcement learning agent, with q-value function approximated with a novel qweb neural network architecture on these smaller",0,0,0,0,1,0
